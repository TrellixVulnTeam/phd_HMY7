{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nengo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import phd\n",
    "from phd import timit\n",
    "from IPython.display import Audio\n",
    "import sys\n",
    "from nengo.utils.testing import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMIT\n",
    "\n",
    "We will use the [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1) corpus.\n",
    "[NLTK](http://www.nltk.org/) includes a small subset of the corpus,\n",
    "which is useful for instruction;\n",
    "however, for training you really want access to the full corpus.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Using TIMIT in PyLearn2\n",
    "  * https://ift6266h14.wordpress.com/experimenting/\n",
    "  * https://github.com/jfsantos/ift6266h14/blob/master/old/timit_full.py\n",
    "  * http://vdumoulin.github.io/articles/timit-part-2/\n",
    "  * https://jpraymond.wordpress.com/2014/02/21/using-the-new-an-improved-pylearn2-timit-dataset/\n",
    "  * https://github.com/vdumoulin/research/blob/master/code/pylearn2/datasets/timit.py\n",
    "  * https://github.com/jfsantos/ift6266h14/tree/master/old/pylearn2_timit\n",
    "\n",
    "### Possibly useful Python packages\n",
    "\n",
    "* [`PySoundFile`](https://github.com/bastibe/PySoundFile) (reads NIST Sphere, hopefully)\n",
    "* [`PySoundCard`](https://github.com/bastibe/PySoundCard)\n",
    "* [`audio` and related tools](https://github.com/boisgera?tab=repositories) (psychoacoustics?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incoporating with Nengo\n",
    "\n",
    "Basically, we want to use TIMIT\n",
    "to generate evaluation points\n",
    "and phoneme targets,\n",
    "which we will use to solve for\n",
    "appropriate decoding weights\n",
    "for the ensembles that represent\n",
    "acoustic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "if($(IPython.toolbar.selector.concat(' > #kill-run-first')).length == 0){\n",
    "  IPython.toolbar.add_buttons_group([\n",
    "    {\n",
    "      'label'   : 'kill and run-first',\n",
    "      'icon'    : 'fa fa-angle-double-down',\n",
    "      'callback': function(){\n",
    "        IPython.notebook.kernel.restart();\n",
    "        $(IPython.events).one('kernel_ready.Kernel', function(){\n",
    "          var idx = IPython.notebook.get_selected_index();\n",
    "          IPython.notebook.select(0);\n",
    "          IPython.notebook.execute_cell();\n",
    "          IPython.notebook.select(idx);\n",
    "        });\n",
    "      }\n",
    "    }\n",
    "  ], 'kill-run-first');\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's work with a single utterance first\n",
    "# Get the utterance and the data associated with it\n",
    "utt = timit.Utterance(\n",
    "    corpus='TRAIN',\n",
    "    region=1,\n",
    "    sex='M',\n",
    "    spkr_id='DAC0',\n",
    "    sent_type='A',\n",
    "    sent_number=1,\n",
    ")\n",
    "\n",
    "data, fs = sf.read(utt.wav)\n",
    "dt = 1. / fs\n",
    "plt.plot(np.arange(data.size) * dt, data)\n",
    "print data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Audio(data=data.ravel(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phone transcriptions are (fortunately!) available\n",
    "in `*.phn` files. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat {utt.phn}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes in TIMIT\n",
    "\n",
    "### Consonants\n",
    "\n",
    "#### Stops\n",
    "\n",
    "| Symbol | Example word    | Possible phonetic transcription |\n",
    "|--------|-----------------|---------------------------------|\n",
    "| b      |    bee          |    BCL B iy                     |\n",
    "| d      |    day          |    DCL D ey                     |\n",
    "| g      |    gay          |    GCL G ey                     |\n",
    "| p      |    pea          |    PCL P iy                     |\n",
    "| t      |    tea          |    TCL T iy                     |\n",
    "| k      |    key          |    KCL K iy                     |\n",
    "| dx     |    muddy, dirty |    m ah DX iy, dcl d er DX iy   |\n",
    "| q      |    bat          |    bcl b ae Q                   |\n",
    "\n",
    "####  Affricates\n",
    "\n",
    "| Symbol | Example word | Possible phonetic transcription |\n",
    "|--------|--------------|---------------------------------|\n",
    "| jh     |    joke      |    DCL JH ow kcl k              |\n",
    "| ch     |    choke     |    TCL CH ow kcl k              |\n",
    "\n",
    "####  Fricatives\n",
    "\n",
    "| Symbol | Example word | Possible phonetic transcription |\n",
    "|--------|--------------|---------------------------------|\n",
    "| s      |    sea       |    S iy                         |\n",
    "| sh     |    she       |    SH iy                        |\n",
    "| z      |    zone      |    Z ow n                       |\n",
    "| zh     |    azure     |    ae ZH er                     |\n",
    "| f      |    fin       |    F ih n                       |\n",
    "| th     |    thin      |    TH ih n                      |\n",
    "| v      |    van       |    V ae n                       |\n",
    "| dh     |    then      |    DH e n                       |\n",
    "\n",
    "#### Nasals\n",
    "\n",
    "| Symbol | Example word  | Possible phonetic transcription |\n",
    "|--------|---------------|---------------------------------|\n",
    "| m      |    mom        |    M aa M                       |\n",
    "| n      |    noon       |    N uw N                       |\n",
    "| ng     |    sing       |    s ih NG                      |\n",
    "| em     |    bottom     |    b aa tcl t EM                |\n",
    "| en     |    button     |    b ah q EN                    |\n",
    "| eng    |    washington |    w aa sh ENG tcl t ax n       |\n",
    "| nx     |    winner     |    w ih NX axr                  |\n",
    "\n",
    "#### Semivowels and glides\n",
    "\n",
    "| Symbol | Example word | Possible phonetic transcription |\n",
    "|--------|--------------|---------------------------------|\n",
    "| l      |    lay       |    L ey                         |\n",
    "| r      |    ray       |    R ey                         |\n",
    "| w      |    way       |    W ey                         |\n",
    "| y      |    yacht     |    Y aa tcl t                   |\n",
    "| hh     |    hay       |    HH ey                        |\n",
    "| hv     |    ahead     |    ax HV eh dcl d               |\n",
    "| el     |    bottle    |    bcl b aa tcl t EL            |\n",
    "\n",
    "###  Vowels\n",
    "\n",
    "| Symbol | Example word | Possible phonetic transcription  |\n",
    "|--------|--------------|----------------------------------|\n",
    "| iy     |    beet      |    bcl b IY tcl t                |\n",
    "| ih     |    bit       |    bcl b IH tcl t                |\n",
    "| eh     |    bet       |    bcl b EH tcl t                |\n",
    "| ey     |    bait      |    bcl b EY tcl t                |\n",
    "| ae     |    bat       |    bcl b AE tcl t                |\n",
    "| aa     |    bott      |    bcl b AA tcl t                |\n",
    "| aw     |    bout      |    bcl b AW tcl t                |\n",
    "| ay     |    bite      |    bcl b AY tcl t                |\n",
    "| ah     |    but       |    bcl b AH tcl t                |\n",
    "| ao     |    bought    |    bcl b AO tcl t                |\n",
    "| oy     |    boy       |    bcl b OY                      |\n",
    "| ow     |    boat      |    bcl b OW tcl t                |\n",
    "| uh     |    book      |    bcl b UH kcl k                |\n",
    "| uw     |    boot      |    bcl b UW tcl t                |\n",
    "| ux     |    toot      |    tcl t UX tcl t                |\n",
    "| er     |    bird      |    bcl b ER dcl d                |\n",
    "| ax     |    about     |    AX bcl b aw tcl t             |\n",
    "| ix     |    debit     |    dcl d eh bcl b IX tcl t       |\n",
    "| axr    |    butter    |    bcl b ah dx AXR               |\n",
    "| ax-h   |    suspect   |    s AX-H s pcl p eh kcl k tcl t |\n",
    "\n",
    "### Others\n",
    "\n",
    "| Symbol | Description                          |\n",
    "|--------|--------------------------------------|\n",
    "| pau    | pause                                |\n",
    "| epi    | epenthetic silence                   |\n",
    "| h#     | begin/end marker (non-speech events) |\n",
    "| 1      | primary stress marker                |\n",
    "| 2      | secondary stress marker              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from phd.timit import consonants, closures, vowels, ignores\n",
    "print consonants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse a `.phn` file into a string of phonemes\n",
    "and their corresponding audio slices.\n",
    "We'll separate these into separate vowel\n",
    "and consonant lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cons = defaultdict(list)\n",
    "vows = defaultdict(list)\n",
    "\n",
    "with open(utt.phn, 'r') as phnfile:\n",
    "    for line in phnfile:\n",
    "        start, end, phn = line.split()\n",
    "        start, end = int(start), int(end)\n",
    "\n",
    "        if phn in ignores:\n",
    "            continue\n",
    "        if phn in closures:\n",
    "            phn = closures[phn]\n",
    "\n",
    "        dataslice = np.array(data[start:end])\n",
    "        if phn in consonants:\n",
    "            cons[phn].append(dataslice)\n",
    "        elif phn in vowels:\n",
    "            vows[phn].append(dataslice)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized phoneme: '%s'\" % phn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at all of the speech samples for a random vowel\n",
    "import random\n",
    "vow_phn = random.choice(list(vows))\n",
    "print(vow_phn)\n",
    "speech = np.concatenate(vows[vow_phn])\n",
    "plt.plot(speech)\n",
    "dt = 1. / fs\n",
    "plt.plot(np.arange(speech.size) * dt, speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's repeat this to get something we can listen to\n",
    "\n",
    "timit_root = os.path.join(extract_path, 'timit')\n",
    "\n",
    "def timit_path(corpus, region, sex, spkr_id, sent_type, sent_number):\n",
    "    return os.path.join(timit_root,\n",
    "                        corpus,\n",
    "                        \"DR%d\" % region,\n",
    "                        \"%s%s\" % (sex, spkr_id),\n",
    "                        \"S%s%d\" % (sent_type, sent_number))\n",
    "\n",
    "def add_utterance(tpath, cons, vows):\n",
    "    data, fs = sf.read(\"%s.WAV\" % tpath)\n",
    "    with open(\"%s.PHN\" % tpath, 'r') as phnfile:\n",
    "        for line in phnfile:\n",
    "            start, end, phn = line.split()\n",
    "            start, end = int(start), int(end)\n",
    "\n",
    "            if phn in ignores:\n",
    "                continue\n",
    "            if phn in closures:\n",
    "                phn = closures[phn]\n",
    "\n",
    "            dataslice = np.array(data[start:end])\n",
    "            if phn in consonants:\n",
    "                cons[phn].append(dataslice)\n",
    "            elif phn in vowels:\n",
    "                vows[phn].append(dataslice)\n",
    "            else:\n",
    "                raise ValueError(\"Unrecognized phoneme: '%s'\" % phn)\n",
    "\n",
    "cons = defaultdict(list)\n",
    "vows = defaultdict(list)\n",
    "\n",
    "region = 1\n",
    "sex = 'M'\n",
    "spkr_id = 'CPM0'\n",
    "\n",
    "for sent_type, sent_number in zip(['A', 'A', 'I', 'I', 'I', 'X',  'X', 'X', 'X'],\n",
    "                                  [1, 2, 564, 1194, 1824, 24, 114, 204, 294, 384]):\n",
    "    tpath = timit_path('TRAIN', region, sex, spkr_id, sent_type, sent_number)\n",
    "    add_utterance(tpath, cons=cons, vows=vows)\n",
    "\n",
    "# Let's hear all the 'ae' phonemes\n",
    "phn = np.concatenate(vows['ow']).ravel()\n",
    "print(phn.shape)\n",
    "Audio(data=phn, rate=fs)  # Dunno why this only works 10% of the time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from phd.sounds import ArrayProcess\n",
    "\n",
    "# Final step: transform cons and vows into eval_points and targets\n",
    "def phn2nengo(model, probe, phonemes, samples):\n",
    "    orig_sound = model.auditory_filter.sound_process\n",
    "    dt = 1. / fs\n",
    "\n",
    "    eval_points = []\n",
    "    targets = []\n",
    "    for i, phoneme in enumerate(phonemes):\n",
    "        sound = np.concatenate(samples[phoneme]).ravel()\n",
    "        target = np.zeros((len(phonemes), sound.size))\n",
    "        target[i]\n",
    "        model.auditory_filter.sound_process = ArrayProcess(sound)\n",
    "        sim = nengo.Simulator(model, dt=dt*.5)\n",
    "        sim.run(dt * sound.size)\n",
    "        #if pool is not None:\n",
    "        #    d = vowel.shape[1] // pool\n",
    "        #    pooled_v = np.zeros((vowel.shape[0], d))\n",
    "        #    for p in range(d):\n",
    "        #        pooled_v[:, p] = np.sum(vowel[:, p*pool:(p+1)*pool], axis=1)\n",
    "        #    vowel = pooled_v\n",
    "        eval_points.append(sim.data[probe])\n",
    "        targets.append(target)\n",
    "\n",
    "    model.auditory_filter.sound_process = orig_sound\n",
    "    return np.concatenate(eval_points), np.concatenate(targets)\n",
    "\n",
    "# fs = 20000.\n",
    "freqs = phd.filters.erbspace(20, 10000, 64)\n",
    "sound = phd.sounds.WavFile('speech.wav')\n",
    "aud_filter = phd.filters.gammatone(freqs)\n",
    "cons_delay = 0.075\n",
    "vowel_delay = 0.03\n",
    "# Note: no integrator here\n",
    "\n",
    "model = phd.SpeechRecognition()\n",
    "model.add_periphery(freqs, sound, aud_filter, fs=fs, middle_ear=True)\n",
    "model.add_derivative(n_neurons=30, delay=cons_delay)\n",
    "model.add_derivative(n_neurons=30, delay=vowel_delay)\n",
    "\n",
    "with model:\n",
    "    # TODO: put all the info into one probe\n",
    "    pass\n",
    "\n",
    "with model:\n",
    "    vowel_p = nengo.Probe(vowel, synapse=0.01, sample_every=0.001)\n",
    "    cons_p = nengo.Probe(cons, synapse=0.01, sample_every=0.001)\n",
    "\n",
    "vowel_ep, vowel_targets = phn2nengo(model, v_probe, vowels, vows)\n",
    "cons_ep, cons_targets = phn2nengo(model, c_probe, consonants, cons)\n",
    "\n",
    "_, vow_detect = model.add_phoneme_detector(15, vowel_ep, vowel_targets, [vowel_delay])\n",
    "_, cons_detect = model.add_phoneme_detector(15, cons_ep, cons_targets, [cons_delay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This should give us all audio clips of a particular phoneme...\n",
    "training = timit.TrainingData(None, [0.01], timit.consonants + timit.vowels)\n",
    "with Timer() as t:\n",
    "    audio = training.generate_audio()\n",
    "print \"Took %s seconds\" % t.duration\n",
    "bytes = sys.getsizeof(audio)\n",
    "for phn in audio:\n",
    "    bytes += sys.getsizeof(audio[phn])\n",
    "print \"Takes up %f MB of memory\" % (float(bytes) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = timit.TrainingData(None, [0.01], ['b', 'd', 't'])\n",
    "with Timer() as t:\n",
    "    audio = training.generate_audio()\n",
    "print \"Took %s seconds\" % t.duration\n",
    "with Timer() as t:\n",
    "    targets = training.generate_targets(audio)\n",
    "print \"Took %s seconds\" % t.duration\n",
    "\n",
    "all_audio = []\n",
    "for phoneme in sorted(list(audio)):\n",
    "    all_audio.append(np.concatenate(audio[phoneme]).ravel())\n",
    "all_audio = np.concatenate(all_audio)\n",
    "print(all_audio.shape)\n",
    "\n",
    "plt.plot(all_audio[:5000:16])\n",
    "plt.plot(targets.T[:5000/16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = phd.Sermo(execution=False)\n",
    "periphery = model.recognition.periphery\n",
    "periphery.fs = 20000\n",
    "periphery.freqs = phd.filters.erbspace(20, 10000, 64)\n",
    "periphery.sound_process = phd.processes.WavFile('speech.wav')\n",
    "periphery.auditory_filter = phd.filters.gammatone(periphery.freqs)\n",
    "fast_deriv = model.recognition.add_derivative(delay=0.01)\n",
    "fast_deriv.klass = 'TrippFF'\n",
    "fast_deriv.args = {}\n",
    "slow_deriv = model.recognition.add_derivative(delay=0.1)\n",
    "slow_deriv.klass = 'TrippFF'\n",
    "slow_deriv.args = {}\n",
    "\n",
    "training = timit.TrainingData(model, [0.01], ['ae'], max_simtime=0.5)\n",
    "print(\"Generated: %s\" % training.generated)\n",
    "print(training.cache_file())\n",
    "timit.TrainingData.clear_cache()\n",
    "try:\n",
    "    training.get()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "training.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep, t = training.get()\n",
    "ep.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from phd.plots import cochleogram\n",
    "\n",
    "freqs = model.recognition.periphery.freqs\n",
    "dims = freqs.size\n",
    "time = np.arange(ep.shape[0]) * training.sample_every\n",
    "cochleogram(ep[:, dims:], time, freqs)\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(time, t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It seems to work; let's generate training data for vowels and consonants.\n",
    "# For now, we'll use the fast derivative for consonants,\n",
    "# slow for vowels.\n",
    "model = phd.Sermo(execution=False)\n",
    "periphery = model.recognition.periphery\n",
    "periphery.fs = 20000\n",
    "periphery.freqs = phd.filters.erbspace(20, 10000, 64)\n",
    "periphery.sound_process = phd.processes.WavFile('speech.wav')\n",
    "periphery.auditory_filter = phd.filters.gammatone(periphery.freqs)\n",
    "fast_deriv = model.recognition.add_derivative(delay=0.01)\n",
    "fast_deriv.klass = 'TrippFF'\n",
    "fast_deriv.args = {}\n",
    "slow_deriv = model.recognition.add_derivative(delay=0.1)\n",
    "slow_deriv.klass = 'TrippFF'\n",
    "slow_deriv.args = {}\n",
    "\n",
    "train_cons = timit.TrainingData(model, [0.01], timit.consonants)\n",
    "train_vow = timit.TrainingData(model, [0.1], timit.vowels)\n",
    "train_cons.generate()\n",
    "train_vow.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are kind of huge amounts of data...\n",
    "from phd import config\n",
    "!du -h {config.cache_dir}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So let's do some testing to see if any of our solvers can handle it\n",
    "# Based on nengo.tests.test_solvers\n",
    "from nengo import solvers\n",
    "\n",
    "def get_rate_function(n_neurons, dims, neuron_type=nengo.LIF, rng=None):\n",
    "    neurons = neuron_type(n_neurons)\n",
    "    gain, bias = neurons.gain_bias(\n",
    "        rng.uniform(200, 400, n_neurons), rng.uniform(-1, 1, n_neurons))\n",
    "    rates = lambda x: neurons.rates(x, gain, bias)\n",
    "    return rates\n",
    "\n",
    "\n",
    "def get_encoders(n_neurons, dims, rng=None):\n",
    "    return nengo.dists.UniformHypersphere(surface=True).sample(n_neurons, dims, rng=rng).T\n",
    "\n",
    "\n",
    "def test_decoder_solver(solver, eval_points, targets):\n",
    "    rng = np.random.RandomState(10)\n",
    "\n",
    "    dims = eval_points.shape[1]\n",
    "    n_neurons = 30 * dims\n",
    "    rates = get_rate_function(n_neurons, dims, rng=rng)\n",
    "    E = get_encoders(n_neurons, dims, rng=rng)\n",
    "\n",
    "    Atargets = rates(np.dot(eval_points, E))\n",
    "    D, info = solver(Atargets, targets, rng=rng)\n",
    "\n",
    "    # est = np.dot(Atargets, D)\n",
    "    # rel_rmse = rms(est - targets) / rms(targets)\n",
    "    print solver\n",
    "    print \"  Time: %s\" % info['time']\n",
    "    print \"  Mean RMSE: %s\" % np.mean(info['rmses'])\n",
    "\n",
    "\n",
    "# We'll use the consonants for now\n",
    "eval_points, targets = train_cons.get()\n",
    "\n",
    "# Things are a little off... that's fine for now, but we should TODO fix that\n",
    "eval_points = eval_points[:targets.shape[1]]\n",
    "\n",
    "print \"eval_points.shape=%s\" % (eval_points.shape,)\n",
    "print \"targets.shape=%s\" % (targets.shape,)\n",
    "\n",
    "solvers = [\n",
    "    solvers.Lstsq(rcond=0.01),\n",
    "    solvers.LstsqNoise(noise=0.1, solver=solvers.cholesky),\n",
    "    solvers.LstsqMultNoise(noise=0.1, solver=solvers.cholesky),\n",
    "    solvers.LstsqL2(reg=0.1, solver=solvers.cholesky),\n",
    "    solvers.LstsqL2nz(reg=0.1, solver=solvers.cholesky),\n",
    "    # solvers.LstsqL1(l1=1e-4, l2=1e-6),  # Way too slow... accurate though\n",
    "    solvers.LstsqDrop(drop=0.25,\n",
    "                      solver1=solvers.LstsqL2nz(reg=0.1, solver=subsolver),\n",
    "                      solver2=solvers.LstsqL2nz(reg=0.01, solver=subsolver)),\n",
    "    solvers.Nnls(),\n",
    "    solvers.NnlsL2(reg=0.1),\n",
    "    solvers.NnlsL2nz(reg=0.1),\n",
    "    solvers.LstsqNoise(noise=0.1, solver=solvers.randomized_svd),\n",
    "    solvers.LstsqMultNoise(noise=0.1, solver=solvers.randomized_svd),\n",
    "    solvers.LstsqL2(reg=0.1, solver=solvers.randomzied_svd),\n",
    "]\n",
    "\n",
    "for solver in solvers:\n",
    "    test_decoder_solver(solver, eval_points, targets.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Last cell takes too long, so I just copied the results here\n",
    "res = [\n",
    "    # solver, time, mean_rmse\n",
    "    ('Lstsq', 154.704920053, 0.141213372439),\n",
    "    ('LstsqNoise (cholesky)', 51.0423779488, 0.130832001688),\n",
    "    ('LstsqMultNoise (cholesky)', 50.0834109783, 0.11170648566),\n",
    "    ('LstsqL2 (cholesky)', 22.3267519474, 0.123292167674),\n",
    "    ('LstsqL2nz (cholesky)', 25.1613600254, 0.11741118202),\n",
    "    ('LstsqL1', 18306.552321, 0.121022449449),\n",
    "    ('LstsqDrop', 638.563055038, 0.105056237968),\n",
    "    ('Nnls', 3678.89870405, 0.154833377335),\n",
    "    ('NnlsL2', 73.7885270119, 0.157904333119),\n",
    "    ('NnlsL2nz', 75.2918219566, 0.165642470283),\n",
    "    ('LstsqNoise (randomized_svd)', 32.4525079727, 0.155343124338),\n",
    "    ('LstsqMultNoise (randomized_svd)', 33.2236630917, 0.154465728835),\n",
    "    ('LstsqL2 (randomized_svd)', 6.79836702347, 0.154354227613),\n",
    "]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'solver': [r[0] for r in res],\n",
    "    'time': [r[1] for r in res],\n",
    "    'mean_rmse': [r[2] for r in res],\n",
    "})\n",
    "\n",
    "plt.figure()\n",
    "sns.barplot(y='solver', x='time', data=data)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Time')\n",
    "plt.xlim(right=200)\n",
    "plt.figure()\n",
    "sns.barplot(y='solver', x='mean_rmse', data=data)\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Mean RMSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
