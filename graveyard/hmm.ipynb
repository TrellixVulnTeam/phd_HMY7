{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import skspeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM definition\n",
    "\n",
    "- $N$: number of states in the model (e.g., # of urns)\n",
    "- $S=S_{1...N}$: the states in the model\n",
    "- $q_t$: the state at time $t$\n",
    "- $M$: the number of observation symbols per state (i.e., alphabet size)\n",
    "- $V=v_{1...M}$: the observation symbols\n",
    "- $A=\\{a_{ij}\\}; a_{ij} = P[q_{t+1} = S_j|q_t = S_i]$: state transition probability distribution\n",
    "- $B=\\{b_j(k)\\}; b_j(k) = P[v_k\\text{ at }t|q_t = S_j]$: observation symbol probability distribution in state $j$\n",
    "- $\\pi=\\{\\pi_i\\}; \\pi_i = P[q_1 = S_i]$: initial state distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_pdf(pdf, size=1):\n",
    "    states = np.arange(pdf.size)\n",
    "    dist = stats.rv_discrete(name='pdf', values=(states, pdf))\n",
    "    return dist.rvs(size)\n",
    "\n",
    "def observe(n_t, a, b, pi):\n",
    "    q_t = sample_pdf(pi)  # initial state\n",
    "    os = []\n",
    "    qs = [q_t]\n",
    "    for t in xrange(n_t):\n",
    "        o_t = sample_pdf(b[q_t])\n",
    "        q_t = sample_pdf(a[q_t])\n",
    "        os.append(o_t)\n",
    "        qs.append(q_t)\n",
    "\n",
    "\n",
    "class HMM(object):\n",
    "    def __init__(self, a, b, pi):\n",
    "        assert a.shape[0] == a.shape[1]\n",
    "        assert b.shape[0] == a.shape[0]\n",
    "        assert pi.size == a.shape[0]\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.pi = pi\n",
    "        self.N = self.a.shape[0]\n",
    "        self.M = self.b.shape[1]\n",
    "        self.states = np.arange(self.N, dtype=int)\n",
    "\n",
    "    def observe(self, n_t):\n",
    "        return observe(n_t, self.a, self.b, self.pi)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # forward variable;\n",
    "        # alpha_t(i) = P(past O sequence, qt = Si | hmm)\n",
    "        f_prev = self.b.T[obs[0]] * self.pi\n",
    "        fwd = [[f_prev]]\n",
    "        for o_i in obs[1:]:\n",
    "            f_curr = np.zeros(self.N)\n",
    "            prev_f_sum = np.sum(f_prev * self.a.T, axis=1)\n",
    "            f_curr = self.b.T[o_i] * prev_f_sum\n",
    "            fwd.append([f_curr])\n",
    "            f_prev = f_curr\n",
    "        return np.concatenate(fwd, axis=0)\n",
    "\n",
    "    def backward(self, obs):\n",
    "        # backward variable;\n",
    "        # beta_t(i) = P(future O sequence|qt = Si, hmm)\n",
    "        b_prev = np.ones(self.N)\n",
    "        bkw = [[b_prev]]\n",
    "        for i, o_i_plus in enumerate(reversed(obs[1:])):\n",
    "            b_curr = np.sum(a * self.b.T[o_i_plus] * b_prev, axis=1)\n",
    "            bkw.insert(0, [b_curr])\n",
    "            b_prev = b_curr\n",
    "        return np.concatenate(bkw, axis=0)\n",
    "\n",
    "    def obs_probability(self, obs):\n",
    "        \"\"\"Computed with the forward-backward procedure.\"\"\"\n",
    "        fwd = self.forward(obs)\n",
    "        return np.sum(fwd[-1])\n",
    "\n",
    "    def optimal_path(self, obs):\n",
    "        \"\"\"Computed with Viterbi algorithm.\"\"\"\n",
    "        return self.viterbi(obs)[1]\n",
    "\n",
    "    def viterbi(self, obs):\n",
    "        V = [self.pi * self.b.T[obs[0]]]\n",
    "        path = [[s] for s in self.states]\n",
    "\n",
    "        for t in xrange(1, len(obs)):\n",
    "            v = V[t-1] * (self.a * self.b.T[obs[t]]).T\n",
    "            prob, state = np.amax(v, axis=1), np.argmax(v, axis=1)\n",
    "            V.append(prob)\n",
    "            path = [path[state[y]] + [y] for y in self.states]\n",
    "        # Return the most likely sequence\n",
    "        prob, state = np.amax(V[-1]), np.argmax(V[-1])\n",
    "        return prob, path[state]\n",
    "\n",
    "    def update(self, obs, n_iters=1):\n",
    "        \"\"\"Update a, b, pi so that `obs` is more likely.\n",
    "\n",
    "        Uses the Baum-Welch algorithm.\n",
    "        \"\"\"\n",
    "        obs = np.asarray(obs)\n",
    "        print(\"Before P(obs) = %f\" % self.obs_probability(obs))\n",
    "\n",
    "        for _ in xrange(n_iters):\n",
    "            alpha = self.forward(obs)\n",
    "            beta = self.backward(obs)\n",
    "\n",
    "            xi = np.zeros((self.N, self.N, len(obs) - 1))\n",
    "            for t in xrange(len(obs) - 1):\n",
    "                denom = np.dot(np.dot(alpha[t], self.a) * self.b.T[obs[t+1]],\n",
    "                               beta[t+1])\n",
    "                for i in xrange(self.N):\n",
    "                    numer = (alpha[t, i]\n",
    "                            * self.a[i]\n",
    "                            * self.b.T[obs[t+1]]\n",
    "                            * beta[t+1])\n",
    "                    xi[i, :, t] = numer / denom\n",
    "  \n",
    "            # gamma_t(i) = P(q_t = S_i | O, hmm)\n",
    "            gamma = np.squeeze(np.sum(xi, axis=1))\n",
    "            # Need final gamma element for new B\n",
    "            prod = (alpha[-1] * beta[-1]).reshape((-1,1))\n",
    "            gamma = np.hstack((gamma,  prod / np.sum(prod))) # append one more to gamma!!!\n",
    "\n",
    "            new_pi = gamma.T[0]\n",
    "            new_a = np.sum(xi, axis=2) / np.sum(gamma[:, :-1], axis=1).reshape((-1,1))\n",
    "            new_b = np.array(b)\n",
    "\n",
    "            if False:\n",
    "                plt.figure()\n",
    "                plt.plot(gamma[1])\n",
    "                plt.ylim(-0.1,1.1)\n",
    "                plt.legend(('Probability State=1'))\n",
    "                plt.xlabel('Time')\n",
    "            \n",
    "            n_levels = self.b.shape[1]\n",
    "            sumgamma = np.sum(gamma, axis=1)\n",
    "            for lev in xrange(n_levels):\n",
    "                ix = obs == lev\n",
    "                new_b.T[lev] = np.sum(gamma[:, ix], axis=1) / sumgamma\n",
    "\n",
    "            self.pi[...] = new_pi\n",
    "            self.a[...] = new_a\n",
    "            self.b[...] = new_b\n",
    "        print(\"After P(obs) = %f\" % self.obs_probability(obs))\n",
    "\n",
    "N = 2\n",
    "M = 3\n",
    "pi = np.array([0.6, 0.4])\n",
    "a = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "b = np.array([[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]])\n",
    "obs = np.array([0, 1, 2])\n",
    "\n",
    "hmm = HMM(a, b, pi)\n",
    "fwd = hmm.forward(obs)\n",
    "bwd = hmm.backward(obs)\n",
    "posterior = fwd * bwd / np.sum(fwd[-1])\n",
    "# print fwd\n",
    "# print bwd\n",
    "# print posterior\n",
    "# print hmm.obs_probability([2])\n",
    "hmm.viterbi(obs)\n",
    "hmm.update([0, 0, 0], n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print hmm.a\n",
    "print np.sum(hmm.a, axis=1)\n",
    "print hmm.b\n",
    "print np.sum(hmm.b, axis=1)\n",
    "print hmm.pi\n",
    "print np.sum(hmm.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous time Baum-Welch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "function res = BaumWelch(x0)\n",
    "  N=100; %Number of points\n",
    "  S = textread(’ProcessedTrace.txt’);\n",
    "  S = S(1:N,1:2);\n",
    "  Times = S(:,1);\n",
    "  Obs = S(:,2);\n",
    "\n",
    "  %%%%%% initialization of parameters %%%%%%%%%%%%%%\n",
    "  a=x0(1);\n",
    "  b=x0(2);\n",
    "  Q = [[-a a];[b -b]];\n",
    "  nu = [1/2 1/2];\n",
    "  G = [nu ; nu];\n",
    "  alpha = ones(N,2);\n",
    "  beta = alpha;\n",
    "  phi = zeros(N-1,4);\n",
    "  Gmem = 0;\n",
    "  D=zeros(N,1);\n",
    "  options = optimset(’LargeScale’,’off’,’MaxFunEvals’, 5000);\n",
    "  \n",
    "  %%%%%% Baum-Welch algorithm\n",
    "  Nits = 0;\n",
    "  while (norm(G-Gmem)>10^(-5) || Nits == 0) %%%% STOP condition\n",
    "    Gmem = G\n",
    "    \n",
    "    %*********** Update the alphas *************%\n",
    "    alpha(1,:) = [nu(1)*G(1,Obs(1)) nu(2)*G(2,Obs(1))];\n",
    "    alpha(1,:) = alpha(1,:)/sum(alpha(1,:));\n",
    "    for t= 2:N\n",
    "      E = expm((Times(t)-Times(t-1))*Q);\n",
    "      for j=1:2\n",
    "        alpha(t,j) = G(j,Obs(t))*E(1,j)*alpha(t-1,1) + G(j,Obs(t))*E(2,j)*alpha(t-1,2);\n",
    "      end\n",
    "      D(t) = sum(alpha(t,:));\n",
    "      alpha(t,:) = alpha(t,:)/D(t);\n",
    "    end\n",
    "\n",
    "    %*********** Update the betas *************%\n",
    "    for t=1:N-1\n",
    "      t1 = N-t;\n",
    "      E = expm((Times(t1+1)-Times(t1))*Q);\n",
    "      for j=1:2\n",
    "        beta(t1,j) = G(1,Obs(t1+1))*E(j,1)*beta(t1+1,1) + G(2,Obs(t1+1))*E(j,2)*beta(t1+1,2);\n",
    "      end\n",
    "      beta(t1,:) = beta(t1,:)/D(t1+1);\n",
    "    end\n",
    "    beta;\n",
    "\n",
    "    %*********** Update the phis *************%\n",
    "    for t=1:N-1\n",
    "      E = expm((Times(t+1)-Times(t))*Q);\n",
    "      phi(t,1) = E(1,1)*alpha(t,1)*G(1,Obs(t+1))*beta(t+1,1)/D(t+1);\n",
    "      phi(t,2) = E(1,2)*alpha(t,1)*G(2,Obs(t+1))*beta(t+1,2)/D(t+1);\n",
    "      phi(t,3) = E(2,1)*alpha(t,2)*G(1,Obs(t+1))*beta(t+1,1)/D(t+1);\n",
    "      phi(t,4) = E(2,2)*alpha(t,2)*G(2,Obs(t+1))*beta(t+1,2)/D(t+1);\n",
    "    end\n",
    "    phi;\n",
    "\n",
    "    %\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\Update the parameters //////////////////////////%\n",
    " \n",
    "    %***************Updating the initial distribution nu *******************%\n",
    "    nu(1) = alpha(1,1)*beta(1,1) / (alpha(1,1)*beta(1,1) + alpha(1,2)*beta(1,2)) ;\n",
    "    nu(2) = alpha(1,2)*beta(1,2) / (alpha(1,1)*beta(1,1) + alpha(1,2)*beta(1,2)) ;\n",
    "   \n",
    "    %*************** Updating the G matrix ****************************%\n",
    "    for i=1:2\n",
    "      for j = 1:2\n",
    "        temp1 = 0;\n",
    "        temp2 = 0;\n",
    "        for k = 1:N\n",
    "          if (Obs(k)) == j\n",
    "            temp1 = temp1 + alpha(k,i)*beta(k,i);\n",
    "          end\n",
    "          temp2 = temp2 + alpha(k,i)*beta(k,i);\n",
    "        end\n",
    "        G(i,j) = temp1/temp2;\n",
    "      end\n",
    "    end\n",
    "\n",
    "    %**************** Updating the Generator matrix Q********************%\n",
    "    s = fsolve(@myfun, [a b], options)\n",
    "    a=s(1);\n",
    "    b=s(2);\n",
    "    Q = [[-a a];[b -b]]\n",
    "    Nits = Nits + 1\n",
    "  end\n",
    "\n",
    "  %**************** Definition of Equations********************%\n",
    "  function F = myfun(x)\n",
    "    F1 = 0;\n",
    "    F2 = 0;\n",
    "    for i=1:N-1\n",
    "      d = Times(i+1)-Times(i);\n",
    "      e = exp(-(x(1)+x(2))*d);\n",
    "      F1 = F1 + phi(i,1) * ((1-d*x(1)- x(1)/(x(1)+x(2)))*e - x(2)/(x(1)+x(2)))/(x(2)+x(1)*e) +\n",
    "                phi(i,2) * ((d*x(1)-1+ x(1)/(x(1)+x(2)))*e - x(1)/(x(1)+x(2))+1)/(x(1)-x(1)*e) +\n",
    "                phi(i,3)*((d*x(2)+ x(2)/(x(1)+x(2)))*e - x(2)/(x(1)+x(2)))/(x(2)-x(2)*e) +\n",
    "                phi(i,4)*((-d*x(2)- x(2)/(x(1)+x(2)))*e - x(1)/(x(1)+x(2))+1)/(x(1)+x(2)*e);\n",
    "      F2 = F2 + phi(i,1) * ((-x(1)/(x(1)+x(2))-d*x(1))*e - x(2)/(x(1)+x(2))+1)/(x(2)+x(1)*e) +\n",
    "                phi(i,2) * ((d*x(1)+ x(1)/(x(1)+x(2)))*e - x(1)/(x(1)+x(2)))/(x(1)-x(1)*e) +\n",
    "                phi(i,3)*((d*x(2)+ x(2)/(x(1)+x(2))-1)*e - x(2)/(x(1)+x(2))+1)/(x(2)-x(2)*e) +\n",
    "                phi(i,4)*((1-x(2)/(x(1)+x(2))-d*x(2))*e - x(1)/(x(1)+x(2)))/(x(1)+x(2)*e);\n",
    "    end\n",
    "    F=[F1 F2];\n",
    "  end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMs for syllable recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a gesture score\n",
    "dt = 0.02\n",
    "gs = skspeech.vtl.parse_ges('ges-de-cvc/das.ges')\n",
    "print gs.t_end\n",
    "traj = gs.trajectory(dt=dt)\n",
    "# For dot products, we change this slighty\n",
    "# so that non-gestures are -1, gestures are 1\n",
    "traj[traj > 0] = 2.\n",
    "traj -= 1.\n",
    "\n",
    "plt.pcolormesh(traj.T)\n",
    "plt.colorbar()\n",
    "traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make an HMM for this syllable\n",
    "\n",
    "# Make a state for each distinct portion of the trajectory\n",
    "onoff = np.diff(np.vstack([traj, np.zeros(traj.shape[1])]), axis=0)\n",
    "\n",
    "# Find all the places where transitions happen\n",
    "transitions = np.unique(np.nonzero(onoff)[0])\n",
    "# transitions 1 timestep long are erroneous (data problem)\n",
    "ix = np.diff(np.hstack([transitions, transitions[-1] + 2])) > 1\n",
    "transitions = transitions[ix]\n",
    "n_states = transitions.size\n",
    "\n",
    "# The VTG for each state is the observation.\n",
    "# We store the actual VTG for that state,\n",
    "# as we will take the dot product to find\n",
    "# the similarity.\n",
    "vtgs = traj[transitions-1]  # Subtract 1 to get VTG before transition\n",
    "\n",
    "# We also add a \"null\" observation for VTGs\n",
    "# not matching those useful for the syllable.\n",
    "# NB! This is probably important.\n",
    "# It should be greater than the other VTGs for\n",
    "# any VTGs that we don't care about.\n",
    "# XXX try with this\n",
    "#null_vtg = -np.ones(vtgs.shape[1])\n",
    "#vtgs = np.vstack([vtgs, null_vtg])\n",
    "plt.pcolormesh(vtgs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a quick test, let's find the normalized\n",
    "# dot product of the VTGs across the trajectory.\n",
    "similarity = np.dot(traj, vtgs.T) / vtgs.shape[1]\n",
    "\n",
    "plt.pcolormesh(traj.T)\n",
    "plt.figure()\n",
    "plt.plot(similarity)\n",
    "plt.legend(np.arange(vtgs.shape[1]))\n",
    "plt.figure()\n",
    "plt.plot(similarity.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a simple left-right Markov process.\n",
    "# The probability to stay in the current state is\n",
    "# how long the state is proportional to the syllable;\n",
    "# otherwise, it transitions to the next state.\n",
    "p_stay = np.hstack([transitions[0], np.diff(transitions)]) / float(traj.shape[0])\n",
    "a = np.zeros((n_states, n_states))\n",
    "di_0, di_1 = np.diag_indices(n_states)\n",
    "a[di_0, di_1] = p_stay\n",
    "a[di_0, np.roll(di_1, -1)] = 1 - p_stay\n",
    "print a\n",
    "\n",
    "# The probabilitity of an observation\n",
    "# is 1 for the observations we want.\n",
    "b = np.identity(n_states)\n",
    "# b = np.hstack([b, np.zeros(n_states)[:, np.newaxis]])\n",
    "print b\n",
    "\n",
    "# We always start in the first state\n",
    "pi = np.zeros(n_states)\n",
    "pi[0] = 1.\n",
    "print pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the HMM, and see how likely our input sequence is\n",
    "hmm = HMM(a, b, pi)\n",
    "obs = np.argmax(similarity, axis=1)\n",
    "print obs\n",
    "hmm.update(obs, n_iters=10000)\n",
    "fwd = hmm.forward(obs)\n",
    "print fwd[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print hmm.a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
