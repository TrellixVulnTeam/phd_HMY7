\chapter{Background}

??? this thesis draws upon and makes contributions
to what would traditionally be thought of a disparate fields:
phonetics and phonology from linguistics,
psychoacoustics from psychology,
knowledge representation
and artificial neural networks from computer science,
spiking neural networks and
synaptic plasticity rules from computational neuroscience,
dynamical systems and control theory
from systems design engineering,
concretely implemented in software
using principles from software engineering.
As such, not all relevant background can
be given equal coverage.
In this chapter, we assume
that the reader's background
in in computer science or engineering.
We therefore spend more time
reviewing background in linguistics
and neuroscience as they fall outside
of the domain of computer science and engineering,
and so can be considered the problem domain
that we will apply our assumed knowledge to.
???yikes reword

First, we review relevant background
for the problem domain of interest,
human speech,
which we organize
into background relevant to speech recognition
and background relevant to speech production.
Then, we review existing models
of speech recognition and production,
focusing on those using
biologically inspired or constrained methods,
and concluding with other integrated approaches
that have incorporated speech recognition
and production in a single system.

\section{Human speech recognition}

We limit our coverage
of human speech recognition
to the topics drawn upon
in the model that will be presented
in Chapters~???:
basic ear physiology,
psychoacoustics,
and neurobiology.
Basic ear physiology is necessary
foundation for understanding
human speech recognition.
Psychoacoustics provides a quantitative account
of how the human auditory system
responds to incoming air pressure levels.
As such, it provides a method to verify
that our model responds to sounds
as humans do.
A primary assumption behind this work
is that the human auditory system
has evolved to be adept at processing speech;
psychoacoustics describes many of the ways
in which the human auditory system
manipulates sound,
which we aim to reproduce.???ugh this needs work
Auditory neurobiology describes both
the physical substrate and organization
of the system that we aim to emulate.
We do not aim to fully replicate
biological neurons,
but by using a simple approximation
of biological neurons,
we constrain our algorithmic choices
to those that could be implemented
in a biological system.
Similarly, by examining the organization
of the auditory brain structures,
we constrain the space of possible
network topologies
to those that match
a network that we know to be successful.

\subsection{Ear physiology}

The human ear transduces fluctuations in
air pressure level to neural signals
that we interpret as sounds.
It does this by mechanically
separating the air pressure level fluctuations
into instantaneous frequency components,
which is the basic representation
that the brain receives.

??? basic physio figure

The outer ear (pinna) funnels air
into the ear canal.
Functionally, it selectively boosts
air pressure fluctuations at
around 3 kHz (see Figure~???),
and modifies the air pressure wave
such that the directionality
of sound can be determined.

??? outer / middle ear transfer function

In the middle ear,
air pressure fluctuations
cause the eardrum (tympanic membrane)
to vibrate;
these miniscule vibrations
cause three tiny bones,
the malleus, incus, and stapes to move.
The stapes sits on the oval window
of the cochlea,
such that when the eardrum vibrates,
the stapes moves in and out of the oval window,
causing disturbances in the liquid
in the cochlea
(see Figure~???).

??? middle and inner ear figure

The inner ear transduces vibrations
of the liquid in the cochlea
to electrical signals transmitted
to the brain.
As liquid in the cochlea
moves past the basilar membrane,
it causes it to deflect
based on the width and thickness
of the membrane at that point
in the cochlea.
The basilar membrane
is shaped such that
the base of the membrane
(near the stapes)
is deflected when
incoming vibrations have power
at high frequencies,
up to 20,000 Hz.
The membrane becomes sensitive
to lower and lower frequencies
going further down the membrane
to the apex
in the center of the cochlea,
which is deflected when incoming vibrations
have power as low as 20 Hz.

The basilar membrane's surface is covered
with hair cells that transduce
the deflections of the membrane
to electric current.
Stereocilia on the top of the hair cell
rests on or near the tectorial membrane
on the outside of the cochlea.
When the basilar membrane deflects,
the stereocilia's orientation
relative to the tectorial membrane changes,
which mechanically opens receptors
on top of the stereocilia,
allowing positive charged ions
(mostly potassium and calcium)
to enter the hair cell.
Inner hair cells synapse with
spiral ganglion cells,
which accumulate the continuously
changing voltage in the inner hair cells
and send action potentials
down the auditory nerve
conveying how much power
is present at the current moment
at the frequency characteristic
of that section of the basilar membrane.
Outer hair cells play a more nuanced
role in transduction.
Their activity is tuned
more broadly in both space and time,
and seems to act as a dynamic amplifier,
amplifying quiet sounds and
attenuating loud sounds.
This dynamic amplification allows
the human ear to have a wide dynamic range,
able to safely hear sounds from 0--130 dB,
which represents 13 orders of magnitude
of absolute air pressure.

\subsection{Psychoacoustics}

??? We'll use psychoacoustics as a means of testing auditory
periphery models...?

Everest book:

- Equal loudness curves (fig 3-6, cite original); phon
  - Threshold of hearing and threshold of feeling
  % https://en.wikipedia.org/wiki/Equal-loudness_contour

- Subjective loudness (sone): doubles every 6 or 10 dB.
  fig 3-9; look at Zwicker et al % http://scitation.aip.org/content/asa/journal/jasa/29/5/10.1121/1.1908963

- Bandwidth effects; explain fig 3-10, 3-11, cite Moore Glasberg
  % http://scitation.aip.org/content/asa/journal/jasa/74/3/10.1121/1.389861

- Short sounds are harder to hear; problem for consonants (fig 3-12)

- Pitch (mel) vs. frequency (Hz): pitch depends on SPL (volume);
  7 degrees of loudness and 7 degrees of pitch (!! Nice !!)

- Timbre: an effect of the frequency components in the sound;
  you hear the fundamental frequency in different musical instruments,
  but the details are different. That's timbre.

- Ignoring everything to do with sound localization

- Temporal integration: ``The rate of presentation of the still
  pictures is important; there must be at least 16 pictures per second
  (62-millisecond interval) to avoid seeing a series of still pictures
  or a flicker. Auditory fusion works best during the first 20 or 30
  milliseconds; beyond 50 to 80 milliseconds discrete echoes dominate.''
  fig 3-19. There's more echo stuff, but we don't care.

- Auditory masking
  ??? Moore 1987
  Sounds make otherwise audible sounds inaudible.
  Across freq ranges; a 1 kHz sound can mask a quieter 1.1 kHz sound,
  but in general frequencies are independent
  (frequency resolution / frequency selectivity).
  If they're close enough to be perceived together,
  they're in the same critical bandwidth.
  Can mask sound immediately preceeding masker (backward masking)
  or immediate following (forward masking).
  % https://en.wikipedia.org/wiki/Auditory_masking

Kollmeier, Brand \& Meyer 2008: best intro

- ``Psychoacoustics is the scientific discipline that
  measures and models the relation between physical
  acoustic quantities (e.g., the intensity of a sinusoidal
  stimulus specified by sound pressure level, frequency,
  and duration) and their respective subjective impression
  (e.g., loudness, pitch, and perceived temporal extent).''

- total loudness: read pg. 64 closely;
  need a 200ms integrator to calculate loudness

- Explain ERB (equivalent rectangular bandwidth)
  and critical band

- p. 67, Kollmeier: expliain modulation filterbank,
  as this seems like it is an important part of the model...

It's unclear if these weird psychoacoustical effects
are beneficial or detrimental to speech perception.
Could be due to limitations of our faulty biological tools,
or maybe the result of evolutionary tuning
to important sounds like speech.
We are mostly agnostic to the reasons,
but feel that speech science benefits from
automated recognition and synthesis systems
that respect biological constraints,
as testable predictions can be made.

\subsection{Neurobiology}

??? Tonotopic organization

\section{Human speech production}

We limit our coverage
of human speech production
to the topics drawn upon
in the model that will be presented
in Chapters~???:
basic vocal tract physiology,
phonetics and phonology,
and neurobiology.
Phonetics and phonology
provide insight into the basic units of speech.
While a full conversational system
would draw upon all areas of linguistics,
we believe that phonetics and phonology
provides the most relevant insights
when assessing speech from the level of
incoming air pressure levels.

\subsection{Vocal tract physiology}

The human vocal tract performs
the opposite role,
translating electrical brain activity
into muscle activations
which cause fluctuations in air pressure.

??? Vocal tract figure

The vocal tract consists of
the layrngeal cavity,
the pharynx, the oral cavity,
and the nasal cavity
(see Figure~???).
Air expelled from the lungs
is transformed
by these four structures
in turn to form the
specific air pressure fluctuations
that we interpret as speech.
Major changes to airflow
occur in the layrngeal cavity,
which contains the glottis.
The glottis is made up of
the vocal folds,
which are muscles that are able to vibrate rapidly,
and the opening between the vocal folds.
When the glottis is completely open,
air passes through mostly undisturbed,
resulting in low-frequency breathy sounds.
When the glottis narrows,
air passes through more turbulently,
resulting in higher frequency breathy sounds,
as in the /h???/ phoneme
in the English word /had???/.
In all voiced phonemes (e.g., all vowels),
the vocal folds vibrate,
resulting in a ``buzzing'' quality;
compare voicing ``ah'' to
forcing air out of your mouth
in the same vocal tract shape.
The exact action of the glottis
is responsible for many
of the subtleties in human speech,
such as intensity (speaking versus shouting),
frequency (low versus high pitch),
and quality (e.g., harsh, breathy, murmured, creaky).

The pharynx, oral cavity, and nasal cavity
can be moved into many different shapes
by the muscles of the vocal tract.
The shape of these remaining portions
of the vocal tract cause further turbulence
of the air passing through,
resulting in specific patterns
that we interpret as phonemes.
The areas of the vocal tract
that can be moved in order to
effect a linguistically relevant sound
are called articulators.
As will be discussed further
in subsequent sections,
the positions of these articulators
relative to each other
determines the phoneme
that will be voiced
when air passes through
the vocal tract.
The seven articulators are
as follows.

\begin{enumerate}
\item The \textbf{pharynx} is a tube at the back of the throat.
  It carries air from the larynx to the oral and nasal cavities.
\item The \textbf{velum} or soft palate permits or restricts
  access to the nasal cavity. It is also a constriction target
  for the tongue, as in the phonemes /k???/ and /g???/.
\item The \textbf{hard palate} is the hard surface
  at the roof of the mouth. It cannot move, but serves
  as a constriction target for the tongue.
\item The \textbf{alveolar ridge} is the area between
  the hard palate and the top front teeth. Like the hard palate,
  it serves as a constriction target for the tongue.
\item The upper and lower \textbf{teeth} are at the front of the mouth.
  While they primarily serve as a constriction target,
  the lower teeth are under muscular control,
  though few sounds utilize the motion of the lower teeth.
\item The \textbf{tongue} is a large, flexible, muscular structure
  that can reach several different constriction targets.
  The tongue is often divided into the tip, blade, dorsum, and root,
  though there are no clear dividing lines between these areas.
\item The \textbf{lips} are another important flexible, muscular
  articulator that is involved in many speech sounds.
  The lips can move toward other articulators to constrict airflow,
  or can become rounded to change the overall quality of a sound.
\end{enumerate}

Other parts of the vocal tract are important to speech;
for example, some sounds require air
to pass through the nasal cavity.
However, air is routed to the nasal cavity
through constrictions of the seven articulators above;
there are few little linguistically relevant
movements that can be done with the nose.
Exact definitions of the articulators
do vary between phoneticians,
and play an important role
we later describe articulatory speech synthesizers
in Section~???.

\subsection{Phonetics and phonology}

???brief intro, explain the difference
between phonetics and phonology
% http://www.phon.ox.ac.uk/jcoleman/PHONOLOGY1.htm
???include phonemic vs phonetic transcription
??? - phonetics: describing the sounds we use in speaking.
  Relating to the way we produce them and the way they sound.
  phonology: more abstract parts of sounds; look at restrictions
  and regularities in a language by studying syllables;
  suprasegmental parts (i.e., prosody:
  things that extend over several segments (phonemes)).
  - phonemic transcription: just the phonemes. Phonetic transcription:
    includes diacritics and other marks to differentiate allophones.

Our treatment of phonetics and phonology
focuses on the hierarchical levels
in human linguistic expression,
paying particular attention to two
sets of characteristics in each level
and in each mapping between two levels:
temporal patterns and constraints,
and the existence of rule-like regularities.
For example,
if our model were to deal with sentences,
it would be important to know that,
temporally, sentences occur sequentially,
and cannot co-occur.
There are grammatical rules
about what words can be in a sentence,
and in what order they can be arranged,
but there are few rules placing constraints
between sentences---any sentence \textit{could}
follow any other sentence,
though there may be semantic issues.
Similar constraints exist at
the lower levels of language that
phonetics and phonology examine,
and these constraints will
inform the structure of the model
we present in Chapter~???ref.

The specific hierarchy that we will
present in this section is pictured
in Figure~???ref.
We will arbitrarily begin at
the lowest (rightmost???) level and work upwards
(though a presentation starting at
the highest level would be equally valid).
For clarity, we will use English for examples,
except when demonstrating that a particular
quality differs across languages.\footnote{
  When not explicitly cited,
  the information in Section~???ref
  should be considered
  basic phonetics and phonology
  that would be covered in an
  introductory undergraduate level
  class or textbook
  (I used ???refRoach as a general reference).
}\footnote{The International Phonetic Alphabet (IPA)
  will be used this and subsequent sections.
  See ???typography for more details.}

% Things I didn't put in but maybe could
% - Pronunciation is way different in connected speech versus
%   careful pronunciation in isolation. E.g.,
%   Assimilation: phonemes in a syllable change due to the phoneme
%   directly after or before.
%   Elision: Sometimes sounds disappear; deleted phonemes.
% - Rhythm: Stress-timing: English is stress timed?
%   Evidence isn't strong.
%   French and others are syllable-timed.
%   Foot: a unit of rhythm. Starts with stressed syllable, includes
%   all other syllables until the next stress.

\subsubsection{Vocal tract gestures}

According to many articulatory phoneticians,
the fundamental unit of speech production
is the ``gesture,''
which describes some movement
of the articulators in the vocal tract
??? Browman \& Goldstein.
Gestures are pre-linguistic,
in that a vocal tract gesture itself
does not necessarily produce
a linguistically interesting sound;
a set of possibly overlapping
vocal tract gesture can produce a phoneme,
which is linguistically relevant.
There is evidence that children
learning to say their first words
often use the correct gestures,
but take time to order them correctly
to produce the desired utterance,
indicating that gestures develop
before language,
and are leveraged when
developing language
???refBrowmanGoldstein.

Gestures are stereotypical movements
of sets of articulators in the vocal tract.
Each gesture has its own temporal dynamics;
that is, a gesture is a function defined
over space (what muscles are contracted)
and time (when contractions occur).
In general, a gesture can be thought of
as a point attractor
in which the position
of one or more vocal tract
articulators is parameterized.
A minimal gesture is parameterized by
the set of articulators engaged
by that gesture,
and the degree to which
the articulators are constricted;
for example,
widening and narrowing the velic aperture
is a gesture consisting of
the articulator involved
(the velum)
and degree to which it is constricted
(low degree for narrow, high degree for wide).
Other gestures require constriction location
and constriction shape parameters
to disambiguate them from other similar gestures;
for example, the tongue tip
is involved in gestures
that depend on location
(e.g., constriction at
the teeth, alveolar ridge, or hard palate)
and shape
(e.g., tongue tip can be straight
or curved backwards).
All gestures are also affected by
global parameters affecting all gestures,
most notably ``stiffness,''
which defines how quickly
the gesture attempts to reach
the point of attraction.

Gestures are rarely discussed in isolation
as phonemes only occur when certain
sets of gestures overlap
or occur in tight succession.
Therefore, gestures are often
grouped into gestural scores
(see Figure~???)
which embody the spatiotemporal
coordination of several gestures
to produce a linguistically
relevant sound.
The vertical axis of a gestural score
is the articulator set,
as although one articulator
can produce multiple gestures,
it can only produce one gesture
at a time.
The horizontal axis of
a gestural score represents time,
though the time axis may be
stretched depending on
the global parameter, stiffness.
Gestural scores can also
be visualized as a graph,
as in Figure~???,
which highlights that
certain gestures must co-occur
or have tight temporal couplings
such that one gesture
must start as another gesture ends.

??? Fig: gestural score

As will be discussed in detail
in Section~???, there is a mapping
between gestures and vocal tract articulators;
in human speech, the mapping links
gestures and vocal tract muscle activations.
In models of human speech,
the mapping depends on what articulators
are available in the articulatory synthesizer,
and how those articulators can be manipulated.
Unfortunately, few models make a clear distinction
between the set of gestures
and the mapping from gestures
to articulator trajectories,
resulting in there being
no agreed upon set of vocal tract gestures
in human speech.
Similarly, several approaches to
temporal sequencing of gestures
have resulted in different
definitions of gesture sets,
with different graph structures as
the one shown in Figure~???.
We will discuss our interpretation
of gestures and gesture sets
in Section~???.
% Development of better human vocal tract models
% will help researchers
% determine a complete set of
% vocal tract gestures.

??? In a sense, vocal tract gestures
can be thought of a subset of all
motor synergies ???cite

\subsubsection{Phonemes}

The smallest linguistically relevant unit
is the phoneme, which describes
a short sound produced by one or more
vocal tract gestures
on the order of tens of milliseconds.
Phonemes are noted by their ability
to change the meaning of some word
when swapped in speech;
for example, the only difference between
the word ``bad'' and ``mad'' is the
consonant sound at the beginning,
and therefore those two consonant sounds
are each separate phonemes.
Even though different individuals
voice each phoneme differently,
the important quality is that
the particular sound is recognized
as a particular phoneme
in the context of an utterance.
A helpful analogy can be made between
phonemes in speech
and alphabetical letters in writing.
With no knowledge of English,
seeing the sentence,
``A bird has a wing,''
one might think that ``A'' and ``a''
are different letters.
However, with enough examples,
one could surmise that in every situation
where ``A'' is used, it appears at the
start of a sequence of letters,
and it could have instead
been replaced by ``a''
had it not appeared at the start of the sequence.
Therefore, ``A'' and ``a'' represent
the same underlying ``letter.''
Similarly, despite individual differences
in the pitch, speed, volume, and quality
of how one voices a particular phoneme,
that phoneme is still considered the same
if it plays the same role
in a linguistically relevant sequence of phonemes.

Generally, there are two types of phonemes:
vowels and consonants.
Vowels are longer sounds
made when the vocal tract is mostly open.
Consonants are shorter sounds
made when some part of the vocal tract
is constricted or transiently closed.
In almost all languages,
there are more consonant phonemes
than there are vowel phonemes,
though pronunciation varies significantly
between dialects,
and transcribing the full set of
phonemes in a language is
not a purely objective exercise.
For example, while most dialects
of English recognize 24 consonant phonemes,
General American English has been transcribed
as having 16 vowel phonemes ???citeWikipedia?,
while Received Pronunciation English
has been transcribed as having 25 vowel phonemes
???citeWells1982.

Vowel phonemes occur when air is freely
moving through the open vocal tract.
The shape of the vocal tract determines
the quality of the sound.
Three factors influence vowel vocal tract shape:
openness, backness, and roundedness;
openness refers to the general position
of the jaw (open or closed) and tongue (low or high),
backness refers to the position of the
tongue relative to the back of the mouth,
and roundedness refers to
whether the lips are rounded.
Roundedness can vary independently
of the other factors;
therefore, each vowel sound has a rounded
and unrounded variant.
Openness and backness are partially coupled,
such that when the vocal tract is open,
it must be mostly (but not completely) back.
The three possible extremes, then,
are /a???ipa/ (open, back),
/i???/ (closed, front),
and /u???/ (closed, back).
Most of the remaining vowel sounds
can be expressed as being
a blend of one of these three vowel sounds
(see ???fig for a visualization).
Pure vowel phonemes (also called monophthongs)
occur when the glottis phonates
and the vocal tract stays
in one of the positions already described.
Diphthongs phonemes occur
when the vocal tract
transitions or ``glides''
from one vocal tract position
to a second vocal tract position
during phonation;
e.g., the English ???
in ??? is a diphthong.
Triphthong phonemes,
in which three vocal tract positions
are visited in sequence,
also occur in some languages;
e.g., in RP English
the word ???
is often voiced with the
triphthong ???.

??? vowel figure

Consonant phonemes occur when some point
of the vocal tract is constricted.
The place and manner of constriction
determines the consonant that will be uttered.
Place refers to the location in the vocal tract
that becomes constricted.
For example, in bilabial consonants,
both lips come together,
as in /m???/ and /b???/;
in velar consonants,
the tongue moves toward the velum
(i.e., soft palate),
as in /k???/ and /g???/.
Manner refers to how the vocal tract
is constricted in that location.
For example, in a plosive,
the vocal tract is completely closed
at the place of articulation;
air compresses behind the place of constriction,
and is then released,
producing the sound recognized
as a plosive, like /t???/ and /k???/.
In fricatives,
the articulators move close together
such that there is a narrow channel
for air to pass through.
The narrow channel causes
the hissing sounds associated
with phonemes like /s???/ and /f???/.
Several other places and manners exist;
those defined by the IPA are shown in Figure~???.

??? consonant figure

While no phoneme's pronunciation is consistent,
consonant pronunciations can vary more than vowels
because consonant sounds are produced in the context
of the vocal tract position for
the prior or upcoming vowel sound.
In the word /bat???/ for example,
the /b???/ and /t???/ sounds
occur in the context of /a???/.
???kroger showed that
each consonant sound is composed
of speech gestures which force some
articulators to change in a particular way,
but allow other articulators
to change freely.

The vowels and consonants described so far
represent the most common phonemes used
in daily speech.
However, as with most aspects
of phonetics and phonology,
there are many examples that do not
match the convenient criteria listed above.
For example, /w???/, as in ``weep''
is a phoneme consonant that is articulated
with a mostly open vocal tract,
like a vowel;
for this reason, it is sometimes called a semivowel.
Some languages use clicks,
in which inward suction releases a complete constriction
resulting in a loud consonantal sound.
However, we will not simulate
these and other phonemes
that are either uncommon or not present in English;
instead, we will state when a part of our model
will require further work
in order to handle these phonemes,
and when the model can be easily
adapted to handle these phonemes.

\subsubsection{Syllables}

Syllables are groups of one or more phonemes
with a well-specified structure.
They consist of a loud vocalic center,
with optional quieter consonantal components
before and after the center
Unlike other levels of organization,
the phonemes in a syllable
may not be strictly sequential;
some phonemes may co-occur
when voicing a syllable.

The phonemes that make up a syllable
are typically grouped into
the onset and rime,
where the onset is a cluster of
consonant phonemes,
and the rime is a vowel phoneme
(called the nucleus)
and an optional cluster of consonant
phonemes (called the coda).
In English, the onset is also optional,
though this is not true in all languages.
Figure~??? summarizes this grouping.
Note that other ways to describe
syllables exist
(cf. ???mora, Chinese model),
but we adopt this method
as it is the most common
(???most widely accepted).

??? syllable structure figure

Phonetically, syllables are a useful
level of organization because
the higher-level aspects
of an utterance---rhythm, prosody and stress,
for example---are easier to analyze
in terms of their effects on
sequences of syllables rather than
on sequences of phonemes;
it is easy to distinguish
changes in pitch and volume
within and between two syllables,
but not between two phonemes,
because they often occur too quickly.

One aspect of the stress of an utterance
is the weight of a syllable.
``Heavy'' or ``strong'' syllables
have a branching rime,
or a branching nucleus,
meaning that they
end in a consonant,
or contain a long vowel or diphthong,
respectively.
``Light'' or ``weak'' syllables
do not have branching rime or nucleus,
and in general are shorter
and quieter.
In English, most vowel phonemes
can only appear in either
heavy or light syllables;
for example, the schwa, /e???/,
can only appear in light syllables,
and diphthongs like /ae???/
can only appear in heavy syllables.
The weight of a syllable determines
whether or not it can be stressed
in an utterance;
specifically, only heavy syllables
can be stressed.

Like all levels of phonetics and phonology,
the above description is a useful simplification
of a more complex phenomenon.
In terms of syllable production,
the onset-nucleus-coda grouping
is not always sufficient.
In English, syllables that may have once
been typical VC syllables have morphed
into ``syllabic consonants,'' in which
the vowel is no longer present;
for example, ``bottle'' is often
pronounced /bottl???/,
where /l???/ is a syllabic consonant.
In terms of syllable recognition,
agreement between native English speakers
is surprisingly low when
asked to segment an utterance
into its component syllables.
Yet, the meaning of the utterance
is understood by all subjects.
Therefore, while the syllable
may be a useful level of organization
for producing utterances,
its role in recognizing them
may be limited.

\subsubsection{Tone-units}

The final level of organization
that we will examine is the tone-unit.
The tone-unit allows us to examine
suprasegmental aspects of speech;
specifically, we will use tone-units
to incorporate stress and intonation
in our model.
A tone-unit is made up of a serially ordered
sequence of syllables.\footnote{Many
  phoneticians consider a tone-unit to be
  composed of ``feet,'' where a foot is
  a single unit of rhythm.
  However, feet are mostly used when describing
  non-conversational utterances,
  such as those found in music and poetry.
  In this thesis we focus on speech
  as a means of conveying linguistic information,
  and therefore ignore the concept of feet.}
In this thesis, we will consider
an utterance to be a sequence of tone-units.

The structure of a tone-unit is similar
to a syllable, except its component parts
are syllables instead of phonemes,
and its components are serially ordered.
A tone-unit must contain a tonic syllable
(sometimes also called the nucleus),
and can optionally contain one or more syllables
in a pre-head, head, or tail section.
The pre-head consists of all syllables
before the first stressed syllable
in a tone-unit.
The head consists of all syllables from
the first stressed syllable
to the tonic syllable.
The tonic syllable is the most significant
syllable in the tone-unit because
pitch changes in the tonic syllable
will occur relative to the tonic syllable;
the tonic syllable is not necessarily
the loudest or most prominently stressed
syllable in the tone-unit,
though it does always contain
a stressed (and therefore heavy) syllable.
The tail consists of all syllables
following the tonic syllable.

??? tone-unit structure figure

Not all heavy syllables are necessarily stressed;
stress is hypothesized to occur when
more muscle activation is used
to voice a particular heavy syllable
???cite?.
Stress is perceived as a heavy syllable
that is louder, longer, and with
a different pitch or quality compared
to other syllables.
In some languages, rules govern
which syllables receive stress;
for example, in French,
the last syllable in a word is
always stressed.
In English, each word defines
its own stress pattern,
and each utterance can add
additional stresses
that emphasize some words over others.
Additionally, stress is not a binary quantity;
English is typically thought to contain
three stress levels
(primary stress, secondary stress, and unstressed).
Deciphering why stress occurs
in some words and utterances
is complicated;
in this thesis,
we will aim to voice stressed
in utterances, but will not
determine where stresses should occur
given an utterance with no stress markers.

Intonation, on the other hand,
is a more straightforward phenomenon to model.
Intonation is the use of pitch changes
to 1) express emotions and attitudes,
2) impart prominence on stressed syllables,
3) exaggerate grammar and syntactic structure,
and 4) clue listeners into what information
is novel and what is thought to be already known.
While intonation can be thought to also include
body language and other prosodic characteristics,
we will focus only on pitch changes.
The tone of a tone-unit
is the overall trajectory of pitch
during the tone-unit.

There are a limited set of possible pitch trajectories
(i.e., tones) in English.
While different sources identify different trajectories,
we will adopt the conventions of ???below
which note six possible pitch trajectories:
high fall, low fall, high rise,
low rise, fall-rise, and rise-fall
(see Figure~???).
The same sequence of syllables
can change its meaning dramatically
by using a different pitch trajectory,
or by changing the position of the
tonic syllable within that pitch trajectory.
The meaning of each pitch trajectory
changes depending on the utterance in question;
we will not investigate meanings further
in this work, as we focus on
sound reproduction rather than on
linguistic meaning.

??? Quick plot of pitch trajectories

% cite https://notendur.hi.is/peturk/KENNSLA/25/IPBE/SHELL/25/nucleus.html
% or Cruttenden

It is important to note that when talking about
the pitch trajectory of an utterance,
pitch is always relative to the
upper and lower range of a particular speaker.
In the pitch trajectory plot in Figure~???,
horizontal lines show the upper and lower ranges.

These six trajectories in Figure~???
interact with the pre-head and tail
in predictable ways.
The pre-head is usually low,
but can be high in front of a low stressed syllable.
The tail trajectory can be predicted
by the trajectory of the tonic syllable;
for example, if the tonic syllable falls,
the tail remains low;
if the tonic syllable rises,
the tail continues to rise.
The head, on the other hand,
is independent of the tonic syllable;
it may remain at a high or low level,
or it can rise or fall like
the trajectories associated with the tonic syllable.
The combinations of head and tonic syllable
pitch trajectories also contributes
to the varying meanings conveyed
by the pitch trajectory of a tone-unit.

??? Look into and summarize Generative phonology

??? Look into, maybe mention Autosegmental intonation
(H = high tone, L = low tone, HL = fall, etc.)

??? Note: constraints and rules aren't necessarily all going to
be taken into account. However, we do want to be sufficiently
flexible or inflexible such that were we to take into account
all rules, we would still scale to biological limits.

Finally, one note about
the absence of ``words'' in our treatment
of phonetics and phonology.
While words are clearly an important concept
that we will use colloquially in this thesis,
they are a linguistic construct
rather than a phonological one.
However, as directly applied
to speech recognition and synthesis,
words are more readily recognized
than sequences of syllables with no meaning.
Fortunately, the pioneering work
of ???allen94 which cites an old work???
gives a direct relationship between
the recognition rates of nonsense syllables and words.
Therefore, since we have not included
higher-level linguistic effects in this model,
the recognition results we achieve
can be considered as the recognition
of nonsense syllables,
and adjusted for linguistic context effect
accordingly (see Section~??? for details).

\subsection{Neurobiology}

??? Broca's area and whatnot

\section{Prior modeling approaches}

In making early steps toward
an integrated speech recognition and synthesis system,
we are applying techniques
in artificial intelligence and control theory
to the speech domain reviewed in the previous section.
There is a long history of applying these techniques
to this domain;
in this section we review prior modeling efforts
and contrast them with the model we will describe
in future chapters.

\subsection{Auditory periphery modeling}

??? figure like izhikevic, with auditory model + efficiency?

??? summary table with phenomena captured, etc

??? Mention artificial neuromorphic cochleas

\subsection{Automatic speech recognition}

\subsection{Articulatory speech synthesis}

??? summary table with different vocal tract models

??? summary table with different acoustic models

??? in the tables, also note available implementations
(so we can justify writing our own).
Include programming language in this

??? include online vs batch in table

\subsection{Speech motor control}

??? note that many art. synths consider this part of their
synthesizer (control model).
But we will consider it separately because
it is a primary contribution of this thesis.

??? Saltzman stuff on task dynamics
is highly related to what we want to do.
But with SPA stuff on top.

??? also hosung nam's work

??? also mention near the end that people haven't
yet connected the Saltzman task dynamics stuff
to the brain; that'll be one of our contributions

\subsection{Integrated recognition and synthesis systems}

??? Review DIVA model

%% ~8-20 pages

%% - More than a literature review
%% - Organize related work - impose structure
%% - Be clear as to how previous work being described relates to your own.
%% - The reader should not be left wondering why you've described something!!
%% - Critique the existing work - Where is it strong where is it weak?
%%   What are the unreasonable/undesirable assumptions?
%% - Identify opportunities for more research (i.e., your thesis).
%%   Are there unaddressed, or more important related topics?
%% - After reading this chapter, one should understand the motivation for
%%   and importance of your thesis
%% - You should clearly and precisely define all of the key concepts
%%   dealt with in the rest of the thesis, and teach the reader what s/he
%%   needs to know to understand the rest of the thesis.
