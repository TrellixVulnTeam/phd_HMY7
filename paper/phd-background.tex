\chapter{Background}

??? this thesis draws upon and makes contributions
to what would traditionally be thought of a disparate fields:
phonetics and phonology from linguistics,
psychoacoustics from psychology,
knowledge representation
and artificial neural networks from computer science,
spiking neural networks and
synaptic plasticity rules from computational neuroscience,
dynamical systems and control theory
from systems design engineering,
concretely implemented in software
using principles from software engineering.
As such, not all relevant background can
be given equal coverage.
In this chapter, we assume
that the reader's background
in in computer science or engineering.
We therefore spend more time
reviewing background in linguistics
and neuroscience as they fall outside
of the domain of computer science and engineering,
and so can be considered the problem domain
that we will apply our assumed knowledge to.
???yikes reword

\section{Problem domain: human speech}

Human speech is studied in many disciplines
using a variety of scientific methods.
In this thesis, we limit our coverage of human speech
to the disciplines of psychoacoustics,
phonetics and phonology,
and auditory biology.

Psychoacoustics provides a quantitative account
of how the human auditory system
responds to incoming air pressure levels.
As such, it provides a method to verify
that our model responds to sounds
as humans do.
A primary assumption behind this work
is that the human auditory system
has evolved to be adept at processing speech;
psychoacoustics describes many of the ways
in which the human auditory system
manipulates sound,
which we aim to reproduce.???ugh this needs work

Phonetics and phonology
provide insight into the basic units of speech.
While a full conversational system
would draw upon all areas of linguistics,
we believe that phonetics and phonology
provides the most relevant insights
when assessing speech from the level of
incoming air pressure levels.

Auditory neurobiology describes both
the and physical substrate organization
of the system that we aim to emulate.
We do no aim to fully replicate
biological neurons,
but by using a simple approximation
of biological neurons,
we constrain our algorithmic choices
to those that could be implemented
in a biological system.
Similarly, by examining the organization
of the auditory brain structures,
we constrain the space of possible
network topologies
to those that match
a network that we know to be successful.

\subsection{Phonetics and phonology}

???brief intro, explain the difference
between phonetics and phonology
% http://www.phon.ox.ac.uk/jcoleman/PHONOLOGY1.htm
???include phonemic vs phonetic transcription
??? - phonetics: describing the sounds we use in speaking.
  Relating to the way we produce them and the way they sound.
  phonology: more abstract parts of sounds; look at restrictions
  and regularities in a language by studying syllables;
  suprasegmental parts (i.e., prosody:
  things that extend over several segments (phonemes)).
  - phonemic transcription: just the phonemes. Phonetic transcription:
    includes diacritics and other marks to differentiate allophones.

Our treatment of phonetics and phonology
focuses on the hierarchical levels
in human linguistic expression,
paying particular attention to two
sets of characteristics in each level
and in each mapping between two levels:
temporal patterns and constraints,
and the existence of rule-like regularities.
For example,
if our model were to deal with sentences,
it would be important to know that,
temporally, sentences occur sequentially,
and cannot co-occur.
There are grammatical rules
about what words can be in a sentence,
and in what order they can be arranged,
but there are few rules placing constraints
between sentences---any sentence \textit{could}
follow any other sentence,
though there may be semantic issues.
Similar constraints exist at
the lower levels of language that
phonetics and phonology examine,
and these constraints will
inform the structure of the model
we present in Chapter~???ref.

The specific hierarchy that we will
present in this section is pictured
in Figure~???ref.
We will arbitrarily begin at
the lowest (rightmost???) level and work upwards
(though a presentation starting at
the highest level would be equally valid).
For clarity, we will use English for examples,
except when demonstrating that a particular
quality differs across languages.\footnote{
  When not explicitly cited,
  the information in Section~???ref
  should be considered
  basic phonetics and phonology
  that would be covered in an
  introductory undergraduate level
  class or textbook
  (I used ???refRoach as a general reference).
}\footnote{The International Phonetic Alphabet (IPA)
  will be used this and subsequent sections.
  See ???typography for more details.}

% Things I didn't put in but maybe could
% - Pronunciation is way different in connected speech versus
%   careful pronunciation in isolation. E.g.,
%   Assimilation: phonemes in a syllable change due to the phoneme
%   directly after or before.
%   Elision: Sometimes sounds disappear; deleted phonemes.
% - Rhythm: Stress-timing: English is stress timed?
%   Evidence isn't strong.
%   French and others are syllable-timed.
%   Foot: a unit of rhythm. Starts with stressed syllable, includes
%   all other syllables until the next stress.

\subsubsection{Vocal tract gestures}

??? talk about gestures here first,
as part of articulatory phonology,
cite Bernd.

- larynx (vocal folds, glottis): big deal! differentiate between
  voiced and voiceless phonemes. Can be varied a lot
  to change air pressure, etc. Can vary:
  intensity (speaking vs shouting),
  frequency (low vs high pitch),
  quality (e.g., harsh, breathy, murmured, creaky)

\subsubsection{Phonemes}

The smallest linguistically relevant unit
is the phoneme, which describes
a short sound produced by one or more
vocal tract gestures
on the order of tens of milliseconds.
Phonemes are noted by their ability
to change the meaning of some word
when swapped in speech;
for example, the only difference between
the word ``bad'' and ``mad'' is the
consonant sound at the beginning,
and therefore those two consonant sounds
are each separate phonemes.
Even though different individuals
voice each phoneme differently,
the important quality is that
the particular sound is recognized
as a particular phoneme
in the context of an utterance.
A helpful analogy can be made between
phonemes in speech
and alphabetical letters in writing.
With no knowledge of English,
seeing the sentence,
``A bird has a wing,''
one might think that ``A'' and ``a''
are different letters.
However, with enough examples,
one could surmise that in every situation
where ``A'' is used, it appears at the
start of a sequence of letters,
and it could have instead
been replaced by ``a''
had it not appeared at the start of the sequence.
Therefore, ``A'' and ``a'' represent
the same underlying ``letter.''
Similarly, despite individual differences
in the pitch, speed, volume, and quality
of how one voices a particular phoneme,
that phoneme is still considered the same
if it plays the same role
in a linguistically relevant sequence of phonemes.

Generally, there are two types of phonemes:
vowels and consonants.
Vowels are longer sounds
made when the vocal tract is mostly open.
Consonants are shorter sounds
made when some part of the vocal tract
is constricted or transiently closed.
In almost all languages,
there are more consonant phonemes
than there are vowel phonemes,
though pronunciation varies significantly
between dialects,
and transcribing the full set of
phonemes in a language is
not a purely objective exercise.
For example, while most dialects
of English recognize 24 consonant phonemes,
General American English has been transcribed
as having 16 vowel phonemes ???citeWikipedia?,
while Received Pronunciation English
has been transcribed as having 25 vowel phonemes
???citeWells1982.

Vowel phonemes occur when air is freely
moving through the open vocal tract.
The shape of the vocal tract determines
the quality of the sound.
Three factors influence vowel vocal tract shape:
openness, backness, and roundedness;
openness refers to the general position
of the jaw (open or closed) and tongue (low or high),
backness refers to the position of the
tongue relative to the back of the mouth,
and roundedness refers to
whether the lips are rounded.
Roundedness can vary independently
of the other factors;
therefore, each vowel sound has a rounded
and unrounded variant.
Openness and backness are partially coupled,
such that when the vocal tract is open,
it must be mostly (but not completely) back.
The three possible extremes, then,
are /a???ipa/ (open, back),
/i???/ (closed, front),
and /u???/ (closed, back).
Most of the remaining vowel sounds
can be expressed as being
a blend of one of these three vowel sounds
(see ???fig for a visualization).
Pure vowel phonemes (also called monophthongs)
occur when the glottis phonates
and the vocal tract stays
in one of the positions already described.
Diphthongs phonemes occur
when the vocal tract
transitions or ``glides''
from one vocal tract position
to a second vocal tract position
during phonation;
e.g., the English ???
in ??? is a diphthong.
Triphthong phonemes,
in which three vocal tract positions
are visited in sequence,
also occur in some languages;
e.g., in RP English
the word ???
is often voiced with the
triphthong ???.

??? vowel figure

Consonant phonemes occur when some point
of the vocal tract is constricted.
The place and manner of constriction
determines the consonant that will be uttered.
Place refers to the location in the vocal tract
that becomes constricted.
For example, in bilabial consonants,
both lips come together,
as in /m???/ and /b???/;
in velar consonants,
the tongue moves toward the velum
(i.e., soft palate),
as in /k???/ and /g???/.
Manner refers to how the vocal tract
is constricted in that location.
For example, in a plosive,
the vocal tract is completely closed
at the place of articulation;
air compresses behind the place of constriction,
and is then released,
producing the sound recognized
as a plosive, like /t???/ and /k???/.
In fricatives,
the articulators move close together
such that there is a narrow channel
for air to pass through.
The narrow channel causes
the hissing sounds associated
with phonemes like /s???/ and /f???/.
Several other places and manners exist;
those defined by the IPA are shown in Figure~???.

??? consonant figure

While no phoneme's pronunciation is consistent,
consonant pronunciations can vary more than vowels
because consonant sounds are produced in the context
of the vocal tract position for
the prior or upcoming vowel sound.
In the word /bat???/ for example,
the /b???/ and /t???/ sounds
occur in the context of /a???/.
???kroger showed that
each consonant sound is composed
of speech gestures which force some
articulators to change in a particular way,
but allow other articulators
to change freely.

The vowels and consonants described so far
represent the most common phonemes used
in daily speech.
However, as with most aspects
of phonetics and phonology,
there are many examples that do not
match the convenient criteria listed above.
For example, /w???/, as in ``weep''
is a phoneme consonant that is articulated
with a mostly open vocal tract,
like a vowel;
for this reason, it is sometimes called a semivowel.
Some languages use clicks,
in which inward suction releases a complete constriction
resulting in a loud consonantal sound.
However, we will not simulate
these and other phonemes
that are either uncommon or not present in English;
instead, we will state when a part of our model
will require further work
in order to handle these phonemes,
and when the model can be easily
adapted to handle these phonemes.

\subsubsection{Syllables}

Syllables are groups of one or more phonemes
with a well-specified structure.
They consist of a loud vocalic center,
with optional quieter consonantal components
before and after the center
Unlike other levels of organization,
the phonemes in a syllable
may not be strictly sequential;
some phonemes may co-occur
when voicing a syllable.

The phonemes that make up a syllable
are typically grouped into
the onset and rime,
where the onset is a cluster of
consonant phonemes,
and the rime is a vowel phoneme
(called the nucleus)
and an optional cluster of consonant
phonemes (called the coda).
In English, the onset is also optional,
though this is not true in all languages.
Figure~??? summarizes this grouping.
Note that other ways to describe
syllables exist
(cf. ???mora, Chinese model),
but we adopt this method
as it is the most common
(???most widely accepted).

??? syllable structure figure

Phonetically, syllables are a useful
level of organization because
the higher-level aspects
of an utterance---rhythm, prosody and stress,
for example---are easier to analyze
in terms of their effects on
sequences of syllables rather than
on sequences of phonemes;
it is easy to distinguish
changes in pitch and volume
within and between two syllables,
but not between two phonemes,
because they often occur too quickly.

One aspect of the stress of an utterance
is the weight of a syllable.
``Heavy'' or ``strong'' syllables
have a branching rime,
or a branching nucleus,
meaning that they
end in a consonant,
or contain a long vowel or diphthong,
respectively.
``Light'' or ``weak'' syllables
do not have branching rime or nucleus,
and in general are shorter
and quieter.
In English, most vowel phonemes
can only appear in either
heavy or light syllables;
for example, the schwa, /e???/,
can only appear in light syllables,
and diphthongs like /ae???/
can only appear in heavy syllables.
The weight of a syllable determines
whether or not it can be stressed
in an utterance;
specifically, only heavy syllables
can be stressed.

Like all levels of phonetics and phonology,
the above description is a useful simplification
of a more complex phenomenon.
In terms of syllable production,
the onset-nucleus-coda grouping
is not always sufficient.
In English, syllables that may have once
been typical VC syllables have morphed
into ``syllabic consonants,'' in which
the vowel is no longer present;
for example, ``bottle'' is often
pronounced /bottl???/,
where /l???/ is a syllabic consonant.
In terms of syllable recognition,
agreement between native English speakers
is surprisingly low when
asked to segment an utterance
into its component syllables.
Yet, the meaning of the utterance
is understood by all subjects.
Therefore, while the syllable
may be a useful level of organization
for producing utterances,
its role in recognizing them
may be limited.

\subsubsection{Tone-units}

The final level of organization
that we will examine is the tone-unit.
The tone-unit allows us to examine
suprasegmental aspects of speech;
specifically, we will use tone-units
to incorporate stress and intonation
in our model.
A tone-unit is made up of a serially ordered
sequence of syllables.\footnote{Many
  phoneticians consider a tone-unit to be
  composed of ``feet,'' where a foot is
  a single unit of rhythm.
  However, feet are mostly used when describing
  non-conversational utterances,
  such as those found in music and poetry.
  In this thesis we focus on speech
  as a means of conveying linguistic information,
  and therefore ignore the concept of feet.}
In this thesis, we will consider
an utterance to be a sequence of tone-units.

The structure of a tone-unit is similar
to a syllable, except its component parts
are syllables instead of phonemes,
and its components are serially ordered.
A tone-unit must contain a tonic syllable
(sometimes also called the nucleus),
and can optionally contain one or more syllables
in a pre-head, head, or tail section.
The pre-head consists of all syllables
before the first stressed syllable
in a tone-unit.
The head consists of all syllables from
the first stressed syllable
to the tonic syllable.
The tonic syllable is the most significant
syllable in the tone-unit because
pitch changes in the tonic syllable
will occur relative to the tonic syllable;
the tonic syllable is not necessarily
the loudest or most prominently stressed
syllable in the tone-unit,
though it does always contain
a stressed (and therefore heavy) syllable.
The tail consists of all syllables
following the tonic syllable.

??? tone-unit structure figure

Not all heavy syllables are necessarily stressed;
stress is hypothesized to occur when
more muscle activation is used
to voice a particular heavy syllable
???cite?.
Stress is perceived as a heavy syllable
that is louder, longer, and with
a different pitch or quality compared
to other syllables.
In some languages, rules govern
which syllables receive stress;
for example, in French,
the last syllable in a word is
always stressed.
In English, each word defines
its own stress pattern,
and each utterance can add
additional stresses
that emphasize some words over others.
Additionally, stress is not a binary quantity;
English is typically thought to contain
three stress levels
(primary stress, secondary stress, and unstressed).
Deciphering why stress occurs
in some words and utterances
is complicated;
in this thesis,
we will aim to voice stressed
in utterances, but will not
determine where stresses should occur
given an utterance with no stress markers.

Intonation, on the other hand,
is a more straightforward phenomenon to model.
Intonation is the use of pitch changes
to 1) express emotions and attitudes,
2) impart prominence on stressed syllables,
3) exaggerate grammar and syntactic structure,
and 4) clue listeners into what information
is novel and what is thought to be already known.
While intonation can be thought to also include
body language and other prosodic characteristics,
we will focus only on pitch changes.
The tone of a tone-unit
is the overall trajectory of pitch
during the tone-unit.

There are a limited set of possible pitch trajectories
(i.e., tones) in English.
While different sources identify different trajectories,
we will adopt the conventions of ???below
which note six possible pitch trajectories:
high fall, low fall, high rise,
low rise, fall-rise, and rise-fall
(see Figure~???).
The same sequence of syllables
can change its meaning dramatically
by using a different pitch trajectory,
or by changing the position of the
tonic syllable within that pitch trajectory.
The meaning of each pitch trajectory
changes depending on the utterance in question;
we will not investigate meanings further
in this work, as we focus on
sound reproduction rather than on
linguistic meaning.

??? Quick plot of pitch trajectories

% cite https://notendur.hi.is/peturk/KENNSLA/25/IPBE/SHELL/25/nucleus.html
% or Cruttenden

It is important to note that when talking about
the pitch trajectory of an utterance,
pitch is always relative to the
upper and lower range of a particular speaker.
In the pitch trajectory plot in Figure~???,
horizontal lines show the upper and lower ranges.

These six trajectories in Figure~???
interact with the pre-head and tail
in predictable ways.
The pre-head is usually low,
but can be high in front of a low stressed syllable.
The tail trajectory can be predicted
by the trajectory of the tonic syllable;
for example, if the tonic syllable falls,
the tail remains low;
if the tonic syllable rises,
the tail continues to rise.
The head, on the other hand,
is independent of the tonic syllable;
it may remain at a high or low level,
or it can rise or fall like
the trajectories associated with the tonic syllable.
The combinations of head and tonic syllable
pitch trajectories also contributes
to the varying meanings conveyed
by the pitch trajectory of a tone-unit.

??? Look into and summarize Generative phonology

??? Look into, maybe mention Autosegmental intonation
(H = high tone, L = low tone, HL = fall, etc.)

??? Note: constraints and rules aren't necessarily all going to
be taken into account. However, we do want to be sufficiently
flexible or inflexible such that were we to take into account
all rules, we would still scale to biological limits.

\subsection{Psychoacoustics}

??? We'll use psychoacoustics as a means of testing auditory
periphery models...

Everest book:

- Equal loudness curves (fig 3-6, cite original); phon
  - Threshold of hearing and threshold of feeling
  % https://en.wikipedia.org/wiki/Equal-loudness_contour

- Subjective loudness (sone): doubles every 6 or 10 dB.
  fig 3-9; look at Zwicker et al % http://scitation.aip.org/content/asa/journal/jasa/29/5/10.1121/1.1908963

- Bandwidth effects; explain fig 3-10, 3-11, cite Moore Glasberg
  % http://scitation.aip.org/content/asa/journal/jasa/74/3/10.1121/1.389861

- Short sounds are harder to hear; problem for consonants (fig 3-12)

- Pitch (mel) vs. frequency (Hz): pitch depends on SPL (volume);
  7 degrees of loudness and 7 degrees of pitch (!! Nice !!)

- Timbre: an effect of the frequency components in the sound;
  you hear the fundamental frequency in different musical instruments,
  but the details are different. That's timbre.

- Ignoring everything to do with sound localization

- Temporal integration: ``The rate of presentation of the still
  pictures is important; there must be at least 16 pictures per second
  (62-millisecond interval) to avoid seeing a series of still pictures
  or a flicker. Auditory fusion works best during the first 20 or 30
  milliseconds; beyond 50 to 80 milliseconds discrete echoes dominate.''
  fig 3-19. There's more echo stuff, but we don't care.

- Auditory masking
  % https://en.wikipedia.org/wiki/Auditory_masking

It's unclear if these weird psychoacoustical effects
are beneficial or detrimental to speech perception.
Could be due to limitations of our faulty biological tools,
or maybe the result of evolutionary tuning
to important sounds like speech.
We are mostly agnostic to the reasons,
but feel that speech science benefits from
automated recognition and synthesis systems
that respect biological constraints,
as testable predictions can be made.

\subsection{Auditory neurobiology}

\subsubsection{Gross anatomy}

\subsection{Other?}

??? Also talk briefly about conversational shadowing

\section{Prior modeling approaches}

In making early steps toward
an integrated speech recognition and synthesis system,
we are applying techniques
in artificial intelligence and control theory
to the speech domain reviewed in the previous section.
There is a long history of applying these techniques
to this domain;
in this section we review prior modeling efforts
and contrast them with the model we will describe
in future chapters.

\subsection{Auditory periphery modeling}

??? figure like izhikevic, with auditory model + efficiency?

??? summary table with phenomena captured, etc

\subsection{Automatic speech recognition}

\subsection{Speech synthesis}

\subsubsection{Articulatory speech synthesis}

??? summary table with different vocal tract models

??? summary table with different acoustic models

??? in the tables, also note available implementations
(so we can justify writing our own).
Include programming language in this

??? include online vs batch in table

\subsection{Speech motor control}

??? note that many art. synths consider this part of their
synthesizer (control model).
But we will consider it separately because
it is a primary contribution of this thesis.

??? Saltzman stuff on task dynamics
is highly related to what we want to do.
But with SPA stuff on top.

??? also hosung nam's work

??? also mention near the end that people haven't
yet connected the Saltzman task dynamics stuff
to the brain; that'll be one of our contributions

\subsection{Integrated recognition and synthesis systems}

??? Review DIVA model

\section{Methods used? Maybe move?}

\subsection{Vector symbolic architectures}

??? make links to knowledge representation in CS

??? give general background for the math in methods?

%% ~8-20 pages

%% - More than a literature review
%% - Organize related work - impose structure
%% - Be clear as to how previous work being described relates to your own.
%% - The reader should not be left wondering why you've described something!!
%% - Critique the existing work - Where is it strong where is it weak?
%%   What are the unreasonable/undesirable assumptions?
%% - Identify opportunities for more research (i.e., your thesis).
%%   Are there unaddressed, or more important related topics?
%% - After reading this chapter, one should understand the motivation for
%%   and importance of your thesis
%% - You should clearly and precisely define all of the key concepts
%%   dealt with in the rest of the thesis, and teach the reader what s/he
%%   needs to know to understand the rest of the thesis.
