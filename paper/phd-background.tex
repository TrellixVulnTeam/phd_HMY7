\chapter{Background}
\label{chapt:bg}

??? this thesis draws upon and makes contributions
to what would traditionally be thought of a disparate fields:
phonetics and phonology from linguistics,
psychoacoustics from psychology,
knowledge representation
and artificial neural networks from computer science,
spiking neural networks and
synaptic plasticity rules from computational neuroscience,
dynamical systems and control theory
from systems design engineering,
concretely implemented in software
using principles from software engineering.
As such, not all relevant background can
be given equal coverage.
In this chapter, we assume
that the reader's background
is in computer science or engineering.
We therefore spend more time
reviewing background in linguistics
and neuroscience as they fall outside
of the domain of computer science and engineering,
and so can be considered the problem domain
that we will apply our assumed knowledge to.
% ???yikes reword

% First, we review relevant background
% for the problem domain of interest,
% human speech,
% which we organize
% into background relevant to speech recognition,
% speech production, and sensorimotor integration.
% Then, we review existing models
% of speech recognition and production,
% focusing on those using
% biologically inspired or constrained methods,
% and concluding with other integrated approaches
% that have incorporated speech recognition
% and production in a single system.

\section{Human speech recognition}

We limit our coverage
of human speech recognition
to the topics drawn upon
in the model that will be presented
in Chapters~\ref{chapt:model}
and \ref{chapt:implementation}:
basic ear physiology,
psychoacoustics,
and neurobiology.
Basic ear physiology is necessary
foundation for understanding
human speech recognition.
Psychoacoustics provides a quantitative account
of how the human auditory system
responds to incoming air pressure levels.
As such, it provides a method to verify
that our model responds to sounds
as humans do.
A primary assumption behind this work
is that the human auditory system
has evolved to be adept at processing speech;
psychoacoustics describes many of the ways
in which the human auditory system
manipulates sound,
which we aim to reproduce. % ???ugh this needs work
Auditory neurobiology describes both
the physical substrate and organization
of the system that we aim to emulate.
We do not aim to fully replicate
biological neurons,
but by using a simple approximation
of biological neurons,
we constrain our algorithmic choices
to those that could be implemented
in a biological system.
Similarly, by examining the organization
of the auditory brain structures,
we constrain the space of possible
network topologies
to those that match
a network that we know
to be successful.\footnote{One notable omission
  from our discussion
  of the human auditory system
  is the integration of information
  coming from both ears.
  There are binaural effects at many levels,
  and while a full human auditory model
  would incorporate these effects,
  at this stage of research
  we only aim to build
  a monaural speech system.}

\subsection{Ear physiology}

The human ear transduces fluctuations in
air pressure level to neural signals
that we interpret as sounds.
It does this by mechanically
separating the air pressure level fluctuations
into instantaneous frequency components,
which is the basic representation
that the brain receives.
Figure~\ref{fig:an-overview} illustrates
the major structures that will be discussed.

\fig{an-overview}{0.8}{??? caption}{??? shortcap}

The outer ear (pinna) funnels air
into the ear canal.
Functionally, it selectively boosts
air pressure fluctuations at
around 3 kHz (see Figure~\ref{fig:ear-tf}),
and modifies the air pressure wave
such that the directionality
of sound can be determined.

% \fig{ear-tf}{0.7}{??? caption}{??? shortcap}

In the middle ear,
air pressure fluctuations
cause the eardrum (tympanic membrane)
to vibrate;
these miniscule vibrations
cause three tiny bones,
the malleus, incus, and stapes to move.
The stapes sits on the oval window
of the cochlea,
such that when the eardrum vibrates,
the stapes moves in and out of the oval window,
causing disturbances in the liquid
in the cochlea
(see Figure~\ref{fig:cochlea}).

\fig{cochlea}{0.5}{??? caption}{??? shortcap}

The inner ear transduces vibrations
of the liquid in the cochlea
to electrical signals transmitted
to the brain.
As liquid in the cochlea
moves past the basilar membrane,
it causes it to deflect
based on the width and thickness
of the membrane at that point
in the cochlea.
The basilar membrane
is shaped such that
the base of the membrane
(near the stapes)
is deflected when
incoming vibrations have power
at high frequencies,
up to 20,000 Hz.
The membrane becomes sensitive
to lower and lower frequencies
going further down the membrane
to the apex
in the center of the cochlea,
which is deflected when incoming vibrations
have power as low as 20 Hz.

The basilar membrane's surface is covered
with hair cells that transduce
the deflections of the membrane
to electric current.
Stereocilia on the top of the hair cell
rests on or near the tectorial membrane
on the outside of the cochlea.
When the basilar membrane deflects,
the stereocilia's orientation
relative to the tectorial membrane changes,
which mechanically opens receptors
on top of the stereocilia,
allowing positive charged ions
(mostly potassium and calcium)
to enter the hair cell.
Inner hair cells synapse with
spiral ganglion cells,
which accumulate the continuously
changing voltage in the inner hair cells
and send action potentials
down the auditory nerve
conveying how much power
is present at the current moment
at the frequency characteristic
of that section of the basilar membrane.
Outer hair cells play a more nuanced
role in transduction.
Their activity is tuned
more broadly in both space and time,
and seems to act as a dynamic amplifier,
amplifying quiet sounds and
attenuating loud sounds.
This dynamic amplification allows
the human ear to have a wide dynamic range,
able to safely hear sounds from 0--130 dB,
which represents 13 orders of magnitude
of absolute air pressure.

\subsection{Psychoacoustics}
\label{sec:psychoacoustics}

While the human ear can be thought of
as a spectrum analyzer for air pressure waves,
there are important differences between
a straightforward Fourier analysis
and how the ear and early auditory brain regions
respond to air pressure waves.
Assuming that the human
auditory system has evolved to be
well-suited for speech,
it stands to reason that
capturing these differences
is important for building
a frontend for speech recognition.
Therefore, the psychoacoustical findings
discussed in this chapter
are presented as motivation for the
auditory periphery models described
in Section~\ref{sec:periphery-models}.

% ??? Either here or in previous section,
% introduce the idea of formants
% and format frequencies...

\subsubsection{Spatial psychoacoustical effects}

Sound pressure levels can be objectively measured
and expressed in terms of the logarithmic
decibel (dB) scale.
Human perception of loudness can help
determine the transfer functions
at different parts of the auditory system.
Early studies investigated
subjective loudness in response
to pure tones,
as tones of equal objective volume
are perceived as louder for higher frequencies.
The exact relationship between
sound pressure level and
loudness was originally proposed
by \cite{fletcher1933},
and further standardized
by the international standards organization
(ISO-226:2003);
see Figure~\ref{fig:el-curves}.
In general, perceived loudness,
as expressed in the phon scale,
increases for higher frequency sounds
up to around 7000 Hz.

% \fig{el-curves}{0.8}{??? caption}{??? caption}

Another scale, the sone,
quantifies the relative differences
to pure tones played with
different sound pressure levels.
Specifically, the sone scale
is linear with perceived loudness;
a sound played with twice the sone value
should be perceived as twice as loud.
The sone for a particular frequency is
\begin{equation}
  \text{sone} \approx 2^{\frac{\text{phon} - 40}{10}},
\end{equation}
above signals of 40 phon or louder.
% However, while it is easy to compare
% the relative volumes of two sound presented
% in close succession,
% absolute loudness is difficult to perceive.
% Kollmeier suggests we perceive only
% seven levels of loudness?

The difference in perceived loudness
across the frequency spectrum,
along with the organization
of the inner ear,
suggests that a better model
of the ear (instead of a spectrum analyzer)
is as a bank of narrowband filters.
Support for this model was provided
in \cite{zwicker1957},
who showed that when two tones
have frequencies that are close enough,
their perceived loudness sums,
suggesting that the two tones
are within the bandwidth of one of the
auditory system's filters.
\cite{moore1983}
summarized other attempts to
determine the bandwidth
of auditory filters
in a simple equation
describing the equivalent
rectangular bandwidth (ERB)
of auditory filters:
\begin{equation}
  \text{ERB} = 6.23 f^2 + 93.39 f + 28.52,
\end{equation}
where $f$ is the center frequency
of the auditory filter of interest.
See Figure~\ref{fig:erb} for a visualization of
this measure for frequencies of relevance
for the human auditory system.
Auditory filters are not rectangular,
but for simplification this equation
treats them as such;
despite this simplification,
the ERB measure has been productive and is still used
to determine the bandwidths
of filters in auditory periphery models.

% \fig{erb}{0.8}{???}{???}

Like volume,
humans perceive
the frequency of incoming sounds
in subjective ways.
The subjective perception of frequency
is referred to as pitch.
\cite{stevens1937} quantified
the relationship between
frequency and pitch
with a simple equation,
\begin{equation}
  \text{mel} = 2595 \log_{10} \left(1 + \frac{f}{700}\right),
\end{equation}
resulting in the mel scale
(see Figure~\ref{fig:mel}).
As with volume, humans are adept at
noticing when two tones differ in pitch;
however, absolute perceptions of pitch
have far less granularity
(musical notation, for example,
uses seven degrees of pitch per octave
and seven degrees of loudness).
Octaves are spaced from one frequency
to double that frequency;
harmonic tones are perceptually similar,
so pitches can only
be easily differentiated
relative to their position in an octave.

% \fig{mel}{0.8}{???}{???}

\subsubsection{Temporal psychoacoustical effects}

To this point, we have discussed
spatial psychoacoustical effects,
as they depend on the content of
the sound pressure level
at a moment in time.
There are also several temporal effects,
which are important for speech.
The first is that the length
of a sound affects its perceived volume.
Specifically, short sounds are harder to hear
(see Figure~\ref{fig:short-sounds}).
As a result, humans find
consonant sounds more difficult
to recognize in situations with background noise
than vowel sounds,
as consonants are shorter than vowels
\cite[Chapter~3]{everest2001}.
There is evidence that sounds with constant volume
are perceived as being monotonically louder
up to approximately 200 ms;
this finding suggests that
at some point in the auditory system,
the power for a given characteristic frequency
is integrated over a time window
of approximately 200 ms
\cite[p.64]{kollmeier2008}.

\fig{short-sounds}{0.4}{???}{???}

Another temporal effect is forward and backward masking.
When two sounds are made in quick succession,
the louder sound of the two can
``mask'' the quieter sound,
rendering it inaudible,
even though it would be audible
were it made in isolation.
When the masked (quieter) sound
precedes the masker (louder sound),
it is called backward masking;
the reverse is called forward masking.
Spatial masking also occurs
when two sounds differ in loudness,
and are within the bandwidth
of an auditory filter.

Our perception of echoes can also tell us
something about how the auditory system
responds to temporally transformed sounds.
We perceive all sounds arriving within
a certain time window as occurring at essentially
the same time;
for example, in an enclosed room,
we hear our own speech as we voice it,
and shortly thereafter when it is reflected
back to us off of the walls.
Yet, we perceive this as a single utterance,
a phenomenon known as auditory fusion.
\citeauthor{haas1972} found that auditory fusion occurs
best when similar sounds are separated by roughly
20 to 30 milliseconds.
When similar sounds are separated by 50 to 80 milliseconds,
we perceive them as discrete echoes.

Together, these temporal psychoacoustical effects
suggest that the incoming frequency information
from the filters implemented by the auditory periphery
are further temporally filtered
by auditory regions of the brain.

\subsection{Neurobiology}
\label{sec:recog-neurobio}

Brain structures in the primate auditory system
are organized in several parallel
hierarchical pathways.
Information from the auditory periphery
arrives at the brainstem. % ???more.
That information is relayed to the cortex
through the medial geniculate nucleus (MGn)
in the thalamus.
The MGn sends information to a part
of cortex in the temporal lobe
called the core.
In the context of auditory cortex,
the core is a small portion of cortex
in Heschl's gyrus (???)
% (???size in mm2)
that serves as the first stage
in the mostly feedforward hierarchy
of auditory cortical areas.
The core appears to be made up of
three subareas, ??? A1, R, and RT.
The core is surrounded by a set of areas
called the belt,
which primarily receives input from
the core.
% As such, there are ???three
% distinct areas in the belt,
% ???.
The belt provides input
to the parabelt.
% which is made up of ???.
See Figure~\ref{fig:corebelt} for a summary of
the neuroanatomical structures and connections
in the auditory hierarchy.

\fig{corebelt}{0.9}{???}{???}

The parabelt provides input
to several areas,
including brain areas
associated with language
in the superior temporal gyrus,
and frontal cortex which is associated
with working memory and executive function.
Historically, the most
salient linguistic area
in the superior temporal gyrus
was Wernicke's area (Brodmann area 22),
as lesions in this area result in
a type of aphasia in which
speech can be produced fluently,
but lacks meaning.
However, more recent hypotheses
concerning how auditory information
becomes discrete linguistic information
assigns roles to much larger portions
of the superior temporal gyrus
and the superior temporal sulcus.
We will discuss this in more detail
in Section~\ref{sec:sm-neurobio}.

While we do not yet know how auditory information
is transformed through the connections
described above,
we get some clues by investigating
how various auditory brain areas
respond to sounds with different
spatial and temporal properties.
The first clue
(which was used to determine the boundaries
between auditory cortical regions)
is that all of the areas
up to the parabelt are
tonotopically organized;
that is, each neuron in these areas
responds preferentially to
a part of frequency spectrum,
and cells close to one another
are likely to be sensitive
to the same frequency band.
In general, lower level regions
like the MGn and core
are more tightly tuned
to a smaller part of the frequency spectrum
(sometimes even more tightly tuned
than the auditory filters in the cochlea),
while higher level regions
like the parabelt
are more broadly tuned
to larger parts of the frequency spectrum.
% ???more, cites
See Figure~\ref{fig:aud-tonotopy} for visualizations
of the tonotopic organization
of primary auditory cortical regions.

\fig{aud-tonotopy}{0.6}{???}{???}

Neurons at various levels in the auditory hierarchy
respond to their selected frequency range
with different temporal characteristics.
In general, we can express the temporal
relationship between the incoming
frequency spectrum and the resulting
neural activity with a causal filter.
Considering that we also
have referred to the spatial tuning
of cells as a filter,
many auditory researchers
summarize the spatial and temporal
tuning properties of auditory neurons
using a single filter,
based on the
Spectral-Temporal Receptive Field (STRF).
STRFs summarize how a neuron responds
to incoming stimuli both spatially
and temporally \cite{aertsen1981}.
Unfortunately, despite many decades
of determining STRFs in
primate ??? cite and
human auditory cortex ???cite
the exact temporal properties of
each area are not known.
In general, STRFs have been found that
show delays of 5--250 ms
(see, e.g., \cite[Supplementary material]{mesgarani2014}).
See \cite{kaas1999,kaas2000,scott2003,semple2003}
for reviews.

It should be noted
that the information gleaned from
spectro-temporal tuning curves
still describes a linear relationship
between the incoming frequency spectrum
and how a particular neuron responds.
\citeauthor{tian2004}
suggest that belt neurons
in rhesus monkeys respond nonlinearly
to frequency-modulated sweep stimuli,
but \cite{kowalski1995}
found that cat A1 responses were linear.
It is difficult to evaluate
the responses of cells
as traditionally the STRF
is determined using simple stimuli
like white noise;
however, see \cite{theunissen2000},
for a method using complex sounds.
\cite{escabi2002}
found nonlinear responses
in approximately 40\%
of the midbrain inferior colliculus
neurons they recorded,
suggesting that nonlinearities
are widespread
throughout the auditory hierarchy.
Regardless, it is clear that
nonlinearities are more prevalent
the higher up you go in the auditory hierarchy,
though the types of nonlinearities
at each processin level are still unclear.
% we aim to provide some insight into
% nonlinearities needed in the auditory system
% in the model presented in this thesis.

In sum, we interpret
the auditory neurobiological literature
to support a model in which
the MGn and other lower brain areas
transmit frequency information
and delayed frequency information
to the core through direct connections,
and connections through intermediate
recurrently connected ensembles.
The core transmits nonlinearly transformed
frequency information to the belt,
which pools the responses from
a wider band of frequencies than the core.
The parabelt performs a similar transformation
and broadening of frequency selectivity.
Finally, the representations from the parabelt
are used by higher-level regions to
do extract discrete
acoustically and linguistically
important information.

\subsection{Automatic speech recognition}

Automatic speech recognition (ASR) has been
an active field of research
in artificial intelligence since 1952,
when \citeauthor{davis1952}
presented a system for recognizing
spoken digits by a single individual
with accuracy above 97\%.
This system was implemented
with special purpose hardware,
matching the spectral properties of
the sound to known spectral patterns.
From these humble beginning,
modern speech recognition systems
can transcribe speech
in realistic environments
with accuracy high enough
to be deployed commercially.
Two significant advancements
have enabled progress in ASR.

The first significant advancement
was the development of
Hidden Markov Models (HMMs)
and related techniques
for speech recognition.
Briefly,
HMM-based speech recognition models
attempt to find the word sequence
with maximum probability given
an audio waveform.
They do this through
a pipeline in which
short time-slices of the audio waveform
(called ``frames'')
are analyzed to yield lower dimensional
feature vectors
through signal processing techniques.
A sequence of feature vectors
is then decoded into words
using an HMM-based acoustic model
that uses sub-phonemic states
to emit phoneme labels,
which are then composed into words
using large corpora of
lexical and linguistic information
(see Figure~\ref{fig:asr} and
\ref{rabiner1989,gales2008} for reviews).
The first part of the pipeline
that produces feature vectors
is often called the ``frontend;''
the second part that uses
statistical methods to infer labels
is called the ``backend.''

\fig{asr}{0.6}{???}{???}

HMM-based systems
were among the first systems
successful enough to be
used commercially
(see \cite{huang2014}).
In 1989, \citeauthor{lee1989}
achieved a phone accuracy rate of % ??? look up: phone or word accuracy?
66.08\% on a subset of the
TIMIT speech corpus.
Four years later,
\citeauthor{lamel1993}
used a more sophisticated HMM-based system
to achieve 72.9\% phone accuracy on
all of the TIMIT speech corpus.
Improvements from then until
the mid-2000s were modest
(see \cite{lopes2011}).

Since 2006, a series of learning algorithms
and network structures in artificial neural networks (ANNs)
that are now referred to as ``deep neural networks'' (DNNs)
have been applied to
the domain of automatic speech recognition
with considerable success
both in academic research
and in commercial application.
In 2013, \cite{graves2013}
achieved
phone classification accuracy on TIMIT
of 82.3\%,
significantly better than the most sophisticated
HMM-based model's error rate of 72.9\%.  % ??? check!
% look at zen2007 zen2009

The architecture of DNN-based systems
is simpler than that of HMM-based systems
(see Figure~\ref{fig:dnn}
and \cite{hinton2012}).
Instead of distinct steps in which
an explicit statistical acoustic model
is evaluated and then combined with language models,
DNNs map directly from the
feature vector input representation
to the output representation of choice
(typically phonemes or word vectors),
which can be further refined with a language model.
This simpler architecture may be
responsible for much of DNN's success,
as its internal representations
are not locked to specific representations
like triphones.
However, the simpler architecture
does not necessarily mean that
using DNNs is straightforward;
the effort in developing ASR models
with DNNs shifted from
developing complicated architectures
to developing complicated learning algorithms
that have many parameters
which must be carefully tuned.

% \fig{dnn}{0.5}{???}{???}
% ??? DNN architecture
% something like slide 8 of Vanhoucke2013
% or check hinton2012

One additional benefit of DNN-based approaches
is that they have analogies to
how human recognize speech.
DNNs leverage simple computational units
(artificial neurons),
which operate in parallel
and communicate through unidirectional
weighted connections.
While the computations done by these neurons
and the learning algorithms that adjust
the connection weights
may not be directly implementable
under certain biological constraints,
mappings between DNNs
and biologically plausible spiking neural networks
are currently being developed
\cite{hunsberger2015}.

\subsection{Auditory periphery modeling}

Most ASR frontends do an ideal
power spectral analysis
with variants of the Fourier transform,
mimicking the function
of the human auditory periphery.
However, as discussed in Section~\ref{sec:psychoacoustics},
the human ear's frequency decomposition
is far from ideal.
Some differences in how the ear
analyzes the frequencies in sound
are due to the continuous nature
of the real world;
it is not possible for the ear
to maintain a perfect history
of the past 50 milliseconds of sound.
Some differences are due to
the nature of biological computation,
which is distributed, analog, and noise robust,
but not easily emulated with digital computers.
Finally, some differences
occur because they are advantageous
for some aspect of hearing
(possibly speech),
and have evolved over time through natural selection.
Unfortunately, it is difficult to know
the reason for each difference.
Historically, auditory periphery models
have attempted to reproduce
the auditory periphery as closely as possible,
whether or not that difference is advantageous
for speech recognition.

Several approaches to auditory periphery modeling
have progress in parallel over the past century.
Detailed mechanical models
like those reviewed in \cite{ni2014}
aim to model ear physiology
as accurately as possible.
Unfortunately, efficient implementations
of these models are not readily available.
Phenomological models, on the other hand,
aim to reproduce the output
of the human ear
as closely as possible
using simplified mathematical models;
efficient implementations of these
models are freely available
\cite{fontaine2011}.
We will focus only on phenomenological
models of the auditory periphery.

Generally, phenomenological auditory periphery models
are made up of a cascade of linear and nonlinear filters.
Together, the filters can model
how the sound pressure level
is transformed through the middle ear,
how it deflects the basilar membrane,
how those deflection are transduced
into electrical signals
by the inner hair cells,
and how those signals result in
action potentials traveling down
the auditory nerve.
Not all models include all of these stages.
For reviews of prominent
auditory periphery models and
the auditory-nerve response characteristic
reproduced by those models,
see \cite{lopez2005}
and \cite{lyon2010}.
The auditory periphery models used
in this thesis will be discussed
in further detail in
Section~\ref{sec:periphery-models}.

\section{Human speech production}

We limit our coverage
of human speech production
to the topics drawn upon
in the model that will be presented
in Chapters~\ref{chapt:model}
and \ref{chapt:implementation}:
basic vocal tract physiology,
phonetics and phonology,
and neurobiology.
Phonetics and phonology
provide insight into the basic units of speech.
While a full conversational system
would draw upon all areas of linguistics,
we believe that phonetics and phonology
provides the most relevant insights
when assessing speech from the level of
incoming air pressure levels.

\subsection{Vocal tract physiology}

The human vocal tract performs
the opposite role,
translating electrical brain activity
into muscle activations
which cause fluctuations in air pressure.

% \fig{vt}{0.4}{???}{???}
% % Generate from Kroger synth, label

The vocal tract consists of
the layrngeal cavity,
the pharynx, the oral cavity,
and the nasal cavity
(see Figure~\ref{vt}).
Air expelled from the lungs
is transformed
by these four structures
in turn to form the
specific air pressure fluctuations
that we interpret as speech.
Major changes to airflow
occur in the layrngeal cavity,
which contains the glottis.
The glottis is made up of
the vocal folds,
which are muscles that are able to vibrate rapidly,
and the opening between the vocal folds.
When the glottis is completely open,
air passes through mostly undisturbed,
resulting in low-frequency breathy sounds.
When the glottis narrows,
air passes through more turbulently,
resulting in higher frequency breathy sounds,
as in the phone \ipa{[h]}
in the English word \ipa{/h \ae d/}.
In all voiced phonemes (e.g., all vowels),
the vocal folds vibrate,
resulting in a ``buzzing'' quality;
compare voicing ``ah'' to
forcing air out of your mouth
in the same vocal tract shape.
The exact action of the glottis
is responsible for many
of the subtleties in human speech,
such as intensity (speaking versus shouting),
frequency (low versus high pitch),
and quality (e.g., harsh, breathy, murmured, creaky).

The pharynx, oral cavity, and nasal cavity
can be moved into many different shapes
by the muscles of the vocal tract.
The shape of these remaining portions
of the vocal tract cause further turbulence
of the air passing through,
resulting in specific patterns
that we interpret as phonemes.
The areas of the vocal tract
that can be moved in order to
effect a linguistically relevant sound
are called articulators.
As will be discussed further
in subsequent sections,
the positions of these articulators
relative to each other
determines the phoneme
that will be voiced
when air passes through
the vocal tract.
The seven articulators are
as follows.

\begin{enumerate}
\item The \textbf{pharynx} is a tube at the back of the throat.
  It carries air from the larynx to the oral and nasal cavities.
\item The \textbf{velum} or soft palate permits or restricts
  access to the nasal cavity. It is also a constriction target
  for the tongue, as in the phones \ipa{[k]} and \ipa{[g]}.
\item The \textbf{hard palate} is the hard surface
  at the roof of the mouth. It cannot move, but serves
  as a constriction target for the tongue.
\item The \textbf{alveolar ridge} is the area between
  the hard palate and the top front teeth. Like the hard palate,
  it serves as a constriction target for the tongue.
\item The upper and lower \textbf{teeth} are at the front of the mouth.
  While they primarily serve as a constriction target,
  the lower teeth are under muscular control,
  though few sounds utilize the motion of the lower teeth.
\item The \textbf{tongue} is a large, flexible, muscular structure
  that can reach several different constriction targets.
  The tongue is often divided into the tip, blade, dorsum, and root,
  though there are no clear dividing lines between these areas.
\item The \textbf{lips} are another important flexible, muscular
  articulator that is involved in many speech sounds.
  The lips can move toward other articulators to constrict airflow,
  or can become rounded to change the overall quality of a sound.
\end{enumerate}

Other parts of the vocal tract are important to speech;
for example, some sounds require air
to pass through the nasal cavity.
However, air is routed to the nasal cavity
through constrictions of the seven articulators above;
there are few little linguistically relevant
movements that can be done with the nose.
Exact definitions of the articulators
do vary between phoneticians,
and play an important role
in the design of a speech production system.
We will discuss this in further detail
in Section~\ref{sec:results-production}.

\subsection{Phonetics and phonology}
\label{sec:phonology}

Our treatment of phonetics and phonology
focuses on the hierarchical levels
in human speech,
paying particular attention to two
sets of characteristics in each level
and in each mapping between two levels:
temporal patterns and constraints,
and the existence of rule-like regularities.
For example,
if our model were to deal with sentences,
it would be important to know that,
temporally, sentences occur sequentially,
and cannot co-occur.
There are grammatical rules
about what words can be in a sentence,
and in what order they can be arranged,
but there are few rules placing constraints
between sentences---any sentence \textit{could}
follow any other sentence,
though there may be semantic issues.
Similar constraints exist at
the lower levels of language that
phonetics and phonology examine,
and these constraints will
inform the structure of the model
we present in Chapter~\ref{chapt:model}.

The specific hierarchy that we will
present in this section is pictured
in Figure~\ref{fig:prod-hierarchy}.
We will arbitrarily begin at
the lowest (rightmost???) level and work upwards
(though a presentation starting at
the highest level would be equally valid).
For clarity, we will use English for examples,
except when demonstrating that a particular
quality differs across languages.\footnote{
  When not explicitly cited,
  the information in Section~\ref{sec:phonology}
  should be considered
  basic phonetics and phonology
  that would be covered in an
  introductory undergraduate level
  class or textbook
  (I used \cite{roach2010} as a general reference).
}\footnote{The International Phonetic Alphabet (IPA)
  will be used this and subsequent sections.
  See Section~\ref{typography} for more details.}

% \fig{prod-hierarchy}{0.5}{???}{???}
% box-arrow diagram with the stuff from this sect

\subsubsection{Vocal tract gestures}

According to many articulatory phoneticians,
the fundamental unit of speech production
is the ``gesture,''
which describes some movement
of the articulators in the vocal tract
\cite{browman1989}.
Gestures are pre-linguistic,
in that a vocal tract gesture
does not necessarily produce
a linguistically interesting sound;
a set of possibly overlapping
vocal tract gesture can produce a phoneme,
which is smallest linguistically relevant unit.
There is evidence that children
learning to say their first words
often use the correct gestures,
but take time to order them correctly
to produce the desired utterance,
indicating that gestures develop
before language,
and are leveraged when
developing language
\cite{browman1989}.

Gestures are stereotyped movements
of sets of articulators in the vocal tract.
Each gesture has its own temporal dynamics;
that is, a gesture is a function defined
over space (what muscles are contracted)
and time (when contractions occur).
In general, a gesture can be thought of
as a point attractor
in which the position
of one or more vocal tract
articulators is parameterized.
A minimal gesture is parameterized by
the set of articulators engaged
by that gesture,
and the degree to which
the articulators are constricted;
for example,
widening and narrowing the velic aperture
is a gesture consisting of
the articulator involved
(the velum)
and degree to which it is constricted
(low degree for narrow, high degree for wide).
Other gestures require constriction location
and constriction shape parameters
to disambiguate them from other similar gestures;
for example, the tongue tip
is involved in gestures
that depend on location
(e.g., constriction at
the teeth, alveolar ridge, or hard palate)
and shape
(e.g., tongue tip can be straight
or curved backwards).
All gestures are also affected by
global parameters affecting all gestures,
most notably ``stiffness,''
which defines how quickly
the gesture attempts to reach
the point of attraction.

Gestures are rarely discussed in isolation
as phonemes only occur when certain
sets of gestures overlap
or occur in tight succession.
Therefore, gestures are often
grouped into gestural scores
(see Figure~\ref{fig:gs})
which embody the spatiotemporal
coordination of several gestures
to produce a linguistically
relevant sound.
The vertical axis of a gestural score
is the articulator set,
as although each set
can be used for multiple gestures,
it can only produce one gesture
at a time.
The horizontal axis of
a gestural score represents time,
though the time axis may be
stretched depending on
the global parameter, stiffness.
Gestural scores can also
be visualized as a graph,
as in Figure~\ref{fig:gs},
which highlights that
certain gestures must co-occur
or have tight temporal couplings
such that one gesture
must start as another gesture ends.

\fig{gs}{0.5}{???}{???}

As will be discussed in detail
in Section~\ref{sec:model-motorcontrol},
there is a mapping
between gestures and vocal tract articulators;
in human speech, the mapping links
gestures and vocal tract muscle activations.
In models of human speech,
the mapping depends on what articulators
are available in the articulatory synthesizer,
and how those articulators can be manipulated.
Unfortunately, few models make a clear distinction
between the set of gestures
and the mapping from gestures
to articulator trajectories,
resulting in there being
no agreed upon set of vocal tract gestures
in human speech.
Similarly, several approaches to
temporal sequencing of gestures
have resulted in different
definitions of gesture sets,
with different graph structures as
the one shown in Figure~\ref{fig:gs}.
We will discuss our interpretation
of gestures and gesture sets
in Section~\ref{sec:results-production}.
% Development of better human vocal tract models
% will help researchers
% determine a complete set of
% vocal tract gestures.

% ??? In a sense, vocal tract gestures
% can be thought of a subset of all
% motor synergies ???cite

\subsubsection{Phonemes}

The smallest linguistically relevant unit
is the phoneme, which describes
a short sound produced by one or more
vocal tract gestures
on the order of tens of milliseconds.
Phonemes are noted by their ability
to change the meaning of some word
when swapped in speech;
for example, the only difference between
the word ``bad'' and ``mad'' is the
consonant sound at the beginning,
and therefore those two consonant sounds
are each separate phonemes.
Even though different individuals
voice each phoneme differently,
the important quality is that
the particular sound is recognized
as a particular phoneme
in the context of an utterance.
A helpful analogy can be made between
phonemes in speech
and alphabetical letters in writing.
With no knowledge of English,
seeing the sentence,
``A bird has a wing,''
one might think that ``A'' and ``a''
are different letters.
However, with enough examples,
one could surmise that in every situation
where ``A'' is used, it appears at the
start of a sequence of letters,
and it could have instead
been replaced by ``a''
had it not appeared at the start of the sequence.
Therefore, ``A'' and ``a'' represent
the same underlying ``letter.''
Similarly, despite individual differences
in the pitch, speed, volume, and quality
of how one voices a particular phoneme,
that phoneme is still considered the same
if it plays the same role
in a linguistically relevant sequence of phonemes.

Generally, there are two types of phonemes:
vowels and consonants.
Vowels are longer sounds
made when the vocal tract is mostly open.
Consonants are shorter sounds
made when some part of the vocal tract
is constricted or transiently closed.
In almost all languages,
there are more consonant phonemes
than there are vowel phonemes,
though pronunciation varies significantly
between dialects,
and transcribing the full set of
phonemes in a language is
not a purely objective exercise.
For example, while most dialects
of English recognize 24 consonant phonemes,
General American English has been transcribed
as having 16 vowel phonemes,
while Received Pronunciation English
has been transcribed as having 25 vowel phonemes
\cite{wells1982}.

Vowel phonemes occur when air is freely
moving through the open vocal tract.
The shape of the vocal tract determines
the quality of the sound.
Three factors influence vowel vocal tract shape:
openness, backness, and roundedness.
Openness refers to the general position
of the jaw (open or closed) and tongue (low or high);
backness refers to the position of the
tongue relative to the back of the mouth;
and roundedness refers to
whether the lips are rounded.
Roundedness can vary independently
of the other factors;
therefore, each vowel sound has a rounded
and unrounded variant.
Openness and backness are partially coupled,
such that when the vocal tract is open,
it must be mostly (but not completely) back.
The three possible extremes, then,
are \ipa{A} (open, back),
\ipa{i} (closed, front),
and \ipa{u} (closed, back).
Most of the remaining vowel sounds
can be expressed as being
a blend of one of these three vowel sounds
(see Figure~\ref{fig:vowels} for a visualization).

Pure vowel phonemes (also called monophthongs)
occur when the glottis phonates
and the vocal tract stays
in one of the positions already described.
Diphthongs phonemes occur
when the vocal tract
transitions or ``glides''
from one vocal tract position
to a second vocal tract position
during phonation;
e.g., the English \ipa{[aU]} ???
in ``loud'' (\ipa{[laUd]})
is a diphthong.
Triphthong phonemes,
in which three vocal tract positions
are visited in sequence,
also occur in some languages;
e.g., in RP English
the word ``hour''
is sometimes voiced with the
triphthong \ipa{[aU@]}.

\begin{figure}[ht!]
  \centering
  \begin{tikzpicture}[scale=3]
    \large
    \tikzset{
      vowel/.style={fill=white, anchor=mid, text depth=0ex, text height=1ex},
      dot/.style={circle,fill=black,minimum size=0.4ex,inner sep=0pt,outer sep=-1pt},
    }
    \coordinate (hf) at (0,2); % high front
    \coordinate (hb) at (2,2); % high back
    \coordinate (lf) at (1,0); % low front
    \coordinate (lb) at (2,0); % low back
    \def\V(#1,#2){barycentric cs:hf={(3-#1)*(2-#2)},hb={(3-#1)*#2},lf={#1*(2-#2)},lb={#1*#2}}

    % Draw the horizontal lines first.
    \draw (\V(0,0)) -- (\V(0,2));
    \draw (\V(1,0)) -- (\V(1,2));
    \draw (\V(2,0)) -- (\V(2,2));
    \draw (\V(3,0)) -- (\V(3,2));

    % Place all the unrounded-rounded pairs next, on top of the horizontal lines.
    \path (\V(0,0))     node[vowel, left] {\ipa{i}} node[vowel, right] {\ipa{y}} node[dot] {};
    \path (\V(0,1))     node[vowel, left] {\ipa{1}} node[vowel, right] {\ipa{0}} node[dot] {};
    \path (\V(0,2))     node[vowel, left] {\ipa{W}} node[vowel, right] {\ipa{u}} node[dot] {};
    \path (\V(0.5,0.4)) node[vowel, left] {\ipa{I}} node[vowel, right] {\ipa{Y}} node[dot] {};
    \path (\V(0.5,1.6)) node[vowel, left] {} node[vowel, right] {\ipa{U}} node[dot] {};
    \path (\V(1,0))     node[vowel, left] {\ipa{e}} node[vowel, right] {\ipa{\o}} node[dot] {};
    \path (\V(1,1))     node[vowel, left] {\ipa{9}} node[vowel, right] {\ipa{8}} node[dot] {};
    \path (\V(1,2))     node[vowel, left] {\ipa{7}} node[vowel, right] {\ipa{o}} node[dot] {};
    \path (\V(2,0))     node[vowel, left] {\ipa{E}} node[vowel, right] {\ipa{\oe}} node[dot] {};
    \path (\V(2,1))     node[vowel, left] {\ipa{3}} node[vowel, right] {\ipa{\textcloseepsilon}} node[dot] {};
    \path (\V(2,2))     node[vowel, left] {\ipa{2}} node[vowel, right] {\ipa{O}} node[dot] {};
    \path (\V(2.5,0))   node[vowel, left] {\ipa{\ae}} node[vowel, right] {} node[   ] {};
    \path (\V(3,0))     node[vowel, left] {\ipa{a}} node[vowel, right] {\ipa{\OE}} node[dot] {};
    \path (\V(3,2))     node[vowel, left] {\ipa{A}} node[vowel, right] {\ipa{6}} node[dot] {};

    % Draw the vertical lines.
    \draw (\V(0,0)) -- (\V(3,0));
    \draw (\V(0,1)) -- (\V(3,1));
    \draw (\V(0,2)) -- (\V(3,2));

    % Place the unpaired symbols last, on top of the vertical lines.
    \path (\V(1.5,1))   node[vowel]       {\ipa{@}};
    \path (\V(2.5,1))   node[vowel]       {\ipa{5}};
  \end{tikzpicture}

  \caption[]{}
  \label{fig:vowels}
\end{figure}

Consonant phonemes occur when some point
of the vocal tract is constricted.
The place and manner of constriction
determines the consonant that will be uttered.
Place refers to the location in the vocal tract
that becomes constricted.
For example, in bilabial consonants,
both lips come together,
as in \ipa{[m]} and \ipa{[b]};
in velar consonants,
the tongue moves toward the velum
(i.e., soft palate),
as in \ipa{[k]} and \ipa{[g]}.
Manner refers to how the vocal tract
is constricted in that location.
For example, in a plosive,
the vocal tract is completely closed
at the place of articulation;
air compresses behind the place of constriction,
and is then released,
producing the sound recognized
as a plosive, like \ipa{[t]} and \ipa{[k]}.
In fricatives,
the articulators move close together
such that there is a narrow channel
for air to pass through.
The narrow channel causes
the hissing sounds associated
with phonemes like \ipa{[s]} and \ipa{[f]}.
Several other places and manners exist;
those defined by the IPA are shown
in Figure~\ref{fig:consonants}.

% from Tim Mahrt
\begin{figure}[ht!]
  \begin{small}
    \hspace{4.6em}
    \begin{tabular}{|l|cc|cc|cc|cc|cc|cc|}
      \hline &
        \multicolumn{2}{|c|}{\footnotesize{Bilabial}} &         % Bilabial
        \multicolumn{2}{|c|}{\footnotesize{Lab. dent.}} &       % Labiodental
        \multicolumn{2}{|c|}{\footnotesize{Dental}} &           % Dental
        \multicolumn{2}{|c|}{\footnotesize{Alveolar}} &         % Alveolar
        \multicolumn{2}{|c|}{\footnotesize{P-alveo.}} &     % Post-alveolar
        \multicolumn{2}{|c|}{\footnotesize{Retroflex}} &        % Retroflex

      \hline Plosive &                % Plosive
        \ipa{p} & \ipa{b} &                     % Bilabial
                &         &                     % Labiodental
        \multicolumn{3}{|r}{\ipa{t}}&           % Dental
        \multicolumn{3}{l|}{\ipa{d}}&           % Alveolar
                                                % Post-alveolar
        \ipa{\:t} & \ipa{\:d}  \\               % Retroflex

      \hline Nasal &              % Nasal
        & \ipa{m} &                         % Bilabial
        & \ipa{M} &                     % Labiodental
        \multicolumn{3}{|r}{}&                % Dental
        \multicolumn{3}{l|}{\ipa{n}}&             % Alveolar
                                      % Post-alveolar
        & \ipa{\:n}  \\                           % Retroflex

      \hline Trill &                  % Trill
        & \ipa{\;B}&                      % Bilabial
        & &                           % Labiodental
        \multicolumn{3}{|r}{}&                % Dental
        \multicolumn{3}{l|}{\ipa{r}}&               % Alveolar
                                      % Post-alveolar
        &   \\                         % Retroflex

      \hline Tap/Flap &             % Tap /Flap
        & &                         % Bilabial
        & &                           % Labiodental
        \multicolumn{3}{|r}{} &         % Dental
        \multicolumn{3}{l|}{\ipa{R}} &          % Alveolar
                                      % Post-alveolar
        & \ipa{\:r}  \\                          % Retroflex

      \hline Fricative &            % Fricative
        \ipa{F} & \ipa{B} &                 % Bilabial
        \ipa{f} & \ipa{v} &                         % Labiodental
        \ipa{T} & \ipa{D} &                 % Dental
        \ipa{s} & \ipa{z} &                         % Alveolar
        \ipa{S} & \ipa{Z} &                 % Post-alveolar
        \ipa{\:s} & \ipa{\:z}   \\             % Retroflex

      \hline Lat. Fric. &           % Lat. Fricative
        \BlankCell        & \BlankCell        &   % Bilabial
        \BlankCell        & \BlankCell        &   % Labiodental
        \multicolumn{3}{|r}{\ipa{\textbeltl}} &       % Dental
        \multicolumn{3}{l|}{\ipa{\textlyoghlig}} &      % Alveolar
                                      % Post-alveolar
        &   \\                         % Retroflex

      \hline Approx &               % Approx.
        & &                           % Bilabial
        & \ipa{V} &                     % Labiodental
        \multicolumn{3}{|r}{}&                % Dental
        \multicolumn{3}{l|}{\ipa{\*r}} &          % Alveolar
                                      % Post-alveolar
        & \ipa{\:R}   \\                   % Retroflex

      \hline Lat. appr. &           % Lat. Approx
        \BlankCell        & \BlankCell        &   % Bilabial
        \BlankCell        & \BlankCell        &   % Labiodental
        \multicolumn{3}{|r}{}&                % Dental
        \multicolumn{3}{l|}{\ipa{l}}&               % Alveolar
                                      % Post-alveolar
        & \ipa{\:l}  \\                    % Retroflex
      \hline
    \end{tabular}

    \vspace{0.3em}
    \hspace{4.6em}
    \begin{tabular}{|l|cc|cc|cc|cc|cc|}
      \hline &
        \multicolumn{2}{|c|}{\footnotesize{Palatal}} &          % Palatal
        \multicolumn{2}{|c|}{\footnotesize{Velar}} &          % Velar
        \multicolumn{2}{|c|}{\footnotesize{Uvular}} &           % Uvular
        \multicolumn{2}{|c|}{\footnotesize{Pharyng.}} &       % Pharyngeal
        \multicolumn{2}{|c|}{\footnotesize{Glottal}}  \\          % Glottal

      \hline Plosive &                % Plosive
      \ipa{c} & \ipa{\textbardotlessj} &                            % Palatal
        \ipa{k} & \ipa{g} &                         % Velar
        \ipa{q} & \ipa{\;G} &                   % Uvular
        & \BlankCell        &               % Pharyngeal
        \ipa{P}& \BlankCell         \\                % Glottal

      \hline Nasal &              % Nasal
       & \textltailn &                           % Palatal
        & \ipa{N} &                           % Velar
        & \ipa{\;N} &                           % Uvular
        \BlankCell        & \BlankCell        &   % Pharyngeal
        \BlankCell        & \BlankCell         \\   % Glottal

      \hline Trill &                  % Trill
        & &                           % Palatal
        \BlankCell        & \BlankCell        &   % Velar
        & \ipa{\;R}&                      % Uvular
        & &                           % Pharyngeal
        \BlankCell        & \BlankCell         \\   % Glottal

      \hline Tap/Flap &             % Tap /Flap
        & &                           % Palatal
        \BlankCell        & \BlankCell        &   % Velar
        & &                           % Uvular
        & &                           % Pharyngeal
        \BlankCell        & \BlankCell         \\   % Glottal

      \hline Fricative &            % Fricative
        \ipa{\c{c}} & \ipa{J} &               % Palatal
        \ipa{x} & \ipa{G} &                     % Velar
        \ipa{X} & \ipa{K} &                 % Uvular
        \textcrh & \ipa{Q} &                % Pharyngeal
        \ipa{h} & \texthth \\                   % Glottal

      \hline Lat. Fric. &           % Lat. Fricative
        & &                           % Palatal
        & &                           % Velar
        & &                           % Uvular
        \BlankCell        & \BlankCell              % Pharyngeal
        & \BlankCell        & \BlankCell         \\   % Glottal

      \hline Approx &               % Approx.
        & \ipa{j} &                           % Palatal
        & \ipa{\textturnmrleg} &                  % Velar
        & &                           % Uvular
        & &                           % Pharyngeal
        \BlankCell        & \BlankCell         \\   % Glottal

      \hline Lat. appr. &           % Lat. Approx
        & \ipa{L} &                       % Palatal
        & \ipa{\;L} &                     % Velar
        & &                           % Uvular
        \BlankCell        & \BlankCell        &   % Pharyngeal
        \BlankCell        & \BlankCell         \\   % Glottal
        \hline
    \end{tabular}
 \end{small}
  \caption[]{}
  \label{fig:consonants}
\end{figure}

While no phoneme's pronunciation is consistent,
consonant pronunciations can vary more than vowels
because consonant sounds are produced in the context
of the vocal tract position for
the prior or upcoming vowel sound.
In the word ``bat'' for example,
the \ipa{[b]} and \ipa{[t]} phones
occur in the context of \ipa{[\ae]}.
\cite{kroger1993} theorizes that
each consonant sound is composed
of speech gestures which force some
articulators to change in a particular way,
but allow other articulators,
like those involved in the vocalic sound,
to change freely.

The vowels and consonants described so far
represent the most common phonemes used
in daily speech.
However, as with most aspects
of phonetics and phonology,
there are many examples that do not
match the convenient criteria listed above.
For example, \ipa{[w]}, as in ``weep''
is a phoneme consonant that is articulated
with a mostly open vocal tract,
like a vowel;
for this reason, it is sometimes called a semivowel.
Some languages use clicks,
in which inward suction releases a complete constriction
resulting in a loud consonantal sound.
However, we will not simulate
these and other phonemes
that are either uncommon or not present in English;
instead, we will state when a part of our model
will require further work
in order to handle these phonemes,
and when the model can be easily
adapted to handle these phonemes.

\subsubsection{Syllables}

Syllables are groups of one or more phonemes
with a well-specified structure.
They consist of a loud vocalic center,
with optional quieter consonantal components
before and after the center.
Unlike other levels of organization,
the phonemes in a syllable
may not be strictly sequential;
some phonemes may co-occur
when voicing a syllable.

The phonemes that make up a syllable
are typically grouped into
the onset and rime,
where the onset is a cluster of
consonant phonemes,
and the rime is a vowel phoneme
(called the nucleus)
and an optional cluster of consonant
phonemes (called the coda).
In English, the onset is also optional,
though this is not true in all languages.
Figure~\ref{fig:syllables} summarizes this grouping.
Note that other ways to describe
syllables exist
(e.g. the \textit{mora}; \cite{otake1993}),
but we adopt this method
as it is the most common
for Germanic languages like English.

\fig{syllables}{0.6}{???}{???}

Phonetically, syllables are a useful
level of organization because
the higher-level aspects
of an utterance---rhythm, prosody and stress,
for example---are easier to analyze
in terms of their effects on
sequences of syllables rather than
on sequences of phonemes;
it is easy to distinguish
changes in pitch and volume
within and between two syllables,
but not between two phonemes,
because they occur quickly
and can co-occur.

One aspect of the stress of an utterance
is the weight of a syllable.
``Heavy'' or ``strong'' syllables
have a branching rime,
or a branching nucleus,
meaning that they
end in a consonant,
or contain a long vowel or diphthong,
respectively.
``Light'' or ``weak'' syllables
do not have branching rime or nucleus,
and in general are shorter
and quieter.
In English, most vowel phonemes
can only appear in either
heavy or light syllables;
for example, the schwa, \ipa{[@]},
can only appear in light syllables,
and diphthongs like \ipa{[aU]}
can only appear in heavy syllables.
The weight of a syllable determines
whether or not it can be stressed
in an utterance;
specifically, only heavy syllables
can be stressed.

Like all levels of phonetics and phonology,
the above description is a useful simplification
of a more complex phenomenon.
In terms of syllable production,
the onset-nucleus-coda grouping
is not always sufficient.
In English, syllables that may have once
been typical VC syllables have morphed
into ``syllabic consonants,'' in which
the vowel is no longer present;
for example, ``bottle'' is often
pronounced \ipa{[b6tl]},
where \ipa{[l]} is a syllabic consonant.
In terms of syllable recognition,
agreement between native English speakers
is surprisingly low when
asked to segment an utterance
into its component syllables.
Yet, the meaning of the utterance
is understood by all subjects.
Therefore, while the syllable
may be a useful level of organization
for producing utterances,
its role in recognizing them
may be limited.

% ??? Here or somewhere else, summarize
% the ``mental syllabary'' from Levelt and Wheeldon, 1994
% also Burki 2015

\subsubsection{Tone-units}

??? move to discussion

The final level of organization
that we will examine is the tone-unit.
The tone-unit allows us to examine
suprasegmental aspects of speech.
A tone-unit is made up of a serially ordered
sequence of syllables.\footnote{Many
  phoneticians consider a tone-unit to be
  composed of ``feet,'' where a foot is
  a single unit of rhythm.
  However, feet are mostly used when describing
  non-conversational utterances,
  such as those found in music and poetry.
  In this thesis we focus on speech
  as a means of conveying linguistic information,
  and therefore ignore the concept of feet.}

The structure of a tone-unit is similar
to a syllable, except its component parts
are syllables instead of phonemes,
and its components are serially ordered.
A tone-unit must contain a tonic syllable
(sometimes also called the nucleus),
and can optionally contain one or more syllables
in a pre-head, head, or tail section.
The pre-head consists of all syllables
before the first stressed syllable
in a tone-unit.
The head consists of all syllables from
the first stressed syllable
to the tonic syllable.
The tonic syllable is the most significant
syllable in the tone-unit because
pitch changes in the tonic syllable
will occur relative to the tonic syllable;
the tonic syllable is not necessarily
the loudest or most prominently stressed
syllable in the tone-unit,
though it does always contain
a stressed (and therefore heavy) syllable.
The tail consists of all syllables
following the tonic syllable.

% ??? tone-unit structure figure
% maybe don't worry for now

Not all heavy syllables are necessarily stressed;
stress is hypothesized to occur when
more muscle activation is used
to voice a particular heavy syllable
???cite?.
Stress is perceived as a heavy syllable
that is louder, longer, and with
a different pitch or quality compared
to other syllables.
In some languages, rules govern
which syllables receive stress;
for example, in French,
the last syllable in a word is
always stressed.
In English, each word defines
its own stress pattern,
and each utterance can add
additional stresses
that emphasize some words over others.
Additionally, stress is not a binary quantity;
English is typically thought to contain
three stress levels
(primary stress, secondary stress, and unstressed).

Intonation, on the other hand,
is a more straightforward phenomenon to model.
Intonation is the use of pitch changes
to 1) express emotions and attitudes,
2) impart prominence on stressed syllables,
3) exaggerate grammar and syntactic structure,
and 4) clue listeners into what information
is novel and what is thought to be already known.
While intonation can be thought to also include
body language and other prosodic characteristics,
we will focus only on pitch changes.
The tone of a tone-unit
is the overall trajectory of pitch
during the tone-unit.

There are a limited set of possible pitch trajectories
(i.e., tones) in English.
While different sources identify different trajectories,
we will adopt the conventions of ???below
which note six possible pitch trajectories:
high fall, low fall, high rise,
low rise, fall-rise, and rise-fall
(see Figure~???).
The same sequence of syllables
can change its meaning dramatically
by using a different pitch trajectory,
or by changing the position of the
tonic syllable within that pitch trajectory.
The meaning of each pitch trajectory
changes depending on the utterance in question;
we will not investigate meanings further
in this work, as we focus on
sound reproduction rather than on
linguistic meaning.

??? Quick plot of pitch trajectories
% again, deal with later

% cite https://notendur.hi.is/peturk/KENNSLA/25/IPBE/SHELL/25/nucleus.html
% or Cruttenden

It is important to note that when talking about
the pitch trajectory of an utterance,
pitch is always relative to the
upper and lower range of a particular speaker.
In the pitch trajectory plot in Figure~???,
horizontal lines show the upper and lower ranges.

These six trajectories in Figure~???
interact with the pre-head and tail
in predictable ways.
The pre-head is usually low,
but can be high in front of a low stressed syllable.
The tail trajectory can be predicted
by the trajectory of the tonic syllable;
for example, if the tonic syllable falls,
the tail remains low;
if the tonic syllable rises,
the tail continues to rise.
The head, on the other hand,
is independent of the tonic syllable;
it may remain at a high or low level,
or it can rise or fall like
the trajectories associated with the tonic syllable.
The combinations of head and tonic syllable
pitch trajectories also contributes
to the varying meanings conveyed
by the pitch trajectory of a tone-unit.

Finally, one note about
the absence of ``words'' in our treatment
of phonetics and phonology.
While words are clearly an important concept
that we will use colloquially in this thesis,
they are a linguistic construct
rather than a phonological one.
However, as directly applied
to speech recognition and synthesis,
words are more readily recognized
than sequences of syllables with no meaning.
Fortunately, the pioneering work
of ???allen94 which summarizes ???
gives a direct relationship between
the recognition rates of nonsense syllables and words,
which can be used to adjust
recognition rates for
non-linguistic speech.

\subsection{Neurobiology}
\label{sec:prod-neurobio}

Traditionally, speech production
was tightly linked to Broca's area,
a sizable region of the dominant
(usually left, for right-handed individuals)
hemisphere of the frontal lobe,
the inferior frontal gyrus.
Paul Broca made this area famous
through study of two patient
with profound difficulty producing language;
one patient could only produce the word ``tan,''
while the other had only a vocabulary
of five words which he would utter repetitively.
% While speech neurobiology is
% difficult to study in depth
% (invasive studies are typically deemed unethical
% in species that can express themselves
% through language),
While we now have a much better understanding
of the brain regions
involved in speech production,
how these regions are connected
and what information they communicate
to connected areas is still
the subject of active research,
which we hope to contribute to
through generating predictions
from our model.

Modern understanding of
the neurobiology of speech production
views Broca's area among several
areas in the inferior frontal gyrii
of both brain hemispheres
that temporally controls a series
of tightly timed activations
of motor cortex.
\cite{flinker2015}
showed through Granger causality analysis
that Broca's area is activated
before motor cortex,
and while the motor cortex is
engaged in producing vocal tract movements,
Broca's area is relatively silent.
We will discuss its role in
transforming incoming linguistic
information from sensory areas
to representations useful for production
in Section~\ref{sec:sm-neurobio},
and instead focus here on
how motor areas effect vocal tract movements.

Controlling vocal tract muscles
in order to produce recognizable speech
can be thought of as a special case of
general motor control,
in which we aim for relatively
low dimensional targets
(i.e., position in three-dimensional space)
with high dimensional controls
(i.e., muscle contractions).
Despite the number of tightly coordinated
muscle activations that come about
when we make an intentional action,
we do not have to explicitly
think about those individual muscle activations,
which has led to the concept
of motor synergies.
Motor synergies are learned sets of
muscle activations that result
from activation of relatively few
neurons in motor cortex.
Synergies can be flexibly weighted
and combined;
for example, if one has a synergy
for reaching forward and one for
reaching to the left,
activating both synergies
equally could effect
a reach diagonally forward-left.
Many different variations
on the idea of motor synergies exist,
and neurobiological support
for these variants
can be found if certain
analysis techniques are used
(see \cite{tresch2009}
for a review in general
and \cite{smith2004,smith2006}
for evidence of speech synergies).
We limit our discussion
of motor synergies to the abstract concept,
and do not choose a specific variant.
As such, our primary interest
is in what brain areas correspond
to the level of the speech hierarchy
(see Figure~\ref{fig:prod-hierarchy}).

Motor synergies, then,
can be thought of as the highest level
of the human speech motor hierarchy.
There is evidence that these motor synergies
exist in the ventral sensorimotor cortex (vSMC).
\cite{bouchard2013} found that activations
of vSMC during the voicing of a syllable
represent activation of vocal tract articulators,
and that the vSMC is organized somatotopically
(i.e., distinct regions of the vSMC
influence different vocal tract articulators).
\cite{breshears2015} built on this work
by directly stimulating vSMC in humans
who were undergoing brain surgery.
vSMC stimulation in different subsections
resulted in movement of vocal tract articulators.
They also found that the somatotopic organization
of vSMC varied for each subject,
but all subjects' vSMCs were organized
in dorsal-ventral articulator order.

\cite{dewolf2010} integrated neurobiological research
into a model of the motor hierarchy
that involves the interaction of
supplementary motor areas,
basal ganglia, cerebellum,
primary motor cortex,
and nuclei in the brain stem and spinal cord
(see Figure~\ref{fig:noch}).
We assume that vocal tract movements
use the same hierarchy,
with vSMC as the top level of that hierarchy
(i.e., it is the supplementary motor area
for the vocal tract).
\cite{wildgruber2001} provides support
for the role of basal ganglia
and cerebellum in speech,
and \cite{brown2009}
provides support for
the role of primary motor cortex
in speech.

% \fig{noch}{0.4}{???}{???}
% % just steal

The motor synergies for vocal tract movements
provide a method for implementing
vocal tract gestures,
but there remains a link
between the phonemic
speech hierarchy in Figure~\ref{fig:prod-hierarchy}
and the motor hierarchy
in Figure~\ref{fig:noch}.
Neurobiologically,
that missing link appears to be
implemented by the anterior insula,
which is located in the lateral sulcus
separating the temporal lobe from
the parietal and frontal lobes.
As reviewed in \cite{ackermann2004},
the insula seems to be responsible
for the coordination of the up to 100 muscles
that shape the vocal tract during speech
and non-speech vocalizations.
They (and others like \cite{ivry1998})
hypothesize that the mapping from
from vocal tract gestures to articulator movements
is lateralized such that
the left anterior insula operates
discretely on a fast phoneme-level timescale,
while the right anterior insula
operates continuously on a slower
syllable or tone-unit level timescale,
suggesting that these two timescales
would be represented
with separate neural resources
in a neural model of the speech system.

\section{Sensorimotor integration}

Unlike speech recognition and production,
speech-specific sensorimotor integration
is difficult to interrogate through
traditional psychological or linguistic means,
as it is most directly observable
during development and other forms of learning.
We briefly review leading theories
of how we develop and learn speech,
then look at neurobiological studies
of sensorimotor integration.

\subsection{Development and learning}

There are two findings in the
speech development literature
that are relevant to our model.
The first is that in
early developmental phases,
sensorimotor learning
takes place rapidly
and in ways that are difficult
to achieve later in development;
these phases are sometimes referred
to as ``critical'' or ``sensitive'' periods.
Babies up to around a year of age
have a sensitivity for phonemes;
they become increasingly sensitive
to the phonemes that they hear regularly,
and increasingly less sensitive
to phonemes that they hear rarely or never
\cite{kuhl2008}.\footnote{
  Interestingly, it has been shown that
  remaining sensitive to phoneme
  that are not regularly encountered
  is predictive of poor language performance
  at two years of age \cite{kuhl2008}.}
Up until puberty,
children have a sensitivity for
acquiring language in general;
after puberty, formal instruction
is less effective,
and speaking foreign languages
without an accent is more difficult
\cite{hurford1991}.
This finding suggests that
parts of our model
that include learning
through changes in synaptic connection weights
may have qualitative or large quantitative
differences at different developmental stages.

The second finding is that
auditory learning
and speech understanding precede
speech production.
While this may seem obvious,
analogies between how humans acquire speech
and how birds acquire songs suggests that
the vocal babbling of children
may be directed toward the goal of
matching speech templates learned
through auditory input
\cite{bolhuis2010}.
This finding suggests that
we develop a repository of speech templates
that we attempt to match when
learning to produce speech.

Despite the prominent role of sensorimotor integration
in development,
it is not the case that this system
plays no role in the adult speech system.
One influential study by \citeauthor{houde1998)
investigated sensorimotor learning
by manipulating adult speech
through a device that recorded
a subject's voicing a syllable,
adjusted the sound such that the syllable's vowel sound
was adjusted in a predictable way,
and played the adjusted sound back through headphones.
Subjects were able to compensate
for the adjustment such that
subsequently uttered syllables
more closely matched the desired syllable,
and that adjustment generalized
to other syllables that had not yet
been tested.

\subsection{Neurobiology}
\label{sec:sm-neurobio}

Unlike recognition and production,
in which we have a sensor and actuator to measure,
sensorimotor neurobiology
can only be investigated
in neural recordings from cortical areas.
While we do not yet
have a full understanding of
how the brain's speech recognition
and production systems are organized
and interact,
two influential theories have provided
a fruitful framework for many studies.

The first theory
is the dual-stream model of speech processing
proposed by \cite{hickok2007}.
This model is conceptually similar
to the dual-stream model of visual processing,
in which the ventral stream
is involved in object recognition
and the dorsal stream is involved
in guiding actions.\footnote{The dual-stream
  model for visual processing has traditionally
  been though of as the ventral ``what'' pathway,
  and the dorsal ``where'' pathway,
  assigning the dorsal stream
  the role of determining object locations
  \cite{ungerleider1982}.
  However, more recent studies have made
  strong links between dorsal stream
  activity and motor actions
  \cite{andersen1997,rizzolatti1997,rizzolatti2003}.}
In the speech processing case,
the ventral stream
is involved in lexical processing
(i.e., the meanings of incoming sounds),
and the dorsal stream
is involved in sensorimotor action mapping.
In this thesis, we develop
a model of parts of the dorsal pathway,
but developing an
interacting ventral pathway
would be a logical extension.

\cite{hickok2007}
suggest that the dorsal stream
is strongly involved in
speech development,
as development requires auditory feedback
to guide the creation and fine-tuning
of motor pathways to effect speech;
the role of the dorsal pathway may
be reduced over time as motor pathways
reliably produce recognizable speech
\cite{schmidt1975,doyon2003}.
Anatomically, the dorsal stream begins with
the dorsal superior temporal gyrus (STG; i.e., Wernicke's area)
and the superior temporal sulcus (STS),
which both project to
a parietal-temporal boundary area in the Sylvian fissure
(called area Spt).
They propose that the Spt is a sensorimotor interface,
which also takes in input from other sensory modalities
(i.e., visual and somatosensory information).
Spt then projects to higher-level articulatory areas,
including the posterior inferior frontal gyrus (pIFG; i.e., Broca's area),
premotor cortex (PM) and the anterior insula.
As was discussed in
Sections~\ref{recog-neurobio} and \ref{prod-neurobio},
the STG has clear roles in auditory processing,
and the anterior insula has clear roles in
motor processing;
area Spt, therefore, is a strong candidate
for where sensorimotor integration might occur.

\cite{hickok2009}
provide human fMRI evidence that
area Spt is significantly active
during both sensory and motor aspects
of a speech task.
However, \cite{cogan2014}
performed electrocorticography
in human epilepsy patients
and found electrodes that activated
for both sensory and motor aspects
of a speech task across all areas
of the dorsal stream
(including STG, middle temporal gyrus, PM,
and inferior frontal gyrus).
Clearly, there is a large overlap
between the information required for
sensory processing and the information required
for motor processing.

The second influential theory
in speech neurobiology is the
motor theory of speech perception.
The central tenet of this theory
is that perceiving speech
is perceiving vocal tract gestures
\cite{liberman1985,galantucci2006}.\footnote{
  The motor theory of speech perception has a long history
  and has made some claims that are not supported by evidence.
  We will ignore these claims, and focus only on
  the central ideas which have empirical support.}
This makes a hypothesis about
the internal representations of speech,
and is therefore difficult
to interrogate directly,
but several experiments appear
to support this hypothesis.
First, we are able to integrate
gestural information from multiple modalities;
the direct perception of gestures
influences how we perceive the sound.
A well-known demonstration of
this phenomenon is the McGurk effect
\cite{mcgurk1976},
in which the same audio waveform
in perceived as representing a different syllable
when accompanied by a video
showing the lip gesture that produces
the audio waveform.\footnote{
  The McGurk effect can be experienced firsthand through
  a video demonstration at
  \url{https://www.youtube.com/watch?v=G-lN8vWm3m0}.}
Second, in a speech reaction task,
humans react $\sim$26 ms quicker
when imitating a cue syllable
compared to when reacting
to the cue syllable
with a predetermined syllable
\cite{fowler2003}.\footnote{
  For clarity, in the task, subjects
  shadowed a recording voicing the \ipa{[A]} phone,
  which switched to either \ipa{[pA]}, \ipa{[tA]} or \ipa{[kA]} at an
  unspecified time.
  Subject either voiced the syllable that they heard,
  or one of \ipa{[pA]}, \ipa{[tA]} or \ipa{[kA]},
  which was kept constant regardless of what was heard.}
Typically, reaction time experiments
in which the subject produces the same reaction
regardless of the cue (e.g., pressing a button)
achieve faster reaction times
than experiments in which the subject
has a choice of reactions
and the cue is arbitrarily related
to the reaction.
If the cue is nonarbitrarily related
to the response
(e.g., the cue appears on the right side
when a button on the right is to be pressed)
then reaction times are closer to those
of the simple experiments.
The fact that imitating the syllable
(i.e., having a choice of three possible responses,
but receiving non-arbitrary cues)
is faster than a simple reaction
implies that the the information gathered
from the cue is directly related
to the motor response;
since vocal tract gestures are thought
to be the basic unit of speech motor action,
then it is likely that vocal tract gestures
are also an early unit
in speech recognition as well.

Neurobiologically,
the motor theory of speech recognition
cites similar empirical evidence as
the dual-stream model;
i.e., there are brain areas (like Spt)
that are selectively activated
when engaged in both speech recognition
and speech production tasks,
so it is likely that they share
a representation which might be vocal tract gestures.
Indeed, while much of the auditory literature
attempts to decode phonemes
from these brain areas
(e.g., ??? cite),
neural activities representing vocal tract gestures
would necessarily be capable of
decoding phonemes due to the fact that
vocal tract gestures are responsible
for effecting movements that produce
those phonemes.
While they do not make this link directly,
\cite{mesgarani2014}
showed support for the motor theory of speech recognition
by showing that in the STG,
which is typically thought of
as an auditory-specific brain region,
there is topographic organization
based on the acoustic properties
of the phoneme being listened to
(see Figure~\ref{fig:stg-phones}).
These acoustic properties are
directly linked to the articulators
producing the phoneme
(e.g., labial versus dorsal consonant phonemes)
and therefore it is plausible that
the STG represents vocal tract gestures.

\fig{stg-phones}{0.8}{???}{???}

Given that neurobiological evidence can
support both theories,
we therefore adopt the view that
the dorsal auditory stream
represents vocal tract gestures explicitly,
though this may or may not be the case
in the ventral stream.
Our model, which purports to replicate
functions of the dorsal stream only,
will reflect these two theories
by explicitly representing vocal tract gestures.

\subsubsection{Automatic speech recognition with production information}
\label{sec:asr-prod}

In terms of higher-level biological organization,
almost all speech recognition systems
can be mapped most directly
to the ventral stream of the human speech system,
as they attempt to decode
lexical information in order to
transcribe speech to text.
In this thesis, we aim to model
the dorsal stream of the human speech system,
which we hypothesize follows the
motor theory of speech recognition,
meaning that it uses vocal tract gestures
as the basic unit of representation.

Several labs have investigated
using measurements
related to speech production
as part of a speech recognition system
(see \cite{king2007} for a review).
The primary advantage of this approach
is that speech production information
is not locked to each phoneme,
so it is not as sensitive to the
differing acoustic realizations
of the same phoneme.
The existence of large datasets
with continuous acoustic
and articulator position recordings
(e.g., \cite{westbury1990,wrench2000,steiner2012})
has spurred development of
systems that use both
acoustic and articulatory information
as input to an ASR system.
\cite{zlokarnik1995}
used continuous acoustic and articulatory
information with an HMM-based system
and reduced word error rates
by more 60\% (relative).
\cite{eide2001}
augmented the feature vector
in a standard HMM system
articulatory features and
reduced word error rates
by 34\% (relative).
Similar studies have been done
with traditional artificial neural networks
(e.g., \cite{kirchhoff2002})
and dynamic Bayesian networks
(e.g., \cite{stephenson2000,stephenson2004}).

Deep neural networks have been used for
a related task,
acoustic-articulatory speech inversion.
Here, articulatory information
is decoded from acoustic information;
that is, the input of the network
is still an acoustic feature vector,
but the desired output
is articulator positions.
\cite{uria2011}
obtained a root mean square error
of 0.95 mm on the MNGU0 dataset,
which is 0.04 mm better than prior
efforts using Guassian mixture models
\cite{richmond2009}.
The extracted articulatory information
could be used by another ASR system
to lower phone or word error rates.

The aforementioned results
use continuous articulator positions.
Some work has also been done
relating this work to
vocal tract gestures.
For example,
\cite{zhuang2008,zhuang2009}
showed that the vocal tract gesture score
can be estimated
from the continuous articulator positions,
which can then be used
to do word classification with
over 80\% accuracy on a synthesized dataset.
\cite{nam2010,nam2012}
showed that a gesture score
can be estimated from natural speech
through a landmark-based time alignment procedure;
however, this model assumed that the
words in the utterance were known \textit{a priori},
and applied their technique primarily
for annotating data sets with time-aligned gesture scores.
Surprisingly, we have been unable to find any literature
attempting to extract vocal tract gesture scores
from acoustic information directly,
nor through a pipeline in which
articulator positions are decoded,
and then gesture scores are estimated
from those articulator positions.

\subsection{Integrated recognition and production systems}
\label{sec:bg-diva-kroger}

For the purposes of this thesis,
we consider a system to be integrated
if speech recognition and production
share internal representations
(i.e., representations other than
the incoming audio waveform or text),
and are connected such that
the output of one system
can be used to improve
the performance of the other system.
By this definition,
the most widely used
speech recognition and production systems
are not considered integrated
even though they can be used
in a conversational manner;
virtual agents like Apple's Siri
can both recognize and produce speech,
but recognized speech is converted
to text before speech synthesis occurs,
and the speech production system
cannot be modified over time
as it relies on a large corpus
of recorded speech.

The most well known integrated system
is called DIVA
(Direction Into Velocities of Articulators;
see Figure~\ref{fig:diva};
\cite{guenther1995,guenther2004,guenther2006,guenther2006a}).
DIVA consists of an articulator synthesizer,
and several interconnected artificial neural networks
that drive the articulatory synthesizer
and process auditory and somatosensory feedback
in order to improve future synthesized speech.
DIVA is not pre-programmed with a repository
of programs to effect various sounds;
instead, it mimics human speech development
by learning to control the synthesizer
through an initial babbling phase
in which random articulator movements
are associated with auditory
and somatosensory feedback.
DIVA has been used to explain
speech production phenomena
like motor equivalence, contextual variability,
and anticipatory and carryover coarticulation
\cite{guenther1995,guenther2003,nieto2005}.
Additionally, all of the neural networks
in the model have been mapped
to brain regions that are hypothesized
to perform similar functions
as the neural networks in the model.

\fig{diva}{0.7}{???}{???}

DIVA is a productive research tool
for studying speech development.
However, I do not believe that DIVA
in its current form
can scale to maintaining conversations
with adult vocabularies.
Currently, DIVA is focused on speech production,
so while it can incorporate acoustic feedback
to learn better speech production,
there is no clear path
to incorporate speech recognition
and higher-level linguistic capabilities.
Additionally, DIVA currently represents
stored speech plans with one artificial
neuron per speech plan,
where a speech plan can correspond to
a single phoneme, a syllable, or one or more words.
In \cite{guenther2006a},
they note that ``it is expected that
premotor cortex sound maps in the brain
involve distributed representations
of each speech sound.''
Using such distributed representations,
as we do in our model,
would require several changes to DIVA's
structure and learning algorithms.
While these changes are possible,
they would represent a significant
modification that may affect
empirical results obtained
with the current version of DIVA.

In addition to using highly localist representations,
DIVA has other biological plausibility issues.
It represents auditory feedback using the first
three formant frequencies,
but does not provide an account of
how the brain might extract those
frequencies from the incoming air pressure levels.
The artificial neural networks
consist of homogeneous neurons
with non-spiking linear activation functions,
though the learning procedure
uses a local Hebbian learning rule.
Auditory and somatosensory feedback
is provided to the model with no delay,
making the learning procedure
much easier than a learning procedure
that must deal with delayed feedback,
as happens in the real world.

\cite{kroger2009} proposed
a large integrated model
similar to DIVA in that
it is composed of
several artificial neural networks
and learns to produce syllables
through a babbling stage
in which feedback from
auditory and somatosensory systems
tunes the weights between
neural populations
(see Figure~\ref{fig:kroger}).
The primary improvements in
Kr\"{o}ger's model compared to DIVA
are the introduction of a phonetic map,
and a motor planning network.

\fig{kroger}{0.7}{???}{???}

In DIVA, auditory and somatosensory feedback
is directly associated with a cell
in the speech sound map,
meaning that each sound must be learned separately.
This approach does not generalize across
similar sounds, nor will it scale
to large vocabularies of speech sounds.
Kr\"{o}ger's model overcomes this limitation
by explicitly decoding phonetic information
from auditory feedback.
It then uses that phonetic information
to generate explicit motor plans.
Motor plans act as parameterizations
of the possible vocal tract gesture scores
that can be produced by the model;
each motor plan is defined by
five parameters which define
the vocalic state,
the gesture-performing articulator,
and the location of articulation.
These five parameters are used
to generate vocal tract gestures,
which are then mapped onto
movements of articulators
in a three-dimensional vocal tract model.

In Kr\"{o}ger's neurocomputational model,
phonetic information is extracted
in an unsupervised manner
using self-organizing maps
\cite{kohonen1982,kohonen2007},
which cause
local clusters of neurons
in the phonetic map to become active
when incoming auditory information
(in the form of the first three formants)
is similar to certain patterns
seen during training.
The maps are able to
discriminate between three vowel phonemes
and three consonant phonemes;
it is not clear whether
maps trained in this way
can scale to discriminating
between the number of phonemes
in a realistic language (over 40).
However, it is clear that an approach
which uses a finite set of intermediate representations
(i.e., phonemes)
between auditory input
and learned motor sequences
will scale better than DIVA's approach
that does not use an intermediate representation.

% ??? Update Kroger section for Cao and 2015 paper

In contrast to these two models,
the model that we present
in the subsequent chapters does not focus on
modeling speech development,\footnote{
  The methods used by DIVA and Kr\"{o}ger's
  neurocomputational model to emulate
  speech development could be adapted
  and added to our model.
  However, since these problem have existing
  solutions in these two models,
  we have focused on other aspects
  of speech recognition and production
  as being of greater scientific interest.}
and instead implements
portions of the human speech system
not modeled by these two models,
and aims to be more biologically realistic.
Biological realism is achieved
by more sophisticated neural network models
that use distributed representations
that can scale to the level
of adult vocabularies,
and by using a human auditory model
to process incoming sounds
rather than preprocessing the sound
to obtain the first three formants.
In addition, while Kr\"{o}ger model
improves on the DIVA model by
introducing phoneme representations,
we follow the motor theory
of speech representation by
explicitly decoding vocal tract gestures
from incoming sound,
which makes integration between
the perception and production aspects
of the model more straightforward.
