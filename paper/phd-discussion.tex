\chapter{Discussion}

\section{Summary and implications of results}

\subsection{Neural cepstral coefficients}

The most important and somewhat surprising result
is that NCCs can yield significantly
better training and test accuracy rates than MFCCs,
which are the most commonly used
feature vector in ASR systems.
Our goal in these experiments was to
show that NCCs are at least as good as MFCCs,
as the primary motivation behind
NCCs is to generate a feature vector
entirely in spiking neurons,
therefore making it suitable
for use in Sermo,
and more broadly being applicable
in neuromorphic systems using
a silicon cochlea.
The fact that they also yielded
better accuracy rates
is promising,
though it should be kept in mind
that these results
do classification through
an ideal statistical linear SVM classifier,
rather than being embedded
in a full continuous ASR system.
It remains to be seen
whether NCCs will be successful
in that more realistic setting.

One advantage of NCCs is that
no explicit normalization is required,
as shown in Section~???,
where it was determined
that NCCs are best used
without z-scoring, unlike MFCCs.
It is important to note, however,
that NCCs do still employ a method
of normalization;
spiking neurons have a refractory period,
meaning that they have a strict maximum firing rate,
which puts an effective limit on the
decoded output of an ensembles.
Unlike MFCCs, however,
this normalization is applied
for every step in the pipeline,
rather than just once at the end.
It is possible that
the normalization inherent in
spiking neural networks
is responsible for the better performance
compared to MFCCs.

While a similar argument could be made
for why adding derivatives had
a deleterious effect on
NCC test accuracy,
it is also possible that
using offline SVM classification
is responsible for decreased performance.
Having many more features
may make the supervised learning procedure
more difficult,
or perhaps since we have the full history,
the derivative is redundant
and only makes it more difficult
to linearly separate data.

We view the final NCC experiment,
in which the auditory periphery model was varied,
as being the main contribution
of this model.
Because the model bridges the gap
between detailed auditory periphery models
and automatic speech recognition systems,
we are able to do apples-to-apples comparisons
of how well the periphery models
process realistic speech samples.
In general, the more realistic the model,
the better it is for speech,
with the notable exception of the Gammatone filter,
which is the least realistic,
but the cheapest to compute,
and as accurate as
the most complicated Tan Carney model.

There are two reasons why we might
be skeptical of the impressive Gammatone results.
For one, the determination of good parameter sets
in the other experiments in Section~???
all used the Gammatone filter.
It is possible that performing the same
experiments with other filters
would find parameters sets that
allow those auditory periphery models
to perform as well or better than
the Gammatone filter.
Additionally, arguably the most difficult part
of continuous ASR systems is
determining where phone boundaries lie,
rather than classifying pre-segmented phones.
While we believe that the experiments
performed in Section~???
are a useful comparison between MFCCs and NCCs,
some of the benefits of more sophisticated models
like the Tan Carney model
may lie in their ability to segment speech.
The same argument holds
for adaptive neuron types;
while they may not be essential
for differentiating between pre-segmented speech samples,
they may be essential for
segmenting continuous speech.
Looking at the impact of these
NCC representation choices
in a continuous ASR system
is therefore an important next step.

\subsection{Syllable sequencing and production}

The syllable sequencing and production model
can generate production information trajectories
for up to ??? syllables from a repository of ???,
and those trajectories can result in
intelligible speech when synthesized
with an articulatory synthesizer.

It is surprisingly that the model is able
to perform at relatively high speeds
(2--??? Hz),
but has trouble with lower speeds.
We believe the source of this difficulty
is in the sequencer network.
When a syllable is about to end,
it starts the DMP network
associated with the next syllable,
while also pushing the current syllable's
DMP network to the starting position.
When frequencies are high,
the drive to the starting position
is approximately the same as the
intrinsic dynamics, so the system
moves as expected.
However, for low frequencies,
the additional drive in the current syllable
forces it to finish the current syllable
too quickly.

??? the size of the syllabary shouldn't matter... but it does..?

% ??? note that we are doing a lot of stuff
% here with inhibition.
% While it is possible to do this stuff
% without neurons in a similar manner,
% the ability to directly inhibit ensembles
% improves the robustness of the network,
% which we take as a concrete benefit
% of neurally implementing the algorithm

% ??? temporal output associative memories
% provide a robust link between the discrete,
% symbolic world of high-level representations
% that can be flexibly combined (see, e.g., Dan's RPM work)
% and the continuous, subsymbolic world
% of low-level representations that must
% be precisely timed (see e.g., Travis's work).

% ??? temporal input associative memories
% provide an analogous link between
% continuously varying low-level sensory inputs
% and high-level symbolic representations

% ??? should we do all of this with direct production info
% and not gestures? It's worth looking into...

% ??? Do some fine tuning?

% \subsection{Vocal tract parameters}

% ??? Instead of gestures, we could instead
% generate parameters. Pro: continuous, lower dimension.
% Con: can't optimize the mapping between
% gesture and parameters.
% However, worth trying out?

% Or, obvious next step!

\subsection{Syllable classification}

In its current state,
the syllable classification model
is extremely sensitive to the
frequency of the syllable
(see Figure~???).
Like the production model,
it performs poorly for
slow syllable trajectories,
in this case due to
a large number of insertions
(although it could be argued that
repeated identification of the correct
syllable is acceptable).
Unlike the production model,
performance is best at around 1.5 Hz,
then monotonically worsens
for higher frequencies.
We believe the decreased accuracy
is due to a coupling between
the speed of the syllable
and other parameters in the model
(e.g., the scale).
In other words, the iDMPs
must progress their state forward
at a higher rate when
the syllable is voiced at a higher rate.
Methods for estimating speech rate
should therefore be investigated
and used to dynamically modify
the effective scale
depending on the overall speech rate.

The poor results of the model
obtained when increasing the number
of syllables in the syllabary
indicate that the competition
introduced to isolate a relatively small
number of iDMPs is insufficient
to allow only one syllable iDMP to be active.
However, the model does operate well
with few syllables,
and many syllables are similar
and would be often confused by people
without linguistic context.
Therefore, we conclude that
the iDMP networks are still suitable
for trajectory classification,
but are not used in a
drift-diffusion style classification system
in which the first iDMP to reach
a threshold value makes a classification.
Instead, we hypothesize that
the sensorimotor integration system
would allow the iDMPs to continuously operate,
and integrate the information from those iDMPs
as well as information from other sources
(including top-down linguistic influences)
to make a final determination
of the uttered syllable.
Aside from sensorimotor learning,
it is possible that
this type of syllable classifier
may only be used in cases where
the linguistic systems
disambiguate between two similar choices anyhow.



% \subsection{Extensive use of DMPs}

% DMPs: they are useful.
% For different things though, depending on the task.
% Here are some guidelines for
% what DMPs are useful for what, and when.

% Ramping DMPs: require constant input; discrete.
% Good for inverse case.

% Deadzone DMPs: one-time input; discrete.
% Good to effectively call a temporal function.

% Rhythmic DMPs: one-time input; rhythmic.
% To do discrete, must explicitly turn off.
% Kind of essential for possibly repetitive action.

% Perhaps there's some useful generalization of these?
% I.e., something that has the benefits of all but
% few downsides?

\section{Comparison to existing models}

\section{Contributions}

% ??? we made a speech recognition thing

% ??? we made a synthesizer

% ??? we made a neural control method for synthesizers

% ??? we put them together

% ??? mostly, we've integrated existing parts in a large scale model

% ??? none of the models by themselves are terribly groundbreaking,
% and can use some work to fix up,
% but the contextual framework of Sermo
% is a large undertaking,
% but we believe it can provide the basis
% for a ton of worthwhile future research.

% ??? additionally, while my stuff is applied to speech,
% the various networks and ideas (the sequencer network,
% trajectory classification, etc) are useful/necessary
% for dealing with any temporally varying stuff in spaun, e.g. gestures, etc.
% I.e., temporal working memories are likely to be
% a big part of future iterations of Spaun.

\subsection{Contributions to computer science}

% Mostly we've talked about brain modeling,
% arguably part of the domain of neuroscience,
% and control, which is traditionally an engineering topic.
% But, this is a CS thesis, so there should be some CS contributions.

% somewhat disappointing: the point of Sermo
% and the goal of the thesis is
% to close the loop, and create an integrated system
% that speaks and hears itself.
% the missing connection to close the loop
% is to decode production information (vocal tract gestures)
% from auditory information (power spectra and cepstra).
% we hope to present this as a machine learning problem
% (benchmark) that can help make machine learning progress

\section{Predictions}

% need more cortex for consonants than vowels?

% - lots of people talk about spectro-temporal features,
%   gabor filters across time and space, etc.
%   Those are useful models, but not directly implementable
%   in neurons; we have to have everything available at
%   the same timestep, so derivatives make more sense

\subsection{Mapping of model to brain areas}

% In the background we discussed the brain areas
% involved in speech recognition and production,
% in part to place connectivity constraints
% on the model as a whole.
% However, in order to generate testable predictions,
% we can also impose a mapping from
% the neural structures in the model
% to speech-related brain regions.

% ??? do, and make predictions based on this.

% ??? possible issues:

% ??? cepstral coefficients currently require activity
% from all frequencies. But, this may not be anatomically
% consistent. Could be possible, instead, to do this
% hierarchically.

\section{Limitations}

% iDMPs aren't complete yet; we recover
% the canonical system state, but more work
% needs to be done to get the point attractor

\section{Future work}

% ??? add bursting neurons in both production
% and recognition systems for better timing

% ??? more sophisticated classification mechanisms
% in classification section

% ??? add speed control to the DMPs;
% pretty easy, just add a dimension to the oscillator.

% ??? neural implementation of mapping
% from gesture sequence to articulator positions

% Summary of things from other sections:

% \subsection{Recognition system}

% How to deal with semivowels, semiconsonants and glides?
% Should it just be one monolithic phoneme detector?

% Represent prosody in a second feature layer (hierarchical organization).

% Learn all of this stuff rather than optimizing for it.

% How to set baseline pitch and volume on the fly?

% Can we use artificial cochleas as is?

% \subsection{Synthesis system}

% We can use this synthesis system to explore
% important phonological questions.
% For example, syllabic consonants
% Do they sound right as separate syllables?
% Or do they sound right as protracted versions
% of the analogous syllable with the vowel included?
% Can these be distinguished from one another?

% We haven't done forward model prediction
% to determine how to adapt
% when things go wrong.
% However, since we've implemented everything
% with the NEF and SPA,
% we can just use the REACH model.

% ??? there's also a bunch of prosodic stuff;
% this could be added (see comments below)

% Aside from phonemes, we also represent pitch and volume.
% Neither of these features are useful
% as an absolute quantity---most humans are poor
% at judging absolute pitch and volume ???cite---so
% we aim to represent relative pitch and volume.
% In both of these cases,
% we must determine a baseline pitch or volume,
% and a method for comparing the current
% pitch or volume to the baseline.

% In speech, baseline pitch is primarily determined
% by speaker identity.
% Each speaker has a baseline pitch,
% determined in part by the shape of their vocal folds,
% so it is likely that we learn
% and remember the baseline pitch
% of speakers that we interact with frequently.
% On the other hand,
% changes in baseline pitch
% (e.g., through illnesses affecting the vocal tract)
% require little to no adaptation period,
% so the mechanism through which we determine
% a speaker's baseline pitch
% is likely to be relatively simple
% and flexible.
% In this work,
% we will not posit a neural mechanism
% for determining baseline pitch,
% and will instead compute it
% from the data offline and provide it as input.

% Given the baseline pitch as input,
% relative pitch will be computed
% primarily through neural inhibition.
% See section ???implementation
% for how this is accomplished.

% Unlike pitch, baseline volume
% is not speaker specific;
% it is easy to notice when a speaker
% talks louder or quieter
% than the norm.
% ???baseline is the overall activity
% of a long-timescale filtered version
% of the current moment's power spectrum?
% ???relative volume is that minus a
% short-timescale filtered version?
% So it's basically just a derivative?
% ???not sure if we even care about volume
% to be honest...
% % http://www.sengpielaudio.com/calculator-loudness.htm

% \section{Syllable consolidation}

% ??? I didn't get to this,
% but it can definitely be done.

% Go from vocal tract gesture score
% to audio signal, in neurons.

% Learn a syllable recognition
% and a syllable production
% network from this.

% Contrast this to syllable production because it's
% an infrequently voiced syllable.

% \subsection{Bootstrapped syllable learning}

% ??? A full picture of speech development involves:
% learning vocal tract gestures
% through reinforced motor babbling,
% learning basic syllables
% through mimicry?,
% and scaling up to the elements of this model.
% While we think that our model
% is a useful starting point for such
% a developmental model,
% as it provides an end-target,
% we don't claim to do this kind of structural learning.
% However, we believe that error-based learning
% could result in this kind of system;
% to show that this is a possibility,
% we consider a minimal learning situation:
% learning voice a new syllable
% given a set of existing syllables.
% This type of thing probably happens
% in second language acquisition,
% when novel combinations of phonemes
% are encountered,
% or even during first language acquisition
% as pronunciation is refined over
% the course of one's life;
% words that were once awkward combinations
% of many syllables are compressed into
% nearly equivalent sequences of fewer,
% more complex syllables.

% As opposed to conversational shadowing,
% which highlights the high-level strengths
% of the integrated speech system,
% syllable learning highlights
% the low-level strengths of this system.
% Syllable learning involves
% learning a novel set of gestures
% and associated articulator trajectories
% that will voice a syllable
% that is encountered for the first time.

% We call our syllable learning system ``bootstrapped''
% because we assume that our system
% has an existing repertoire of syllables
% that it is already able to voice.
% These existing syllables will be
% used in learning the new syllable.
% Bootstrapped syllable learning contrasts with
% the type of syllable learning
% done as an infant and toddler,
% which uses reinforced speech babbling
% to learn novel syllables.
% While we believe that learning syllables
% from babbling is an important research direction
% that can be explored in this system,
% it has also been explored in many other systems
% (???cite Diva etc),
% and so we have chosen to leave this type of learning
% for future work.

% The bootstrapped syllable learning system
% learns new syllables in three steps.

% \begin{enumerate}
% \item Initialize the new syllable from the most
%   similar existing syllable.
%   For example, when learning to voice
%   the syllable /ba/, the system should
%   start from the syllable /fa/ if it is known.
% \item Swap compatible speech gestures.
%   For example, vowel producing gestures
%   would be compatible, allowing for modifying
%   a /ba/ to a /bu/, and so on.
%   The choice of which gesture to swap and
%   how to swap it will be informed by
%   ???figure out.
% \item Fine-tune the voiced syllable
%   until it can be recognized as the
%   syllable to be learned.
%   ???more
% \end{enumerate}

% ??? NB: the oscillator being learned should exist outside of
% the normal speech system so that they can both run
% at the same time, but there should be a switch kind of thing
% to put the new syllable through.

% ??? hypothesis (not sure where to put this):
% the mapping between phoneme to gesture
% is such that is not advantageous
% to represent in the synthesis
% (maybe also recognition?) system(s).
% Therefore, it may be the case that
% not all people have phoneme representations.
% However, we propose that phonemes
% are a useful construct for learning to voice
% novel syllables in a second-language learning situation.

% These steps require several systems
% that have already been implemented
% in the recognition and synthesis systems separately;
% for example, the ability to compare
% voiced syllables to those already known
% is one of the primary goals of the recognition system itself,
% so it can be leveraged when trying to learn new syllables.
% However, these steps also point to new systems
% that must be implemented.
% First, the system requires a method
% to transfer a syllable-producing function
% from one ensemble to another.
% Second, some knowledge of which gestures
% are compatible must be built into the system.
% Finally, ???fine-tuning.

% It has been shown that in second-language learning,
% slowing down the syllable can have
% a significant increase in learning effectiveness
% ???cite.
% We believe that our system emulates
% how a native speaker of one language
% would learn to voice novel syllables in a second language.
% Therefore, we predict that slowing down
% the speed at which the syllables are heard
% and uttered will improve ???learning speed
% and / or quality of learned syllable.

% ??? Mental syllabary (1-s2.0-S009394X...pdf)
% supports our architecture
