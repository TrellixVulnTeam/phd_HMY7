\chapter{Discussion}

\section{Reflection on results}

\section{Comparison to existing models}

\section{Contributions}

??? we made a speech recognition thing

??? we made a synthesizer

??? we made a neural control method for synthesizers

??? we put them together

??? mostly, we've integrated existing parts in a large scale model

\subsection{Contributions to computer science}

Mostly we've talked about brain modeling,
arguably part of the domain of neuroscience,
and control, which is traditionally an engineering topic.
But, this is a CS thesis, so there should be some CS contributions.

\section{Predictions}

need more cortex for consonants than vowels?

- lots of people talk about spectro-temporal features,
  gabor filters across time and space, etc.
  Those are useful models, but not directly implementable
  in neurons; we have to have everything available at
  the same timestep, so derivatives make more sense

\subsection{Mapping of model to brain areas}

In the background we discussed the brain areas
involved in speech recognition and production,
in part to place connectivity constraints
on the model as a whole.
However, in order to generate testable predictions,
we can also impose a mapping from
the neural structures in the model
to speech-related brain regions.

??? do, and make predictions based on this.

??? possible issues:

??? cepstral coefficients currently require activity
from all frequencies. But, this may not be anatomically
consistent. Could be possible, instead, to do this
hierarchically.

\section{Future work}

Summary of things from other sections:

\subsection{Recognition system}

How to deal with semivowels, semiconsonants and glides?
Should it just be one monolithic phoneme detector?

Represent prosody in a second feature layer (hierarchical organization).

Learn all of this stuff rather than optimizing for it.

How to set baseline pitch and volume on the fly?

Can we use artificial cochleas as is?

\subsection{Synthesis system}

We can use this synthesis system to explore
important phonological questions.
For example, syllabic consonants
Do they sound right as separate syllables?
Or do they sound right as protracted versions
of the analogous syllable with the vowel included?
Can these be distinguished from one another?

We haven't done forward model prediction
to determine how to adapt
when things go wrong.
However, since we've implemented everything
with the NEF and SPA,
we can just use the REACH model.
