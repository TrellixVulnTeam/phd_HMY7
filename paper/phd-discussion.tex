\chapter{Discussion}

\section{Reflection on results}

\subsection{Syllable sequencing and production}

% ??? note that we are doing a lot of stuff
% here with inhibition.
% While it is possible to do this stuff
% without neurons in a similar manner,
% the ability to directly inhibit ensembles
% improves the robustness of the network,
% which we take as a concrete benefit
% of neurally implementing the algorithm

% ??? temporal output associative memories
% provide a robust link between the discrete,
% symbolic world of high-level representations
% that can be flexibly combined (see, e.g., Dan's RPM work)
% and the continuous, subsymbolic world
% of low-level representations that must
% be precisely timed (see e.g., Travis's work).

% ??? temporal input associative memories
% provide an analogous link between
% continuously varying low-level sensory inputs
% and high-level symbolic representations

\subsection{Extensive use of DMPs}

% DMPs: they are useful.
% For different things though, depending on the task.
% Here are some guidelines for
% what DMPs are useful for what, and when.

% Ramping DMPs: require constant input; discrete.
% Good for inverse case.

% Deadzone DMPs: one-time input; discrete.
% Good to effectively call a temporal function.

% Rhythmic DMPs: one-time input; rhythmic.
% To do discrete, must explicitly turn off.
% Kind of essential for possibly repetitive action.

% Perhaps there's some useful generalization of these?
% I.e., something that has the benefits of all but
% few downsides?

\section{Comparison to existing models}

\section{Contributions}

% ??? we made a speech recognition thing

% ??? we made a synthesizer

% ??? we made a neural control method for synthesizers

% ??? we put them together

% ??? mostly, we've integrated existing parts in a large scale model

% ??? none of the models by themselves are terribly groundbreaking,
% and can use some work to fix up,
% but the contextual framework of Sermo
% is a large undertaking,
% but we believe it can provide the basis
% for a ton of worthwhile future research.

% ??? additionally, while my stuff is applied to speech,
% the various networks and ideas (the sequencer network,
% trajectory classification, etc) are useful/necessary
% for dealing with any temporally varying stuff in spaun, e.g. gestures, etc.
% I.e., temporal working memories are likely to be
% a big part of future iterations of Spaun.

\subsection{Contributions to computer science}

% Mostly we've talked about brain modeling,
% arguably part of the domain of neuroscience,
% and control, which is traditionally an engineering topic.
% But, this is a CS thesis, so there should be some CS contributions.

% somewhat disappointing: the point of Sermo
% and the goal of the thesis is
% to close the loop, and create an integrated system
% that speaks and hears itself.
% the missing connection to close the loop
% is to decode production information (vocal tract gestures)
% from auditory information (power spectra and cepstra).
% we hope to present this as a machine learning problem
% (benchmark) that can help make machine learning progress

\section{Predictions}

% need more cortex for consonants than vowels?

% - lots of people talk about spectro-temporal features,
%   gabor filters across time and space, etc.
%   Those are useful models, but not directly implementable
%   in neurons; we have to have everything available at
%   the same timestep, so derivatives make more sense

\subsection{Mapping of model to brain areas}

% In the background we discussed the brain areas
% involved in speech recognition and production,
% in part to place connectivity constraints
% on the model as a whole.
% However, in order to generate testable predictions,
% we can also impose a mapping from
% the neural structures in the model
% to speech-related brain regions.

% ??? do, and make predictions based on this.

% ??? possible issues:

% ??? cepstral coefficients currently require activity
% from all frequencies. But, this may not be anatomically
% consistent. Could be possible, instead, to do this
% hierarchically.

\section{Limitations}

% iDMPs aren't complete yet; we recover
% the canonical system state, but more work
% needs to be done to get the point attractor

\section{Future work}

% ??? add bursting neurons in both production
% and recognition systems for better timing

% ??? more sophisticated classification mechanisms
% in classification section

% ??? add speed control to the DMPs;
% pretty easy, just add a dimension to the oscillator.

% ??? neural implementation of mapping
% from gesture sequence to articulator positions

% Summary of things from other sections:

% \subsection{Recognition system}

% How to deal with semivowels, semiconsonants and glides?
% Should it just be one monolithic phoneme detector?

% Represent prosody in a second feature layer (hierarchical organization).

% Learn all of this stuff rather than optimizing for it.

% How to set baseline pitch and volume on the fly?

% Can we use artificial cochleas as is?

% \subsection{Synthesis system}

% We can use this synthesis system to explore
% important phonological questions.
% For example, syllabic consonants
% Do they sound right as separate syllables?
% Or do they sound right as protracted versions
% of the analogous syllable with the vowel included?
% Can these be distinguished from one another?

% We haven't done forward model prediction
% to determine how to adapt
% when things go wrong.
% However, since we've implemented everything
% with the NEF and SPA,
% we can just use the REACH model.

% ??? there's also a bunch of prosodic stuff;
% this could be added (see comments below)

% Aside from phonemes, we also represent pitch and volume.
% Neither of these features are useful
% as an absolute quantity---most humans are poor
% at judging absolute pitch and volume ???cite---so
% we aim to represent relative pitch and volume.
% In both of these cases,
% we must determine a baseline pitch or volume,
% and a method for comparing the current
% pitch or volume to the baseline.

% In speech, baseline pitch is primarily determined
% by speaker identity.
% Each speaker has a baseline pitch,
% determined in part by the shape of their vocal folds,
% so it is likely that we learn
% and remember the baseline pitch
% of speakers that we interact with frequently.
% On the other hand,
% changes in baseline pitch
% (e.g., through illnesses affecting the vocal tract)
% require little to no adaptation period,
% so the mechanism through which we determine
% a speaker's baseline pitch
% is likely to be relatively simple
% and flexible.
% In this work,
% we will not posit a neural mechanism
% for determining baseline pitch,
% and will instead compute it
% from the data offline and provide it as input.

% Given the baseline pitch as input,
% relative pitch will be computed
% primarily through neural inhibition.
% See section ???implementation
% for how this is accomplished.

% Unlike pitch, baseline volume
% is not speaker specific;
% it is easy to notice when a speaker
% talks louder or quieter
% than the norm.
% ???baseline is the overall activity
% of a long-timescale filtered version
% of the current moment's power spectrum?
% ???relative volume is that minus a
% short-timescale filtered version?
% So it's basically just a derivative?
% ???not sure if we even care about volume
% to be honest...
% % http://www.sengpielaudio.com/calculator-loudness.htm

% \section{Syllable consolidation}

% ??? I didn't get to this,
% but it can definitely be done.

% Go from vocal tract gesture score
% to audio signal, in neurons.

% Learn a syllable recognition
% and a syllable production
% network from this.

% Contrast this to syllable production because it's
% an infrequently voiced syllable.

% \subsection{Bootstrapped syllable learning}

% ??? A full picture of speech development involves:
% learning vocal tract gestures
% through reinforced motor babbling,
% learning basic syllables
% through mimicry?,
% and scaling up to the elements of this model.
% While we think that our model
% is a useful starting point for such
% a developmental model,
% as it provides an end-target,
% we don't claim to do this kind of structural learning.
% However, we believe that error-based learning
% could result in this kind of system;
% to show that this is a possibility,
% we consider a minimal learning situation:
% learning voice a new syllable
% given a set of existing syllables.
% This type of thing probably happens
% in second language acquisition,
% when novel combinations of phonemes
% are encountered,
% or even during first language acquisition
% as pronunciation is refined over
% the course of one's life;
% words that were once awkward combinations
% of many syllables are compressed into
% nearly equivalent sequences of fewer,
% more complex syllables.

% As opposed to conversational shadowing,
% which highlights the high-level strengths
% of the integrated speech system,
% syllable learning highlights
% the low-level strengths of this system.
% Syllable learning involves
% learning a novel set of gestures
% and associated articulator trajectories
% that will voice a syllable
% that is encountered for the first time.

% We call our syllable learning system ``bootstrapped''
% because we assume that our system
% has an existing repertoire of syllables
% that it is already able to voice.
% These existing syllables will be
% used in learning the new syllable.
% Bootstrapped syllable learning contrasts with
% the type of syllable learning
% done as an infant and toddler,
% which uses reinforced speech babbling
% to learn novel syllables.
% While we believe that learning syllables
% from babbling is an important research direction
% that can be explored in this system,
% it has also been explored in many other systems
% (???cite Diva etc),
% and so we have chosen to leave this type of learning
% for future work.

% The bootstrapped syllable learning system
% learns new syllables in three steps.

% \begin{enumerate}
% \item Initialize the new syllable from the most
%   similar existing syllable.
%   For example, when learning to voice
%   the syllable /ba/, the system should
%   start from the syllable /fa/ if it is known.
% \item Swap compatible speech gestures.
%   For example, vowel producing gestures
%   would be compatible, allowing for modifying
%   a /ba/ to a /bu/, and so on.
%   The choice of which gesture to swap and
%   how to swap it will be informed by
%   ???figure out.
% \item Fine-tune the voiced syllable
%   until it can be recognized as the
%   syllable to be learned.
%   ???more
% \end{enumerate}

% ??? NB: the oscillator being learned should exist outside of
% the normal speech system so that they can both run
% at the same time, but there should be a switch kind of thing
% to put the new syllable through.

% ??? hypothesis (not sure where to put this):
% the mapping between phoneme to gesture
% is such that is not advantageous
% to represent in the synthesis
% (maybe also recognition?) system(s).
% Therefore, it may be the case that
% not all people have phoneme representations.
% However, we propose that phonemes
% are a useful construct for learning to voice
% novel syllables in a second-language learning situation.

% These steps require several systems
% that have already been implemented
% in the recognition and synthesis systems separately;
% for example, the ability to compare
% voiced syllables to those already known
% is one of the primary goals of the recognition system itself,
% so it can be leveraged when trying to learn new syllables.
% However, these steps also point to new systems
% that must be implemented.
% First, the system requires a method
% to transfer a syllable-producing function
% from one ensemble to another.
% Second, some knowledge of which gestures
% are compatible must be built into the system.
% Finally, ???fine-tuning.

% It has been shown that in second-language learning,
% slowing down the syllable can have
% a significant increase in learning effectiveness
% ???cite.
% We believe that our system emulates
% how a native speaker of one language
% would learn to voice novel syllables in a second language.
% Therefore, we predict that slowing down
% the speed at which the syllables are heard
% and uttered will improve ???learning speed
% and / or quality of learned syllable.

% ??? Mental syllabary (1-s2.0-S009394X...pdf)
% supports our architecture
