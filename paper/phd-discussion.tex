\chapter{Discussion}
\label{chapt:discussion}

\section{Main results}

\subsection{Neural cepstral coefficients}

The most important and somewhat surprising result
from the NCC model
is that NCCs can yield significantly
better training and test accuracy rates than MFCCs,
which are the most commonly used
feature vector in ASR systems.
Our goal in these experiments was to
show that NCCs are at least as good as MFCCs,
as the primary motivation behind
NCCs is to generate a feature vector
entirely in spiking neurons,
therefore making it suitable
for use in Sermo
and in neuromorphic systems using
silicon cochleas.
The fact that they also yielded
better accuracy rates
is promising,
though it should be kept in mind
that these results
are from classification through
an ideal statistical linear SVM classifier,
rather than through
a full continuous ASR system.
It remains to be seen
whether NCCs will be successful
in that more realistic setting.

One advantage of NCCs is that
no explicit normalization is required,
as shown in Section~\ref{sec:results-ncc},
where it was determined
that NCCs are best used
without z-scoring, unlike MFCCs.
It is important to note, however,
that NCCs do still employ a method
of normalization;
spiking neurons have a refractory period,
meaning that they have a strict maximum firing rate,
which puts a limit on the
decoded output of an ensemble.
Unlike MFCCs, however,
this normalization is applied
for every step in the pipeline,
rather than just once at the end.
It is possible that
the normalization inherent in
spiking neural networks
is responsible for the improved performance
compared to MFCCs.

While a similar argument could be made
for why adding derivatives had
a deleterious effect on
NCC test accuracy,
it is also possible that
using offline SVM classification
is responsible for decreased performance.
Having many more features
may make the supervised learning procedure
more difficult,
or perhaps since we have the full history,
the derivative is redundant
and only makes it more difficult
to linearly separate data.

We view the final NCC experiment,
in which the auditory periphery model was varied,
as being the main contribution
of this model.
Because the model bridges the gap
between detailed auditory periphery models
and automatic speech recognition systems,
we are able to do apples-to-apples comparisons
of how well the periphery models
process realistic speech samples.
In general, the more realistic the model,
the better it is for speech,
with the notable exception of the Gammatone filter,
which is the least realistic,
cheapest to compute,
yet as accurate as
the most complicated Tan Carney model.

There are two reasons why we might
be skeptical of the impressive Gammatone results.
For one, the determination of good parameter sets
in the other experiments in Section~\ref{sec:results-ncc}
all used the Gammatone filter.
It is possible that performing the same
experiments with other filters
would find parameters sets that
allow those auditory periphery models
to perform as well or better than
the Gammatone filter.
Additionally, arguably the most difficult part
of continuous ASR systems is
determining where phone boundaries lie,
rather than classifying pre-segmented phones.
While we believe that the experiments
performed in Section~\ref{sec:results-ncc}
are a useful comparison between MFCCs and NCCs,
some of the benefits of more sophisticated models
like the Tan Carney model
may lie in their ability to segment speech.
The same argument holds
for adaptive neuron types;
while they may not be essential
for differentiating between pre-segmented speech samples,
they may be essential for
segmenting continuous speech.
Looking at the impact of these
NCC representation choices
in a continuous ASR system
is therefore an important next step.

\subsection{Syllable sequencing and production}

The syllable sequencing and production model
can generate production information trajectories
of several seconds in length
from a mental syllabary of size four,
and those trajectories can result in
intelligible speech when synthesized
with the VocalTractLab articulatory synthesizer.

It is surprisingly that the model is able
to perform at relatively high speeds
(2.5--6.5~Hz),
but has trouble with lower speeds.
We believe the source of this difficulty
is in the sequencer network.
When a syllable is about to end,
the sequencer network
switches the gating signal
and starts the reset signal,
effectively starting the
DMP network associated with the next syllable,
while also pushing the current syllable's
DMP network to the starting position.
When frequencies are high,
the drive to the starting position
is approximately the same as the
intrinsic dynamics, so the system
moves as expected.
However, for low frequencies,
the additional drive in the current syllable
forces it to finish the current syllable
too quickly.

The results in Figure~\ref{fig:prod-n_syllables}
are surprising because syllables
not taking part in a sequence
should be completely inhibited,
and therefore not affect the
operation of other syllable DMPs.
Yet, a large number of deletions
occur when the syllabary has
four or more syllables.
While we believe that the SPA modules
are the source of these insertions
(due to the relatively low
dimensionality of syllable representations
used in the experiment),
further inspection of these models
is crucial to scaling it to large vocabularies.

While the production network is able to produce
discrete vocal tract gesture trajectories successfully,
DMPs were created for continuous control problems,
and therefore can create continous trajectories
more accurately.
In Chapter~\ref{chapt:model},
we noted that the choice between discrete
and continuous production information
is an empricial one.
While we have not ruled out discrete
vocal tract gestures
as a production information representation,
we should still investigate whether
continuous trajectories are
more natural for the methods
employed in the syllable sequencing and production model.

The final neural model employs neural inhibition
in several key areas
(see Figure~\ref{fig:prod-network}).
While employing neural inhibition was not
an expclit goal in the design process,
it provides a method for
making qualitative behavioral changes rapidly,
and therefore was used when such changes
were necessary.
Having neural inhibition
as one of the tools
at a modeler's disposal
provides another concrete benefit
of using spiking neural models
over other modeling techniques.

One process in the model that
required significant effort
it the translation from
a recorded production information trajectory
to a gesture score.
The approach using temporal derivatives
works well,
but is parameterized with a few parameters
which were chosen through a manual process
rather than through an
automatic optimization process.
It would be worth looking at the
gesture score reconstruction algorithm
in more detail to examine if
an improved algorithm changes
the accuracy results,
or the quality of synthesized speech.

Finally, it is important to emphasize that
this model captures the end result
of some learning process,
which continues to operate
even in adulthood.
One possible way forward for this model
is to begin with it in its current state,
and use NEF-based learning techniques
(e.g., \citealt{macneil2011,bekolay2013a})
to fine-tune its performance based on
sensory feedback.

\subsection{Syllable recognition}

The syllable recognition model
successfully addresses the main criteria
influencing its design.
The model is able to lassify trajectories
with relatively high degrees of freedom
(at least 7),
that advance non-uniformly through time,
and that occur in a continuous stream
with no segmentation signals.

One of the biggest limitations
of the syllable recognition model
in its current state,
is its sensitivity to the
frequency of the syllable
(see Figure~\ref{fig:recog-freq}).
Like the production model,
it performs poorly for
slow syllables,
yielding a large number of insertions
(although it could be argued that
repeated identification of the correct
syllable is acceptable).
Unlike the production model,
performance is best at around 1.8~Hz,
then monotonically worsens
for higher frequencies.
As mentioned previously,
we believe that the decreased accuracy
is due to a coupling between
the speed of the syllable
and other parameters in the model
(e.g., the scale).
In other words, the iDMPs
must progress their state forward
at a higher rate when
the syllable is voiced at a higher rate.
Methods for estimating speech rate
should therefore be investigated
and used to dynamically modify
the effective scale
depending on the overall speech rate.

The poor results of the model
with increased numbers
of syllables in the syllabary
indicate that the competition
introduced to isolate a relatively small
number of iDMPs is insufficient
to allow only one syllable iDMP to be active.
However, the model does operate well
with few syllables,
and many syllables are similar
and would be often confused by people
without linguistic context.
Therefore, we conclude that
the iDMP networks are still suitable
for trajectory classification,
but may not necessarily be used in a
drift-diffusion style classification system
in which the first iDMP to reach
a threshold value makes a classification.
Instead, we aim to explore a model in which
the sensorimotor integration system
would allow the iDMPs to continuously operate,
and integrate the information from those iDMPs
as well as information from other sources
(including top-down linguistic influences)
to make a final determination
of the uttered syllable
(when necessary).
Aside from sensorimotor learning,
it is possible that
this type of syllable classifier
may only be used in cases where
the linguistic systems
disambiguate between two similar choices.

\section{Comparison to existing models}

Currently, it is difficult to make
apples-to-apples comparisons between
the models presented in this thesis
and other models,
especially those in speech literature.
The models that are well known and discussed
in speech literature are traditional
connectionist models,
which provide interesting theoretical predictions,
but are not functional models
that can replicate speech function.
Essentially, artificial intelligence
has adopted statistical methods
for speech recognition and synthesis,
which work well in practice,
but provide little theoretical benefit
to speech literature.
Speech scientists provide theoretical models,
but these models are at too high
a level of abstraction to
be able to recognize speech
or control articulatory synthesizers.
We hope that the neural models presented
in this thesis bridge the gap
between productive and theoretically interesting models
by solving recognition and syntehsis problems
with methods that can be directly related
to linguistic and neuroscientific theories.

Putting aside the general argument
that all of the models in this thesis
are implemented with spiking neurons,
there are still useful comparisons
to make with the previous work
surveyed in Chapter~\ref{chapt:previouswork}.

For the NCC model, explicit comparisons were made
with the commonly used MFCC feature vector
as part of its evaluation.
Surprisingly, NCCs
are better suited than MFCCs
for classifying pre-segmented phones.
While there are many other variants of
MFCCs using Gammatone and other auditory filters,
we could not find any MFCC variants
that gave state-of-the-art results on
a large speech corpus like TIMIT.
Instead, the motivation for using
more biologically realistic
auditory filters is that
they are more noise robust,
and so the studies using these filters
use speech that is either naturally noisy,
or has noise added to it.
As the NCC model is the first
that I am aware of to show
an improvement with
a subset of pre-segmented TIMIT,
it will be revealing
to use NCCs in a
continuous online ASR system.

For the syllable sequencing model,
the syllable sequences
are far more temporally flexible
and large scale than the sequencing methods
reviewed in Section~\ref{sec:prev-sequencing},
except for the OSE model,
which we employ in our sequencing model.
Our additions to the sequencing model
do not add anything useful for
solving the general problem
of serial list memory,
but they are useful in other situations
that require rapid iteration
through a list of symbols,
which may be common
in highly learned actions
driven by cortical networks
rather than the
cortex-basal ganglia-thalamus-cortex loop.
As such, we will extract
this aspect of the model out
in order to make it more approachable
for other researchers using the SPA
to generate symbol sequences.

For trajectory generation,
it is not yet clear
the exact comparison between DMPs
and Task Dynamics.
At their core, the two techniques
are remarkably similar.
In the trajectory generation model,
we only use the canonical system
and forcing function of the DMP;
however, a more robust mapping
from the production information trajectory
to articulator positions in
the VocalTractLab synthesizer
would also take into account
the DMP's point attractors.\footnote{
  In general, a tighter integration with
  VocalTractLab would improve aspects
  of the model significantly;
  unfortunately, VocalTractLab
  is currently not an open source program,
  making integration more difficult.}
Making a more robust motor expansion system
and providing articulator positions
to VocalTractLab,
rather than a gesture score,
could improve synthesized speech quality.
Similarly, it would be worth
attempting a neural implementation
of task dynamics in order to
compare the two approaches
using the same methods.

For trajectory classification,
the iDMP is a novel trajectory classification technique
which warrants further investigation
outside of the context of a neural model.
Some early experiments using a non-neural
version of the model were successful
classifying toy problems;
however, it remains to be seen
whether it could be used in
gesture recognition systems
with high accuracy,
like HMM-based methods and the GVF algorithm.

\section{Contributions}

As this thesis draws on theories
and techniques from a myriad of fields,
it also attempts to make contributions
to many of those fields.
While a general contribution to each field
is relating the concepts in that field
to concepts in other fields,
we also note specific contributions
for several fields.

\subsection{Contributions to computer science}

Progress in machine learning
has been accelerated significantly
through the existence of
benchmark data sets with
easy to reproduce
state-of-the-art results
(e.g., the MNIST data set,
which is now solved better
by machine vision algorithms than humans).
However, one issue with benchmarks
is that they can lead to myopic advances
which eke out one or two more
accuracy points with little
context of how the improvement
might generalize to other problems,
or how it could play a role
in a larger integrated system.

I believe that we make two contributions
to machine learning through
providing a novel benchmark data set,
and an integrated system that can
be used to benchmark how algorithmic choices
affect other systems.
First, we intend to release a data set
consisting of time-aligned
vocal tract gesture trajectories
and speech samples synthesized
by the VocalTractLab synthesizer.
As previously discussed,
data sets like the
X-ray microbeam database \citep{westbury1990}
and MNGU0 \citep{steiner2012}
provide data sets
consisting of time-aligned
vocal tract articulator positions
and speech samples.
However, I am aware of no available data set
of time-aligned vocal tract gestures
and speech samples.
We theorize in Sermo
that production information
is decoded from auditory features.
In order to determine whether
that production information
could take the form of vocal tract gestures,
we will release the
gesture trajectory data set
so that machine learning researchers
can apply statistical methods
to determine if the acoustic signal
has enough information to decode
vocal tract gestures.

Additionally, we theorize in Sermo
that the lexical decoding process
often used in ASR systems
is a necessary component
of the speech system.
By providing a conceputal framework
through which to contextualize
the lexical decoding process,
machine learning researchers can
see how an ASR system might fit into
a biologically plausible speech system.
As was shown in the comparison
between auditory periphery models,
integrated systems like Sermo
enable comparisons between related techniques
on the basis of its role in a larger system,
rather than its ability to match
an accuracy rating on a benchmark problem.

Outside of benchmarking, the NCC
feature vector representation
may be useful for
general automatic speech recognition,
a subfield of artificial intelligence.
The iDMP algorithm
introduced in this thesis is likely to be
applicable to gesture recognition,
a field of interest to human-computer interaction
and machine learning researchers.

\subsection{Contributions to linguistics}

While Sermo summarizes a large body of
linguistics research with language
that should be understandable for
computer scientists,
the theories embodied in Sermo
are not novel in the context of linguistics.
However, it may be instructive to see
linguistic theories
discussed with computational
and neurobiological terms.

Our primary contribution to linguistics
is a method for connecting speech
to higher-level linguistic concepts.
Previously, \citet{blouw2013,blouw2015,stewart2015}
have explored solutions to
syntactic and semantic problems
using the Semantic Pointer Architecture (SPA).
We propose that audio signals
can be mapped to semantic pointers
through an auditory feature extraction
and linguistic decoding step.
The linguistic decoding may also be informed
by production information
and syllable classifications,
which are also represented with semantic pointers.

\subsection{Contributions to neural modeling}

We make two main contributions to
large-scale neural modeling using the NEF and SPA:
connecting spiking neural networks
to the domain of speech,
and associative memories using
temporal inputs and outputs.

The models presented in this thesis
are the first instances (to my knowledge)
of incorporating the sensory and motor
aspects of speech into the NEF
through Nengo.
The NCC model processes speech online
using auditory periphery models
constructed with \textit{Brian hears}.
The speech sequencing and production model
produces audio speech samples
using the VocalTractLab articulatory synthesizer.
The addition of these sensors and actuators
enables new avenues
for large-scale neural modeling research;
one could envision a version of Spaun
that uses audio inputs and outputs
rather than vision and handwritten digits.

The SPA deals with symbol-like representations
using high-dimensonal vector spaces.
Currently, the semantic pointers
associated with concepts are
static vectors that can be manipulated
through the merging and binding operators,
among other operations.
However, the concepts represented in speech
are not static.
In order to deal with the temporal nature of speech,
we presented temporal output associative memories
and temporal input associative memories.
Temporal output associative memories
generate continuously varying output signals
from semantic pointer representations
using DMPs.
They provide a robust link between the discrete,
symbolic world of high-level representations
that can be flexibly combined
and the continuous, subsymbolic world
of low-level representations that must
be precisely timed.
Temporal input associative memories
map a continuously varying input signal
to semantic pointer classifications
using iDMPs.
They provide an analogous link between
continuously varying low-level sensory inputs
and high-level symbolic representations.
While we have introduced these networks
to deal with speech,
they can also be used for other
situations where temporal information
is important (e.g., video analysis,
general motor control).

\section{Predictions}

%% \subsection{Mapping to brain areas}

% In the background we discussed the brain areas
% involved in speech recognition and production,
% in part to place connectivity constraints
% on the model as a whole.
% However, in order to generate testable predictions,
% we can also impose a mapping from
% the neural structures in the model
% to speech-related brain regions.

% cepstral coefficients currently require activity
% from all frequencies. But, this may not be anatomically
% consistent. Could be possible, instead, to do this
% hierarchically.

\section{Limitations}

% iDMPs aren't complete yet; we recover
% the canonical system state, but more work
% needs to be done to get the point attractor

Many of the benefits of these models
also underlie their limitations

\section{Future work}

??? these can interact with silicon cochleas.
obviously, we should try doing this.

??? actually replicate some experiments,
e.g. the syllable repeating one

% ??? add bursting neurons in both production
% and recognition systems for better timing

% ??? more sophisticated classification mechanisms
% in classification section

% ??? add speed control to the DMPs;
% pretty easy, just add a dimension to the oscillator.

% ??? neural implementation of mapping
% from gesture sequence to articulator positions

% Summary of things from other sections:

% \subsection{Recognition system}

% How to deal with semivowels, semiconsonants and glides?
% Should it just be one monolithic phoneme detector?

% Represent prosody in a second feature layer (hierarchical organization).

% Learn all of this stuff rather than optimizing for it.

% How to set baseline pitch and volume on the fly?

% Can we use artificial cochleas as is?

% \subsection{Synthesis system}

% We can use this synthesis system to explore
% important phonological questions.
% For example, syllabic consonants
% Do they sound right as separate syllables?
% Or do they sound right as protracted versions
% of the analogous syllable with the vowel included?
% Can these be distinguished from one another?

% We haven't done forward model prediction
% to determine how to adapt
% when things go wrong.
% However, since we've implemented everything
% with the NEF and SPA,
% we can just use the REACH model.

% ??? there's also a bunch of prosodic stuff;
% this could be added (see comments below)

% Aside from phonemes, we also represent pitch and volume.
% Neither of these features are useful
% as an absolute quantity---most humans are poor
% at judging absolute pitch and volume ???cite---so
% we aim to represent relative pitch and volume.
% In both of these cases,
% we must determine a baseline pitch or volume,
% and a method for comparing the current
% pitch or volume to the baseline.

% In speech, baseline pitch is primarily determined
% by speaker identity.
% Each speaker has a baseline pitch,
% determined in part by the shape of their vocal folds,
% so it is likely that we learn
% and remember the baseline pitch
% of speakers that we interact with frequently.
% On the other hand,
% changes in baseline pitch
% (e.g., through illnesses affecting the vocal tract)
% require little to no adaptation period,
% so the mechanism through which we determine
% a speaker's baseline pitch
% is likely to be relatively simple
% and flexible.
% In this work,
% we will not posit a neural mechanism
% for determining baseline pitch,
% and will instead compute it
% from the data offline and provide it as input.

% Given the baseline pitch as input,
% relative pitch will be computed
% primarily through neural inhibition.
% See section ???implementation
% for how this is accomplished.

% Unlike pitch, baseline volume
% is not speaker specific;
% it is easy to notice when a speaker
% talks louder or quieter
% than the norm.
% ???baseline is the overall activity
% of a long-timescale filtered version
% of the current moment's power spectrum?
% ???relative volume is that minus a
% short-timescale filtered version?
% So it's basically just a derivative?
% ???not sure if we even care about volume
% to be honest...
% % http://www.sengpielaudio.com/calculator-loudness.htm

% \section{Syllable consolidation}

% ??? I didn't get to this,
% but it can definitely be done.

% Go from vocal tract gesture score
% to audio signal, in neurons.

% Learn a syllable recognition
% and a syllable production
% network from this.

% Contrast this to syllable production because it's
% an infrequently voiced syllable.

\subsection{Bootstrapped syllable learning}
\label{sec:syllable-learning}

% ??? A full picture of speech development involves:
% learning vocal tract gestures
% through reinforced motor babbling,
% learning basic syllables
% through mimicry?,
% and scaling up to the elements of this model.
% While we think that our model
% is a useful starting point for such
% a developmental model,
% as it provides an end-target,
% we don't claim to do this kind of structural learning.
% However, we believe that error-based learning
% could result in this kind of system;
% to show that this is a possibility,
% we consider a minimal learning situation:
% learning voice a new syllable
% given a set of existing syllables.
% This type of thing probably happens
% in second language acquisition,
% when novel combinations of phonemes
% are encountered,
% or even during first language acquisition
% as pronunciation is refined over
% the course of one's life;
% words that were once awkward combinations
% of many syllables are compressed into
% nearly equivalent sequences of fewer,
% more complex syllables.

% As opposed to conversational shadowing,
% which highlights the high-level strengths
% of the integrated speech system,
% syllable learning highlights
% the low-level strengths of this system.
% Syllable learning involves
% learning a novel set of gestures
% and associated articulator trajectories
% that will voice a syllable
% that is encountered for the first time.

% We call our syllable learning system ``bootstrapped''
% because we assume that our system
% has an existing repertoire of syllables
% that it is already able to voice.
% These existing syllables will be
% used in learning the new syllable.
% Bootstrapped syllable learning contrasts with
% the type of syllable learning
% done as an infant and toddler,
% which uses reinforced speech babbling
% to learn novel syllables.
% While we believe that learning syllables
% from babbling is an important research direction
% that can be explored in this system,
% it has also been explored in many other systems
% (???cite Diva etc),
% and so we have chosen to leave this type of learning
% for future work.

% The bootstrapped syllable learning system
% learns new syllables in three steps.

% \begin{enumerate}
% \item Initialize the new syllable from the most
%   similar existing syllable.
%   For example, when learning to voice
%   the syllable /ba/, the system should
%   start from the syllable /fa/ if it is known.
% \item Swap compatible speech gestures.
%   For example, vowel producing gestures
%   would be compatible, allowing for modifying
%   a /ba/ to a /bu/, and so on.
%   The choice of which gesture to swap and
%   how to swap it will be informed by
%   ???figure out.
% \item Fine-tune the voiced syllable
%   until it can be recognized as the
%   syllable to be learned.
%   ???more
% \end{enumerate}

% ??? NB: the oscillator being learned should exist outside of
% the normal speech system so that they can both run
% at the same time, but there should be a switch kind of thing
% to put the new syllable through.

% ??? hypothesis (not sure where to put this):
% the mapping between phoneme to gesture
% is such that is not advantageous
% to represent in the synthesis
% (maybe also recognition?) system(s).
% Therefore, it may be the case that
% not all people have phoneme representations.
% However, we propose that phonemes
% are a useful construct for learning to voice
% novel syllables in a second-language learning situation.

% These steps require several systems
% that have already been implemented
% in the recognition and synthesis systems separately;
% for example, the ability to compare
% voiced syllables to those already known
% is one of the primary goals of the recognition system itself,
% so it can be leveraged when trying to learn new syllables.
% However, these steps also point to new systems
% that must be implemented.
% First, the system requires a method
% to transfer a syllable-producing function
% from one ensemble to another.
% Second, some knowledge of which gestures
% are compatible must be built into the system.
% Finally, ???fine-tuning.

% It has been shown that in second-language learning,
% slowing down the syllable can have
% a significant increase in learning effectiveness
% ???cite.
% We believe that our system emulates
% how a native speaker of one language
% would learn to voice novel syllables in a second language.
% Therefore, we predict that slowing down
% the speed at which the syllables are heard
% and uttered will improve ???learning speed
% and / or quality of learned syllable.

% ??? Mental syllabary (1-s2.0-S009394X...pdf)
% supports our architecture

\subsection{Prosody}

\subsubsection{Tone-units}

??? move to discussion

The final level of organization
that we will examine is the tone-unit.
The tone-unit allows us to examine
suprasegmental aspects of speech.
A tone-unit is made up of a serially ordered
sequence of syllables.\footnote{Many
  phoneticians consider a tone-unit to be
  composed of ``feet,'' where a foot is
  a single unit of rhythm.
  However, feet are mostly used when describing
  non-conversational utterances,
  such as those found in music and poetry.
  In this thesis we focus on speech
  as a means of conveying linguistic information,
  and therefore ignore the concept of feet.}

The structure of a tone-unit is similar
to a syllable, except its component parts
are syllables instead of phonemes,
and its components are serially ordered.
A tone-unit must contain a tonic syllable
(sometimes also called the nucleus),
and can optionally contain one or more syllables
in a pre-head, head, or tail section.
The pre-head consists of all syllables
before the first stressed syllable
in a tone-unit.
The head consists of all syllables from
the first stressed syllable
to the tonic syllable.
The tonic syllable is the most significant
syllable in the tone-unit because
pitch changes in the tonic syllable
will occur relative to the tonic syllable;
the tonic syllable is not necessarily
the loudest or most prominently stressed
syllable in the tone-unit,
though it does always contain
a stressed (and therefore heavy) syllable.
The tail consists of all syllables
following the tonic syllable.

% ??? tone-unit structure figure
% maybe don't worry for now

Not all heavy syllables are necessarily stressed;
stress is hypothesized to occur when
more muscle activation is used
to voice a particular heavy syllable
???cite?.
Stress is perceived as a heavy syllable
that is louder, longer, and with
a different pitch or quality compared
to other syllables.
In some languages, rules govern
which syllables receive stress;
for example, in French,
the last syllable in a word is
always stressed.
In English, each word defines
its own stress pattern,
and each utterance can add
additional stresses
that emphasize some words over others.
Additionally, stress is not a binary quantity;
English is typically thought to contain
three stress levels
(primary stress, secondary stress, and unstressed).

Intonation, on the other hand,
is a more straightforward phenomenon to model.
Intonation is the use of pitch changes
to 1) express emotions and attitudes,
2) impart prominence on stressed syllables,
3) exaggerate grammar and syntactic structure,
and 4) clue listeners into what information
is novel and what is thought to be already known.
While intonation can be thought to also include
body language and other prosodic characteristics,
we will focus only on pitch changes.
The tone of a tone-unit
is the overall trajectory of pitch
during the tone-unit.

There are a limited set of possible pitch trajectories
(i.e., tones) in English.
While different sources identify different trajectories,
we will adopt the conventions of ???below
which note six possible pitch trajectories:
high fall, low fall, high rise,
low rise, fall-rise, and rise-fall
(see Figure~???).
The same sequence of syllables
can change its meaning dramatically
by using a different pitch trajectory,
or by changing the position of the
tonic syllable within that pitch trajectory.
The meaning of each pitch trajectory
changes depending on the utterance in question;
we will not investigate meanings further
in this work, as we focus on
sound reproduction rather than on
linguistic meaning.

??? Quick plot of pitch trajectories
% again, deal with later

% cite https://notendur.hi.is/peturk/KENNSLA/25/IPBE/SHELL/25/nucleus.html
% or Cruttenden

It is important to note that when talking about
the pitch trajectory of an utterance,
pitch is always relative to the
upper and lower range of a particular speaker.
In the pitch trajectory plot in Figure~???,
horizontal lines show the upper and lower ranges.

These six trajectories in Figure~???
interact with the pre-head and tail
in predictable ways.
The pre-head is usually low,
but can be high in front of a low stressed syllable.
The tail trajectory can be predicted
by the trajectory of the tonic syllable;
for example, if the tonic syllable falls,
the tail remains low;
if the tonic syllable rises,
the tail continues to rise.
The head, on the other hand,
is independent of the tonic syllable;
it may remain at a high or low level,
or it can rise or fall like
the trajectories associated with the tonic syllable.
The combinations of head and tonic syllable
pitch trajectories also contributes
to the varying meanings conveyed
by the pitch trajectory of a tone-unit.

Finally, one note about
the absence of ``words'' in our treatment
of phonetics and phonology.
While words are clearly an important concept
that we will use colloquially in this thesis,
they are a linguistic construct
rather than a phonological one.
However, as directly applied
to speech recognition and synthesis,
words are more readily perceived
than sequences of syllables with no meaning.
Fortunately, the pioneering work
of ???allen94 which summarizes ???
gives a direct relationship between
the classification rates of nonsense syllables and words,
which can be used to adjust
recognition rates for
non-linguistic speech.
