\chapter{Discussion}

\section{Reflection on results}

\section{Comparison to existing models}

\section{Contributions}

??? we made a speech recognition thing

??? we made a synthesizer

??? we made a neural control method for synthesizers

??? we put them together

??? mostly, we've integrated existing parts in a large scale model

??? none of the models by themselves are terribly groundbreaking,
and can use some work to fix up,
but the contextual framework of Sermo
is a large undertaking,
but we believe it can provide the basis
for a ton of worthwhile future research.

??? additionally, while my stuff is applied to speech,
the various networks and ideas (the sequencer network,
trajectory classification, etc) are useful/necessary
for dealing with any temporally varying stuff in spaun, e.g. gestures, etc

\subsection{Contributions to computer science}

Mostly we've talked about brain modeling,
arguably part of the domain of neuroscience,
and control, which is traditionally an engineering topic.
But, this is a CS thesis, so there should be some CS contributions.

\section{Predictions}

need more cortex for consonants than vowels?

- lots of people talk about spectro-temporal features,
  gabor filters across time and space, etc.
  Those are useful models, but not directly implementable
  in neurons; we have to have everything available at
  the same timestep, so derivatives make more sense

\subsection{Mapping of model to brain areas}

In the background we discussed the brain areas
involved in speech recognition and production,
in part to place connectivity constraints
on the model as a whole.
However, in order to generate testable predictions,
we can also impose a mapping from
the neural structures in the model
to speech-related brain regions.

??? do, and make predictions based on this.

??? possible issues:

??? cepstral coefficients currently require activity
from all frequencies. But, this may not be anatomically
consistent. Could be possible, instead, to do this
hierarchically.

\section{Future work}

??? add speed control to the DMPs;
pretty easy, just add a dimension to the oscillator.

??? neural implementation of mapping
from gesture sequence to articulator positions

Summary of things from other sections:

\subsection{Recognition system}

How to deal with semivowels, semiconsonants and glides?
Should it just be one monolithic phoneme detector?

Represent prosody in a second feature layer (hierarchical organization).

Learn all of this stuff rather than optimizing for it.

How to set baseline pitch and volume on the fly?

Can we use artificial cochleas as is?

\subsection{Synthesis system}

We can use this synthesis system to explore
important phonological questions.
For example, syllabic consonants
Do they sound right as separate syllables?
Or do they sound right as protracted versions
of the analogous syllable with the vowel included?
Can these be distinguished from one another?

We haven't done forward model prediction
to determine how to adapt
when things go wrong.
However, since we've implemented everything
with the NEF and SPA,
we can just use the REACH model.

??? there's also a bunch of prosodic stuff;
this could be added (see comments below)

% Aside from phonemes, we also represent pitch and volume.
% Neither of these features are useful
% as an absolute quantity---most humans are poor
% at judging absolute pitch and volume ???cite---so
% we aim to represent relative pitch and volume.
% In both of these cases,
% we must determine a baseline pitch or volume,
% and a method for comparing the current
% pitch or volume to the baseline.

% In speech, baseline pitch is primarily determined
% by speaker identity.
% Each speaker has a baseline pitch,
% determined in part by the shape of their vocal folds,
% so it is likely that we learn
% and remember the baseline pitch
% of speakers that we interact with frequently.
% On the other hand,
% changes in baseline pitch
% (e.g., through illnesses affecting the vocal tract)
% require little to no adaptation period,
% so the mechanism through which we determine
% a speaker's baseline pitch
% is likely to be relatively simple
% and flexible.
% In this work,
% we will not posit a neural mechanism
% for determining baseline pitch,
% and will instead compute it
% from the data offline and provide it as input.

% Given the baseline pitch as input,
% relative pitch will be computed
% primarily through neural inhibition.
% See section ???implementation
% for how this is accomplished.

% Unlike pitch, baseline volume
% is not speaker specific;
% it is easy to notice when a speaker
% talks louder or quieter
% than the norm.
% ???baseline is the overall activity
% of a long-timescale filtered version
% of the current moment's power spectrum?
% ???relative volume is that minus a
% short-timescale filtered version?
% So it's basically just a derivative?
% ???not sure if we even care about volume
% to be honest...
% % http://www.sengpielaudio.com/calculator-loudness.htm
