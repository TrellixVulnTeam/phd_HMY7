\chapter{Implementation}

In this thesis, we implement and evaluate
four neural models that make up
parts of the conceptual Sermo model.
Each model provides
an explanation for how the brain might
solve a task necessary for
speech recognition or synthesis
using the computational devices at hand,
namely spiking neurons.
However, where possible,
we also present analogous non-neural models
that could be incorporated in
non-neural speech recognition and synthesis systems.

\section{Auditory feature extraction}

\probbox{A raw audio waveform has high temporal resolution,
  but low spatial resolution, as it is a one-dimensional signal.
  Audio must therefore be preprocessed such that
  all the temporal information necessary to recognize speech
  is available at the current moment in time.
  Automatic speech recognition systems preprocess speech by
  extracting feature vectors containing cepstral coefficients.
  How can a neural model like Sermo preprocess speech?}

The frontend of most automatic speech recognition systems
extracts a feature vector from the incoming audio waveform.
As reviewed in section ???,
mel-frequency cepstral coefficients (MFCCs)
have been used successfully in many ASR systems ???cite.
To a certain extent, MFCCs are motivated
by the organization of the human auditory system.
We propose neural cepstral coefficients (NCCs)
as an alternative feature vector representation
for automatic speech recognition
and other tasks involving speech processing.
Unlike MFCCs, NCCs are both motivated
by the organization of the human auditory system
and are implemented the same basic computational units
as the human auditory system,
spiking neural networks.

\section{Neural cepstral coefficients}

??? NCC pipeline, like AuditoryBasedFeatureVectors.pdf
or KumarKimSternICA11.pdf

Figure~??? shows the NCC processing pipeline,
as compared to the MFCC processing pipeline.
While the pipeline implements
essentially the same computations,
the manner in which those computations
are performed differs significantly.
The primary difference is that
incoming audio is processed
in a continuous online fashion,
rather than in a frame-based fashion.
Typical MFCC extraction algorithms
break the audio signal into
overlapping fixed-length windows
called frames,
which are processed independently
until the smoothing stage,
where discontinuities
due to frame boundaries are
minimized.
The NCC extraction algorithm,
on the other hand,
maintains internal state
at each processing stage,
and updates that internal state
for each incoming audio sample.
As each internal state update
depends on the current state,
changes occur smoothly through time,
which obviates the need for
an explicit smoothing step.

Another implementation difference
is in the computation of
the inverse discrete cosine transform.
Typically, this computation is done as
\begin{equation}
  y_k = \frac{x_0}{\sqrt{N}} + \sqrt{\frac{2}{N}} \sum_{n=1}^{N-1}
  x_n \cos \left( \frac{\pi}{N} n \left( k + \frac{1}{2} \right) \right)
  \text{ for } 0 \le k < N
\end{equation}
where $y_k$ is the $k$th cepstral coefficient,
$x_n$ is the $n$th auditory filter output,
and $N$ is the number of auditory filter outputs.
In matrix notation,
this equation can be expressed as
\begin{align}
  \label{idct}
  \mathbf{k} &= \left[ 0, 1, \ldots, N-1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{s} &= \left[ \sqrt{2}, 1, 1, \ldots, 1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{T} &= \sqrt{2}{N} \, \mathbf{s} \circ \cos \left( \frac{\pi}{N} \left(
    \mathbf{k} + \frac{1}{2} \right) \otimes \mathbf{k} \right)
    & N \times N \text{ matrix} \nonumber \\
  \mathbf{y} &= \mathbf{T}\mathbf{x} & N \times 1 \text{ vector}
\end{align}
where $\circ$ is the Hadamard (element-wise) product,
and $\otimes$ is the outer product.
This rearranged equation
allows us to precompute a matrix, $\mathbf{T}$,
that maps auditory filter outputs
directly to cepstral coefficients,
which enables us to embed this matrix
in the connection weights between
two neural population
(see ??? network diagram for more details).
By restricting $\mathbf{T}$
to the first $M$ rows,
equation \eqref{idct} yields the first
$M$ cepstral coefficients;
typically, ASR systems
use $N \approx 26$ auditory filter outputs
and $M \approx 13$ cepstral coefficients.

\subsection{NCC neural model}

??? NB! Try adaptive LIF!

The NCC pipeline is implemented
in a Nengo network that uses
the Brian hears library
of auditory filter models
??? cite Brian hears,
and two layers
leaky integrate-and-fire (LIF) neurons ensembles
connected in a feed-forward manner
(see ??? figure).
While Brian has the capability
to simulate spiking neurons,
we only use Brian's implementation of
auditory periphery models,
which do not emit spikes.
These models,
summarized in ??? sect prev work,
emulate the effect of the
audio waveform on the basilar membrane,
and the resulting inner hair cell activity
arising from basilar membrane deflections,
which may include rectification
and compression
(i.e., it accomplishes the first
??? steps of the MFCC pipeline).
The output of these models
can be though of as
the current input
to spiral ganglion cells in the ear.
These auditory periphery models
accomplish a significant portion
of the MFCC extraction pipeline,
which is unsurprising as MFCC extraction
is designed to emulate certain aspects
of the human auditory system.

??? figure: net diagram

The simulation timesteps
for the Nengo model and the contained
Brain auditory filter model
are uncoupled.
The auditory filter model
is always run with the same timestep
as the auditory stimulus;
that is, its internal state is updated
for each audio sample.
The Nengo model, on the other hand,
runs at a timestep independent
of the auditory stimulus.
On each Nengo timestep,
therefore, one or more samples
of the audio input
is fed through the
auditory filter model.
The \textit{auditory nerve} ensembles
receives as input the current
state of the auditory filter model,
regardless of how many timesteps
the Brian model has advanced.

The \textit{auditory nerve} ensembles
mimic the activity of spiral ganglion cells
projecting down the auditory nerve.
For each characteristic frequency,
a heterogeneous population of neurons
is driven by the current output by
the auditory filter model.
Each characteristic frequency
drives an independent neural ensemble,
meaning that this layer of ensembles
can be thought of as being cochleotopically organized,
as the mean activity of an individual neuron
only gives information about the output
of a single auditory filter
(although that filter may be modulated
by power in nearby frequencies).
Together, the decoded output of
each ensemble gives an approximation of
the compressed power in the part of the spectrum
that the auditory filter is sensitive to.
This quantity is essentially
the output of the auditory filter
(i.e., steps ???--??? in the MFCC pipeline),
but now represented in
cochleotopically organized spiking neurons.
It is also the $X$ vector in
the inverse discrete cosine transform,
equation~\eqref{idct}.

The \textit{cepstra} ensembles
each represent one cepstral coefficient.
The inverse discrete cosine transform
for that coefficient is computed
through the connections between
the auditory nerve ensembles
and the cepstral ensembles.
Specifically, each connection
between an auditory periphery
and cepstrum ensemble
scales the decoded value of the
cepstrum ensemble by
the corresponding value
the $\mathbf{T}$ matrix.
Since this is a linear transform,
it is done by computing
normal decoding weights
according to equation~\eqref{???}
and scaling the decoders by $T_{i,j}$.
Since the currents coming from
multiple connections to an ensemble
are summed, the cepstral ensembles
end up representing the
dot product between the row of
$T$ that corresponds to its cepstrum
and the $X$ vector,
which is collectively represented
by the auditory periphery layer.

The end result of the networks
is the decoded output
of the \textit{cepstra} ensembles,
which is a continuously varying,
automatically smoothed set of cepstral coefficients.
We hypothesize that these coefficients can be used
as the feature vectors in an
automatic speech recognition system.

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Delta neural cepstral coefficients}

Another common technique in designing frontends
for automatic speech recognition is to
include temporal derivatives
in the feature vector.
Appending MFCC derivatives
to the feature vector
(i.e., MFCC velocities;
called Delta-Cepstral Coefficients [DCC])
improve ASR accuracy.
Appending derivatives of the MFCC derivatives
(i.e., MFCC accelerations;
called Double Delta-Cepstral Coefficients [DDCC])
further improves recognition,
though by a smaller margin than
the improvement from the first derivative
??? find good cite.
??? Kumar et al kumarkimsternica11.pdf
used the derivative of the auditory filter output
instead of the raw auditory filter output
in a MFCC-like pipeline
(called Delta-Spectral Cepstral Coefficients [DSCC])
and found that the ASR system
was more robust to noise and reverberation.

Temporal derivatives in these systems
are typically computed through straightforward
finite differences.
That is, for an MFCC $y_k$
at frame $t$, the corresponding delta is
\begin{equation}
  \label{dcc}
  \delta_k(t) = \frac{\sum_{n=1}^N y_k(t+n) - y_k(t-n)}{
    2 \sum_{n=1}^N n^2},
\end{equation}
where $N$ is a small number of frames
(typically between one and three).
The double delta applies the same operation
to the $\delta_k$ vectors.

% An additional nonlinearity is applied
% for DSCCs,
% as they use the derivative of the power spectrum.
% Using the raw power spectrum derivatives
% results in very small cepstral coefficients,
% so ??? Kim et al proposed a ``Gaussianization''
% step in which the power spectrum
% is made to follow a Gaussian distribution
% by inverting the normal cumulative distribution function.
% ??? more?

\subsection{NDCC and NSCC neural models}

Implementing temporal differentiation
in spiking neural networks
is not as straightforward as implementing
equation~\eqref{dcc} in neurons,
as the neural simulation operates
online in continuous time,
and therefore does not necessarily
have access to the information
from previous timesteps.
However, there are several ways
to build networks that maintain
information from the past
and can therefore implement temporal differentiation.

??? ref Tripp describes six such models,
two of which use feedforward connections,
two use adaptive neuron models,
and two implement linear time-invariant (LTI) filters
through recurrent connections.
All six models successfully
compute a temporal derivative,
so for simplicity we implemented
the two feedforward temporal differentiation models,
with the assumption that
more sophisticated temporal differentiation
models could be used if desired.

??? Figure: both derivs

In the first feedforward differentiation model,
an intermediate ensemble is introduced
between the ensemble representing the input signal
and the ensemble that we want to represent
the derivative of the input signal
(see Figure~???).
The \textit{derivative} ensemble
receives direct positively scaled input from
the \textit{input} ensemble,
and delayed negatively scaled input from
the \textit{intermediate} ensemble.
The slight delay between the positive signal
communicated from the \textit{input}
and the negative signal
from the \textit{intermediate} ensemble
results in the \textit{derivative} ensemble
representing the temporal derivative
of the input signal.

Information transmitted between two ensembles
is slightly delayed
because changes to the input currents
of the first ensemble
must first be integrated for one or more
timesteps before those changes can result in
spikes that influence
the second ensemble's input currents.
The amount of time between
when input signal changes are reflected
in the first and second populations
depends on several factors,
including the simulation timestep,
the neuron model, and the synaptic filter model.
In the feedforward differentiation model
with an intermediate ensemble,
all connections use a lowpass filter
with a relatively long time constant
($\tau=0.1 \text{ s}$)
as the synaptic filter model.\footnote{
  Synaptic filter time constants of around 100 ms
  are consistent with NMDA glutamate receptors,
  which ??? cite 1213.full.pdf have suggested
  are part of differentiator circuits
  underlying respiratory rhythm.
  Additionally, in terms of
  the biological plausibility of the network as a whole,
  ??? Tripp notes that this form of network
  has been observed in \textit{C. elegans} ??? cite Dunn
  and may underlie chemotaxis.}

In the second feedforward differentiation model,
two connections are made between
the \textit{input} and \textit{derivative} ensembles.
The first positively scaled connection
uses a synaptic filter with a fast time constant
($\tau_F=0.005 \text{ s}$),
and the second negatively scaled connection
uses a synaptic filter with a slow time constant
($\tau_s=0.1 \text{ s}$).\footnote{
  Synaptic filter time constants of 5 ms
  are consistent with some types of
  AMPA receptors ??? cite Jonasetal.}
Like the first model,
this approach exploits the delays introduced
by synaptic filtering,
subtracting a more slowly changing signal
from a signal changing quickly,
resulting in a temporal derivative.
While this network has the advantage
of requiring fewer neurons
as no intermediate connection is introduced,
it adds the constraint that
the neurons in the \textit{derivative} ensemble
contain both fast and slow
neurotransmitter receptors.\footnote{
  Since AMPA and NMDA are both glutamate receptors,
  no constraint is placed on the \textit{input} population.
  ??? Tripp notes that a derivative circuit
  in the vestibular system of oyster toadfish
  ??? PNAS-2004-Hols...
  is likely to match the form of the second
  feedforward differentiation model.}

These differentiator networks
are added to the NCC network
shown in Figure~???
to compute the desired derivative.

??? Figure of whole NCC circuit

% ?? discuss whether we can do gaussianization?
% we probably can... but maybe not in a bioplaus manner.
% Depends if it's necessary anyhow

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Evaluation}

We hypothesize that NCCs and DNCCs
are suitable feature vectors
for traditional ASR systems
and for more biologically plausible
models of the human speech system like Sermo.
In order to evaluate their effectiveness
in these situations,
we compare MFCCs with and without DCCs
and DDCCs to NCCs and DNCCs
on phoneme and word classification tasks.

??? choose an SVM implementation and
explain it here

??? it's not clear if the derivatives will help
since the SVM should manage these,
so if they don't help then I will
just use the derivatives

??? Use TIMIT utterances

% From earlier iteration

% ??? Functional evaluation: can we decode downstream features?
% How well do we do compared to other systems?
% We don't have to do as well as them,
% but we have to do well enough that it's not ridiculous.

% ??? for phonemes: our gold standard is a set of labels
% over time with the proper phoneme.
% If the system categorizes to that label in that period,
% then it's correct. Or maybe what's more important
% is that the phonemes are all in the same order,
% and we also have some kind of timing information?

% ??? Neural evaluation: One thing that's commonly measured
% in auditory neuroscience is the spectro-temporal receptive field
% of auditory neurons. We can make these for our downstream feature
% neurons and compare them to published STRFs and see if they look similar.
% Note that it would be easy to make neurons that can replicate STRFs,
% as we would just use the temporal transformations that matter.
% But, we make our choice of transformations based on what would
% be best to decode certain features;
% being able to also replicate STRFs gives an indication that
% we made good choices, and that this system is engineered
% in a similar way to how the system developed and evolved (maybe?)

% ??? should we talk about the experiments from the results section here?

% ??? Nonsense CVCs: cite Allen 94;

% ``Typical ASR systems start with a `front-end' that transforms
% the speech signal into a `feature vector' which is then processed
% by a `back-end' classifier. These systems frequently place a
% heavy emphasis on word and language models as a method of
% increasing the recognition scores.''

% ``Because of confusion and misunderstanding based on
% coarticulation arguments, only a small amount of research
% has been done on the automatic recognition of nonsense CVCs.
% From the work of Fletcher and Steinberg, it should be clear
% that the real challenge in machine recognition today is
% human-like performance for phones and nonsense CVC
% under conditions of typical channel distortions.
% Since the human performance is well-known under
% these conditions [15], [19], nonsense CVCs
% represent an excellent database. Decreasing
% the error rates for these elementary signals would have
% a major impact on overall system performance and robustness.''

\section{Syllable production}

\probbox{Once the intention to voice a syllable sequence
  has been formed, the motor system must translate
  a sequence of static conceptual syllable representations
  to a continuous trajectory of motor commands
  that will cause muscle activations in the vocal tract.
  How can a neural model like Sermo generate
  a motor command trajectory from
  a sequence of static syllable representation?}

We theorize that the speech motor system
is organized at the level of syllables,
and that motor command trajectories are explicitly stored
for syllables that are frequently voiced.
In this neural model,
we describe how sequences of syllables are represented,
and how those representations
generate motor command trajectories
that control an articulatory synthesizer.\footnote{
  The model described in this section was created
  in collaboration with Bernd Kr\"{o}ger.
  A preliminary version of the model was presented
  in ??? cite kroger.2014.pdf}

A sequence of syllables is represented
by a semantic pointer
that tags each syllable with its place
in the list; i.e.,
\begin{equation}
  \text{SEQ} = \text{SYLL1} \circledast \text{POS1} \oplus
    \text{SYLL2} \circledast \text{POS2} \ldots
\end{equation}
For example, the word ``family'' (??? /fam schwa lee/)
would be represented as
\begin{equation}
  \text{FAMILY} = \text{FAM} \circledast \text{POS1} \oplus
    \text{SCHWA} \circledast \text{POS2} \oplus
    \text{LEE} \circledast \text{POS3}.
\end{equation}
Representing a syllable sequence
by binding syllables to list positions
allows us to query the sequence
for its constituent syllables,
as was done in ??? Choo 2010, eliasmith2012.
For example, to query the model
for the first syllable,
we can bind the sequence semantic pointer
with the inverse of the first position pointer.
\begin{equation}
  \text{FAM} \approx \text{FAMILY} \circledast \text{POS1}^{-1}
\end{equation}

The semantic pointers representing syllable positions
are constructed such that subsequent positions
are the result of binding a position with
a randomly generated $\text{NEXTPOS}$ pointer.
For example,
\begin{equation}
  \label{nextpos}
  \text{POS2} = \text{POS1} \circledast \text{NEXTPOS}
\end{equation}
The initial position, $\text{POS1}$,
and semantic pointers for each syllable
are also randomly generated.\footnote{
  Syllable semantic pointer could be constructed to
  reflect how the syllable is produced; e.g.,
  it could have a representation like
  $\text{BA} = \text{B} \circledast \text{ONSET} \oplus
    \text{A} \circledast \text{NUCLEUS}$,
  where semantic pointers for the individual phonemes
  and syllable parts are randomly generated.
  However, as we will show in section~???,
  we do not need this information to
  produce or recognize syllables.
  Therefore, we deliberately do not impose
  any structure on the syllable semantic pointers
  in this model because we anticipate
  that higher-level linguistic subsystems in Sermo
  will dictate the features of the syllable
  that must be contained
  in the syllable semantic pointer representation.}
Throughout the simulation,
the current syllable to be voiced
is computed by binding the sequence
with the current position pointer.
\begin{equation}
  \text{SYLL} = \text{WORD} \circledast \text{CURRPOS}^{-1},
\end{equation}
where $\text{CURRPOS}$ starts as $\text{POS1}$
and changes over the course of the simulation
by binding with the $\text{NEXTPOS}$ pointer
when each syllable has been voiced.

It should be noted that
this choice of representation
sets a soft upper bound
on the number of syllables in a sequence.
In ??? eliasmith2012,
it was shown that a similar network
could represent lists with six elements accurately;
elements in the middle of
lists with seven or more elements
were often unable to be queried.
However, we assume that the
language subsystems producing
syllable sequences to be voiced
are organized at the level of words,
meaning that most syllable sequences
will be between one and three syllables
on average (see ??? Pellegrino...pdf).
Linguistic subsystems would themselves
use hierarchical representations
of higher level linguistic structures,
allowing for the syllable production subsystem
to receive a reliable stream of
short syllable sequences
that are indistinguishable from
a single long syllable sequence
in most analyses.

Each syllable semantic pointer
is associated with a
trajectory generation subsystem
based on dynamic movement primitives (DMP)
??? cite Schall, DeWolf.
DMPs allow for reliable trajectory generation
that can be temporally scaled,
allowing for syllables to be voiced
quickly or slowly.
Each DMP generates a trajectory
of vocal tract gestures,
which are then mapped to
motor commands that cause
muscle activations in the vocal tract.

Traditionally, DMPs can be either discrete or rhythmic,
depending on whether the trajectory
is followed repetitively.
Since syllables are voiced
one after another in typical speech,
a discrete DMP would seem to be
the obvious choice;
however, it is possible for
the same syllable to be voiced
twice in a row,
and some forms of singing
feature rhythmic repetitions
of the same syllable.
Therefore, we use a hybrid approach
in which syllable DMPs can be used rhythmically
or discretely as needed.

Each DMP consists of a point attractor
and a forcing function.
Currently, we do not implement
separate point attractors for each DMP,
and instead assume that all DMPs
have the same point attractor,
whose stable point is
to have no active vocal tract gestures.
The forcing function for each DMP
generates a sequence of temporally coordinated
vocal tract gestures
that are defined manually.\footnote{
  We assume that these vocal tract gesture sequences
  would be learned through babbling and imitation phases,
  as are modeled in DIVA and the Kr\"{o}ger model.
  We leave the incorporation of this learning process
  into the Sermo model as future work.}

??? should we do all of this with direct production info
and not gestures? It's worth looking into...

??? Do some fine tuning?

\subsection{Neural model}

??? Neural model figure

SPA modules implemented in Nengo are used
to robustly iterate through
the syllable sequence list.
The input sequence is represented
in a \textit{state} module,
which does not modify the sequence.
The current position pointer
is tracked by two interconnected
\textit{working memory} modules,
as was done in ??? eliasmith2012.
The current syllable is computed
by binding the \textit{state}
with the inverse of the
current pointer represented
in \textit{working memory}.
That syllable is sent to
a \textit{temporal associative memory},
which associates a DMP network
with each syllable in the model vocabulary.

The binding operation is computed
in a straightforward manner
(see ??? something for details),
but the two working memory modules
warrant further explanation
in regard to how they
calculate the next position pointer.
This calculation
uses the current position
(see equation~\eqref{nextpos}),
so the straightforward approach
of computing this in a single recurrent loop
fails because one of the two pointers
being used in the computation
is also the result of the computation.
Instead, we exploit the fact that
each working memory module is gated
such that it operates in two regimes.
In the ``acquisition'' regime,
the memory module acquires a state to remembered.
In the ``maintenance'' regime,
the memory module ignores any incoming state,
and maintains a memory of the last state
encountered in the first regime.
The two working memory modules
are connected to each other,
and are always in the opposite regime.
Initially, the first memory
acquires the $\text{POS1}$ state
from an external source.
The gating signal turns on,
which sets the first memory
into the maintenance regime,
representing the $\text{POS1}$ state.
This simultaneously sets the second memory
into the acquisition regime,
where it acquires the state
being maintained by the first memory
($\text{POS1}$ initially).
The second memory is connected
to the input of the first memory
such that it computes the
next position in the connection,
and therefore is providing
$\text{POS2}$ as input to the first memory,
though it is currently ignoring it.
When the gating signal is released,
the two memories switch regimes.
The second memory maintains
its memory of $\text{POS1}$
and therefore projects $\text{POS2}$
to the first memory;
the first memory acquires
the $\text{POS2}$ state from the second memory.
This process repeats for each position
in the syllable sequence;
each time the gating signal
is turned on and off,
the position pointer increments.

This method of incrementing the position pointer
is both robust and fast.
It takes less than 50 ms to acquire
a new memory state,
and that state can be maintained
for several seconds.
Since syllables take
on the order of hundred of milliseconds
to be voiced,
the circuit can go through a syllable sequence
fast enough for spirited conversation.
However, one aspect of the circuit as described
poses a timing challenge.
As discussed in section~??? derivs,
there is a slight transmission delay
when sending a signal between two ensembles.
Yet, humans are capable of producing
continuous speech with
few to no discernible pauses between syllables.
Therefore, the next syllable to be voiced
must follow the current syllable very quickly,
even overlapping when appropriate.

To facilitate this,
we use the trajectory generating DMPs
to also generate control signals
to switch the working memories
to the next syllable,
and to initiate the next syllable's DMP oscillator.
Since the DMP is already involved in
temporally coordinating
the vocal tract movements
producing the syllable itself,
it is ideally suited to
ensure that the next syllable
is timed appropriately.
Since it is only providing control signals
to the working memory modules,
it can transition to the next syllable
without needing to know
what syllable will be voiced next.

Like non-temporal associative memories
(see ??? stewart2011.pdf),
the temporal associative memory
contains an ensemble
for each semantic pointer
in the memory's vocabulary
that is responsible for computing
the similarity between
that semantic pointer
and some input signal.
This computation is carried out
through the connection between
the ensemble representing the input signal
and the ensemble representing
the similarity to the desired pointer.
The weights on this linear connection
are set to be the semantic pointer itself;
the encoding process (see equation~\eqref{???})
therefore computes the dot product
between the desired semantic pointer
and the input signal.
Since semantic pointers are not necessarily
orthogonal to each other,
more than one ensemble may activate
for a given input signal.
In order to clean up to a single
output semantic pointer,
all of the ensembles
representing semantic pointer similarity
mutually inhibit one another.
As long as the semantic pointers
are sufficiently dissimilar
to each other,
this mutual inhibition
implements a winner-take-all function,
resulting in only the ensemble
representing similarity for the
semantic pointer represented by
the input signal being active.

In non-temporal associative memories,
the similarity ensembles
project either the same (autoassociative)
or another (heteroassociative) semantic pointer
to a \textit{state} module,
which therefore represents
a static cleaned up version of the pointer
associated with
the original noisy input signal.
In temporal associative memories,
we instead associate each similarity ensemble
with a DMP network so that
the \textit{state} module
represents a continuous trajectory
in some vector space.
In our case, the vector space of interest
consists of 48 dimensions,
each representing a vocal tract gesture.

As previously mentioned,
our DMP networks can be used in either
a discrete or rhythmic way.
Normally, discrete DMP networks
are created by decoding the forcing function
off of a one-dimensional ramp,
playing out the trajectory defined by the forcing function
as the ramp climbs from zero to one.
Rhythmic DMPs are created by
decoding the forcing function
off of a controlled two-dimensional oscillator,
playing out the trajectory defined by the forcing function
as the oscillator's angle moves from
$0$ to $2\pi$.
The ability to climb the ramp
at different velocities
or drive the oscillator
at different frequencies
is what enables the DMP
to generate trajectories at different timescales.

In our DMP networks,
we enable discrete or rhythmic behavior
by using a two-dimensional oscillator
which has a ``dead zone'' in
phase space such that,
without intervention,
it will perform only one incomplete cycle,
allowing for a discrete trajectory to
be decoded from the oscillator.
However, unlike discrete trajectories
that use ramps,
providing a short stimulus as the oscillator
reaches the dead zone can allow it
to complete another oscillatory cycle;
discrete trajectories using ramps must
be completely inhibited before another
trajectory can be generated,
which would cause disfluencies
when repeating syllables.
However, the ability to operate
both discretely and rhythmically
comes at the cost of robustness
and frequency range.
Typical rhythmic DMPs
can operate at high frequencies.
High frequencies could cause our DMPs
to have enough momentum to pass through
the dead zone and continue oscillating.
In general, the size of the dead zone
can be used to balance the tradeoff
between how much of the phase space
can be used for the forcing function,
and how high the frequency can be pushed
before unwanted rhythmic behavior occurs.

The forcing function of each DMP
interpolates a vector representation of
a manually defined
vocal tract gesture score over time.
The gesture scores are
designed to be used with the VocalTractLab
articulatory synthesizer ??? cite.
While a full speech production system
would also include a mapping from
the gesture score to
vocal tract parameter changes,
this mapping would be identical
for all syllables
and so we leave the neural implementation
of this mapping as future work.
Instead, we automatically generate
a gesture score from the neurally generated
vector representation over time,
which can then be synthesized
by VocalTractLab.

\subsection{Vocal tract parameters}

??? Instead of gestures, we could instead
generate parameters. Pro: continuous, lower dimension.
Con: can't optimize the mapping between
gesture and parameters.
However, worth trying out?

\subsection{Evaluation}

??? compare original VTG -> audio to Nengo -> VTG -> audio

??? The goal of a syllable production system
is to generate an audio waveform
that is perceptually classified
by a listener as corresponding to
the intended syllable sequence.
While it is possible to quantify
whether this goal is achieved
by ...

??? For testing, we use gesture scores
for German syllables
provided by Bernd Kr\"{o}ger.

\section{Syllable recognition}

Go from a vocal tract gesture score
to a syllable semantic pointer, in neurons.

\subsection{Neural model}

\subsection{Evaluation}

\section{Syllable consolidation}

Go from vocal tract gesture score
to audio signal, in neurons.

Learn a syllable recognition
and a syllable production
network from this.

Contrast this to syllable production because it's
an infrequently voiced syllable.

\subsection{Bootstrapped syllable learning}

??? A full picture of speech development involves:
learning vocal tract gestures
through reinforced motor babbling,
learning basic syllables
through mimicry?,
and scaling up to the elements of this model.
While we think that our model
is a useful starting point for such
a developmental model,
as it provides an end-target,
we don't claim to do this kind of structural learning.
However, we believe that error-based learning
could result in this kind of system;
to show that this is a possibility,
we consider a minimal learning situation:
learning voice a new syllable
given a set of existing syllables.
This type of thing probably happens
in second language acquisition,
when novel combinations of phonemes
are encountered,
or even during first language acquisition
as pronunciation is refined over
the course of one's life;
words that were once awkward combinations
of many syllables are compressed into
nearly equivalent sequences of fewer,
more complex syllables.

As opposed to conversational shadowing,
which highlights the high-level strengths
of the integrated speech system,
syllable learning highlights
the low-level strengths of this system.
Syllable learning involves
learning a novel set of gestures
and associated articulator trajectories
that will voice a syllable
that is encountered for the first time.

We call our syllable learning system ``bootstrapped''
because we assume that our system
has an existing repertoire of syllables
that it is already able to voice.
These existing syllables will be
used in learning the new syllable.
Bootstrapped syllable learning contrasts with
the type of syllable learning
done as an infant and toddler,
which uses reinforced speech babbling
to learn novel syllables.
While we believe that learning syllables
from babbling is an important research direction
that can be explored in this system,
it has also been explored in many other systems
(???cite Diva etc),
and so we have chosen to leave this type of learning
for future work.

The bootstrapped syllable learning system
learns new syllables in three steps.

\begin{enumerate}
\item Initialize the new syllable from the most
  similar existing syllable.
  For example, when learning to voice
  the syllable /ba/, the system should
  start from the syllable /fa/ if it is known.
\item Swap compatible speech gestures.
  For example, vowel producing gestures
  would be compatible, allowing for modifying
  a /ba/ to a /bu/, and so on.
  The choice of which gesture to swap and
  how to swap it will be informed by
  ???figure out.
\item Fine-tune the voiced syllable
  until it can be recognized as the
  syllable to be learned.
  ???more
\end{enumerate}

??? NB: the oscillator being learned should exist outside of
the normal speech system so that they can both run
at the same time, but there should be a switch kind of thing
to put the new syllable through.

??? hypothesis (not sure where to put this):
the mapping between phoneme to gesture
is such that is not advantageous
to represent in the synthesis
(maybe also recognition?) system(s).
Therefore, it may be the case that
not all people have phoneme representations.
However, we propose that phonemes
are a useful construct for learning to voice
novel syllables in a second-language learning situation.

These steps require several systems
that have already been implemented
in the recognition and synthesis systems separately;
for example, the ability to compare
voiced syllables to those already known
is one of the primary goals of the recognition system itself,
so it can be leveraged when trying to learn new syllables.
However, these steps also point to new systems
that must be implemented.
First, the system requires a method
to transfer a syllable-producing function
from one ensemble to another.
Second, some knowledge of which gestures
are compatible must be built into the system.
Finally, ???fine-tuning.

It has been shown that in second-language learning,
slowing down the syllable can have
a significant increase in learning effectiveness
???cite.
We believe that our system emulates
how a native speaker of one language
would learn to voice novel syllables in a second language.
Therefore, we predict that slowing down
the speed at which the syllables are heard
and uttered will improve ???learning speed
and / or quality of learned syllable.

??? Mental syllabary (1-s2.0-S009394X...pdf)
supports our architecture

\subsection{Network diagram}

\subsection{Evaluation}
