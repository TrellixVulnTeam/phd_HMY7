\chapter{Implementation}

In this thesis, we implement and evaluate
four neural models that make up
parts of the conceptual Sermo model.
Each model provides
an explanation for how the brain might
solve a task necessary for
speech recognition or synthesis
using the computational devices at hand,
namely spiking neurons.
However, where possible,
we also present analogous non-neural models
that could be incorporated in
non-neural speech recognition and synthesis systems.

\section{Neural cepstral coefficients}

The frontend of most automatic speech recognition systems
extracts a feature vector from the incoming audio waveform.
As reviewed in section ???,
mel-frequency cepstral coefficients (MFCCs)
have been used successfully in many ASR systems ???cite.
To a certain extent, MFCCs are motivated
by the organization of the human auditory system.
We propose neural cepstral coefficients (NCCs)
as an alternative feature vector representation
for automatic speech recognition
and other tasks involving speech processing.
Unlike MFCCs, NCCs are both motivated
by the organization of the human auditory system
and are implemented the same basic computational units
as the human auditory system,
spiking neural networks.

??? NCC pipeline, like AuditoryBasedFeatureVectors.pdf
or KumarKimSternICA11.pdf

Figure~??? shows the NCC processing pipeline,
as compared to the MFCC processing pipeline.
While the pipeline implements
essentially the same computations,
the manner in which those computations
are performed differs significantly.
The primary difference is that
incoming audio is processed
in a continuous online fashion,
rather than in a frame-based fashion.
Typical MFCC extraction algorithms
break the audio signal into
overlapping fixed-length windows
called frames,
which are processed independently
until the smoothing stage,
where discontinuities
due to frame boundaries are
minimized.
The NCC extraction algorithm,
on the other hand,
maintains internal state
at each processing stage,
and updates that internal state
for each incoming audio sample.
As each internal state update
depends on the current state,
changes occur smoothly through time,
which obviates the need for
an explicit smoothing step.

Another implementation difference
is in the computation of
the inverse discrete cosine transform.
Typically, this computation is done as
\begin{equation}
  y_k = \frac{x_0}{\sqrt{N}} + \sqrt{\frac{2}{N}} \sum_{n=1}^{N-1}
  x_n \cos \left( \frac{\pi}{N} n \left( k + \frac{1}{2} \right) \right)
  \text{ for } 0 \le k < N
\end{equation}
where $y_k$ is the $k$th cepstral coefficient,
$x_n$ is the $n$th auditory filter output,
and $N$ is the number of auditory filter outputs.
In matrix notation,
this equation can be expressed as
\begin{align}
  \label{idct}
  \mathbf{k} &= \left[ 0, 1, \ldots, N-1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{s} &= \left[ \sqrt{2}, 1, 1, \ldots, 1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{T} &= \sqrt{2}{N} \, \mathbf{s} \circ \cos \left( \frac{\pi}{N} \left(
    \mathbf{k} + \frac{1}{2} \right) \otimes \mathbf{k} \right)
    & N \times N \text{ matrix} \nonumber \\
  \mathbf{y} &= \mathbf{T}\mathbf{x} & N \times 1 \text{ vector}
\end{align}
where $\circ$ is the Hadamard (element-wise) product,
and $\otimes$ is the outer product.
This rearranged equation
allows us to precompute a matrix, $T$,
that maps auditory filter outputs
directly to cepstral coefficients,
which enables us to embed this matrix
in the connection weights between
two neural population
(see ??? network diagram for more details).
By restricting $\mathbf{T}$
to the first $M$ rows,
equation \eqref{idct} yields the first
$M$ cepstral coefficients;
typically, ASR systems
use $N \approx 26$ auditory filter outputs
and $M \approx 13$ cepstral coefficients.

\subsection{NCC neural model}

The NCC pipeline is implemented
in a Nengo network that uses
the Brian hears library
of auditory filter models
??? cite Brian hears,
and two layers
leaky integrate-and-fire (LIF) neurons ensembles
connected in a feed-forward manner
(see ??? figure).
While Brian has the capability
to simulate spiking neurons,
we only use Brian's implementation of
auditory periphery models,
which do not emit spikes.
These models,
summarized in ??? sect prev work,
emulate the effect of the
audio waveform on the basilar membrane,
and the resulting inner hair cell activity
arising from basilar membrane deflections,
which may include rectification
and compression
(i.e., it accomplishes the first
??? steps of the MFCC pipeline).
The output of these models
can be though of as
the current input
to spiral ganglion cells in the ear.
These auditory periphery models
accomplish a significant portion
of the MFCC extraction pipeline,
which is unsurprising as MFCC extraction
is designed to emulate certain aspects
of the human auditory system.

??? figure: net diagram

The simulation timesteps
for the Nengo model and the contained
Brain auditory filter model
are uncoupled.
The auditory filter model
is always run with the same timestep
as the auditory stimulus;
that is, its internal state is updated
for each audio sample.
The Nengo model, on the other hand,
runs at a timestep independent
of the auditory stimulus.
On each Nengo timestep,
therefore, one or more samples
of the audio input
is fed through the
auditory filter model.
The \textit{auditory nerve} ensembles
receives as input the current
state of the auditory filter model,
regardless of how many timesteps
the Brian model has advanced.

The \textit{auditory nerve} ensembles
mimic the activity of spiral ganglion cells
projecting down the auditory nerve.
For each characteristic frequency,
a heterogeneous population of neurons
is driven by the current output by
the auditory filter model.
Each characteristic frequency
drives an independent neural ensemble,
meaning that this layer of ensembles
can be thought of as being cochleotopically organized,
as the mean activity of an individual neuron
only gives information about the output
of a single auditory filter
(although that filter may be modulated
by power in nearby frequencies).
Together, the decoded output of
each ensemble gives an approximation of
the compressed power in the part of the spectrum
that the auditory filter is sensitive to.
This quantity is essentially
the output of the auditory filter
(i.e., steps ???--??? in the MFCC pipeline),
but now represented in
cochleotopically organized spiking neurons.
It is also the $X$ vector in
the inverse discrete cosine transform,
equation~\eqref{idct}.

The \textit{cepstra} ensembles
each represent one cepstral coefficient.
The inverse discrete cosine transform
for that coefficient is computed
through the connections between
the auditory nerve ensembles
and the cepstral ensembles.
Specifically, each connection
between an auditory periphery
and cepstrum ensemble
scales the decoded value of the
cepstrum ensemble by
the corresponding value
the $\mathbf{T}$ matrix.
Since this is a linear transform,
it is done by computing
normal decoding weights
according to equation~\eqref{???}
and scaling the decoders by $T_{i,j}$.
Since the currents coming from
multiple connections to an ensemble
are summed, the cepstral ensembles
end up representing the
dot product between the row of
$T$ that corresponds to its cepstrum
and the $X$ vector,
which is collectively represented
by the auditory periphery layer.

The end result of the networks
is the decoded output
of the \textit{cepstra} ensembles,
which is a continuously varying,
automatically smoothed set of cepstral coefficients.
We hypothesize that these coefficients can be used
as the feature vectors in an
automatic speech recognition system.

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Delta neural cepstral coefficients}

Another common technique in designing frontends
for automatic speech recognition is to
include temporal derivatives
in the feature vector.
Appending MFCC derivatives
to the feature vector
(i.e., MFCC velocities;
called Delta-Cepstral Coefficients [DCC])
improve ASR accuracy.
Appending derivatives of the MFCC derivatives
(i.e., MFCC accelerations;
called Double Delta-Cepstral Coefficients [DDCC])
further improves recognition,
though by a smaller margin than
the improvement from the first derivative
??? find good cite.
??? Kumar et al kumarkimsternica11.pdf
used the derivative of the auditory filter output
instead of the raw auditory filter output
in a MFCC-like pipeline
(called Delta-Spectral Cepstral Coefficients [DSCC])
and found that the ASR system
was more robust to noise and reverberation.

Temporal derivatives in these systems
are typically computed through straightforward
finite differences.
That is, for an MFCC $y_k$
at frame $t$, the corresponding delta is
\begin{equation}
  \label{dcc}
  \delta_k(t) = y_k(t+n) - y_k(t-n),
\end{equation}
where $n$ is a small number of frames
(typically between one and three).
The double delta applies the same operation
to the $\delta_k$ vectors.

% An additional nonlinearity is applied
% for DSCCs,
% as they use the derivative of the power spectrum.
% Using the raw power spectrum derivatives
% results in very small cepstral coefficients,
% so ??? Kim et al proposed a ``Gaussianization''
% step in which the power spectrum
% is made to follow a Gaussian distribution
% by inverting the normal cumulative distribution function.
% ??? more?

\subsection{NDCC and NSCC neural models}

Implementing temporal differentiation
in spiking neural networks
is not as straightforward as implementing
equation~\eqref{dcc} in neurons,
as the neural simulation operates
online in continuous time,
and therefore does not necessarily
have access to the information
from previous timesteps.
However, there are several ways
to build networks that maintain
information from the past
and can therefore implement temporal differentiation.

??? ref Tripp describes six such models,
two of which use feedforward connections,
two use adaptive neuron models,
and two implement linear time-invariant (LTI) filters
through recurrent connections.
All six models successfully
compute a temporal derivative,
so for simplicity we implemented
the two feedforward temporal differentiation models,
with the assumption that
more sophisticated temporal differentiation
models could be used if desired.

??? Figure: both derivs

In the first feedforward differentiation model,
an intermediate ensemble is introduced
between the ensemble representing the input signal
and the ensemble that we want to represent
the derivative of the input signal
(see Figure~???).
The \textit{derivative} ensemble
receives direct positively scaled input from
the \textit{input} ensemble,
and delayed negatively scaled input from
the \textit{intermediate} ensemble.
The slight delay between the positive signal
communicated from the \textit{input}
and the negative signal
from the \textit{intermediate} ensemble
results in the \textit{derivative} ensemble
representing the temporal derivative
of the input signal.

Information transmitted between two ensembles
is slightly delayed
because changes to the input currents
of the first ensemble
must first be integrated for one or more
timesteps before those changes can result in
spikes that influence
the second ensemble's input currents.
The amount of time between
when input signal changes are reflected
in the first and second populations
depends on several factors,
including the simulation timestep,
the neuron model, and the synaptic filter model.
In the feedforward differentiation model
with an intermediate ensemble,
all connections use a lowpass filter
with a relatively long time constant
($\tau=0.1 \text{ s}$)
as the synaptic filter model.\footnote{
  Synaptic filter time constants of around 100 ms
  are consistent with NMDA glutamate receptors,
  which ??? cite 1213.full.pdf have suggested
  are part of differentiator circuits
  underlying respiratory rhythm.
  Additionally, in terms of
  the biological plausibility of the network as a whole,
  ??? Tripp notes that this form of network
  has been observed in \textit{C. elegans} ??? cite Dunn
  and may underlie chemotaxis.}

In the second feedforward differentiation model,
two connections are made between
the \textit{input} and \textit{derivative} ensembles.
The first positively scaled connection
uses a synaptic filter with a fast time constant
($\tau_F=0.005 \text{ s}$),
and the second negatively scaled connection
uses a synaptic filter with a slow time constant
($\tau_s=0.1 \text{ s}$).\footnote{
  Synaptic filter time constants of 5 ms
  are consistent with some types of
  AMPA receptors ??? cite Jonasetal.}
Like the first model,
this approach exploits the delays introduced
by synaptic filtering,
subtracting a more slowly changing signal
from a signal changing quickly,
resulting in a temporal derivative.
While this network has the advantage
of requiring fewer neurons
as no intermediate connection is introduced,
it adds the constraint that
the neurons in the \textit{derivative} ensemble
contain both fast and slow
neurotransmitter receptors.\footnote{
  Since AMPA and NMDA are both glutamate receptors,
  no constraint is placed on the \textit{input} population.
  ??? Tripp notes that a derivative circuit
  in the vestibular system of oyster toadfish
  ??? PNAS-2004-Hols...
  is likely to match the form of the second
  feedforward differentiation model.}

These differentiator networks
are added to the NCC network
shown in Figure~???
to compute the desired derivative.

??? Figure of whole NCC circuit

% ?? discuss whether we can do gaussianization?
% we probably can... but maybe not in a bioplaus manner.
% Depends if it's necessary anyhow

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Evaluation}

We hypothesize that NCCs and DNCCs
are suitable feature vectors
for traditional ASR systems
and for more biologically plausible
models of the human speech system like Sermo.
In order to evaluate their effectiveness
in these situations,
we compare MFCCs with and without DCCs
and DDCCs to NCCs and DNCCs
on phoneme and word classification tasks.

??? choose an SVM implementation and
explain it here

??? it's not clear if the derivatives will help
since the SVM should manage these,
so if they don't help then I will
just use the derivatives

\section{Syllable recognition}

Go from a vocal tract gesture score
to a syllable semantic pointer, in neurons.

\subsection{Network diagram}

\subsection{Evaluation}

\section{Syllable production}

Go from a syllable semantic pointer
to audio signal, in neurons.

??? Do some fine tuning?

??? Make a note that this section was done
in collaboration with Bernd

\subsection{Network diagram}

\subsection{Evaluation}

\section{Syllable consolidation}

Go from vocal tract gesture score
to audio signal, in neurons.

Learn a syllable recognition
and a syllable production
network from this.

\subsection{Network diagram}

\subsection{Evaluation}
