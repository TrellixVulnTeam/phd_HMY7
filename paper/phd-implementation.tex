\chapter{Implementation}

In this thesis, we implement and evaluate
four neural models that make up
parts of the conceptual Sermo model.
Each model provides
an explanation for how the brain might
solve a task necessary for
speech recognition or synthesis
using the computational devices at hand,
namely spiking neurons.
However, where possible,
we also present analogous non-neural models
that could be incorporated in
non-neural speech recognition and synthesis systems.

\section{Neural cepstral coefficients}

\probbox{A raw audio waveform has high temporal resolution,
  but low spatial resolution, as it is a one-dimensional signal.
  Audio must therefore be preprocessed such that
  all the temporal information necessary to recognize speech
  is available at the current moment in time.
  Automatic speech recognition systems preprocess speech by
  extracting feature vectors containing cepstral coefficients.
  How can a neural model like Sermo preprocess speech?}

The frontend of most automatic speech recognition systems
extracts a feature vector from the incoming audio waveform.
As reviewed in section ???,
mel-frequency cepstral coefficients (MFCCs)
have been used successfully in many ASR systems ???cite.
To a certain extent, MFCCs are motivated
by the organization of the human auditory system.
We propose neural cepstral coefficients (NCCs)
as an alternative feature vector representation
for automatic speech recognition
and other tasks involving speech processing.
Unlike MFCCs, NCCs are both motivated
by the organization of the human auditory system
and are implemented the same basic computational units
as the human auditory system,
spiking neural networks.

??? NCC pipeline, like AuditoryBasedFeatureVectors.pdf
or KumarKimSternICA11.pdf

Figure~??? shows the NCC processing pipeline,
as compared to the MFCC processing pipeline.
While the pipeline implements
essentially the same computations,
the manner in which those computations
are performed differs significantly.
The primary difference is that
incoming audio is processed
in a continuous online fashion,
rather than in a frame-based fashion.
Typical MFCC extraction algorithms
break the audio signal into
overlapping fixed-length windows
called frames,
which are processed independently
until the smoothing stage,
where discontinuities
due to frame boundaries are
minimized.
The NCC extraction algorithm,
on the other hand,
maintains internal state
at each processing stage,
and updates that internal state
for each incoming audio sample.
As each internal state update
depends on the current state,
changes occur smoothly through time,
which obviates the need for
an explicit smoothing step.

Another implementation difference
is in the computation of
the inverse discrete cosine transform.
Typically, this computation is done as
\begin{equation}
  y_k = \frac{x_0}{\sqrt{N}} + \sqrt{\frac{2}{N}} \sum_{n=1}^{N-1}
  x_n \cos \left( \frac{\pi}{N} n \left( k + \frac{1}{2} \right) \right)
  \text{ for } 0 \le k < N
\end{equation}
where $y_k$ is the $k$th cepstral coefficient,
$x_n$ is the $n$th auditory filter output,
and $N$ is the number of auditory filter outputs.
In matrix notation,
this equation can be expressed as
\begin{align}
  \label{idct}
  \mathbf{k} &= \left[ 0, 1, \ldots, N-1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{s} &= \left[ \sqrt{2}, 1, 1, \ldots, 1 \right] & 1 \times N \text{ vector} \nonumber \\
  \mathbf{T} &= \sqrt{2}{N} \, \mathbf{s} \circ \cos \left( \frac{\pi}{N} \left(
    \mathbf{k} + \frac{1}{2} \right) \otimes \mathbf{k} \right)
    & N \times N \text{ matrix} \nonumber \\
  \mathbf{y} &= \mathbf{T}\mathbf{x} & N \times 1 \text{ vector}
\end{align}
where $\circ$ is the Hadamard (element-wise) product,
and $\otimes$ is the outer product.
This rearranged equation
allows us to precompute a matrix, $\mathbf{T}$,
that maps auditory filter outputs
directly to cepstral coefficients,
which enables us to embed this matrix
in the connection weights between
two neural population
(see ??? network diagram for more details).
By restricting $\mathbf{T}$
to the first $M$ rows,
equation \eqref{idct} yields the first
$M$ cepstral coefficients;
typically, ASR systems
use $N \approx 26$ auditory filter outputs
and $M \approx 13$ cepstral coefficients.

\subsection{NCC neural model}

The NCC pipeline is implemented
in a Nengo network that uses
the Brian hears library
of auditory filter models
??? cite Brian hears,
and two layers
leaky integrate-and-fire (LIF) neurons ensembles
connected in a feed-forward manner
(see ??? figure).
While Brian has the capability
to simulate spiking neurons,
we only use Brian's implementation of
auditory periphery models,
which do not emit spikes.
These models,
summarized in ??? sect prev work,
emulate the effect of the
audio waveform on the basilar membrane,
and the resulting inner hair cell activity
arising from basilar membrane deflections,
which may include rectification
and compression
(i.e., it accomplishes the first
??? steps of the MFCC pipeline).
The output of these models
can be though of as
the current input
to spiral ganglion cells in the ear.
These auditory periphery models
accomplish a significant portion
of the MFCC extraction pipeline,
which is unsurprising as MFCC extraction
is designed to emulate certain aspects
of the human auditory system.

??? figure: net diagram

The simulation timesteps
for the Nengo model and the contained
Brain auditory filter model
are uncoupled.
The auditory filter model
is always run with the same timestep
as the auditory stimulus;
that is, its internal state is updated
for each audio sample.
The Nengo model, on the other hand,
runs at a timestep independent
of the auditory stimulus.
On each Nengo timestep,
therefore, one or more samples
of the audio input
is fed through the
auditory filter model.
The \textit{auditory nerve} ensembles
receives as input the current
state of the auditory filter model,
regardless of how many timesteps
the Brian model has advanced.

The \textit{auditory nerve} ensembles
mimic the activity of spiral ganglion cells
projecting down the auditory nerve.
For each characteristic frequency,
a heterogeneous population of neurons
is driven by the current output by
the auditory filter model.
Each characteristic frequency
drives an independent neural ensemble,
meaning that this layer of ensembles
can be thought of as being cochleotopically organized,
as the mean activity of an individual neuron
only gives information about the output
of a single auditory filter
(although that filter may be modulated
by power in nearby frequencies).
Together, the decoded output of
each ensemble gives an approximation of
the compressed power in the part of the spectrum
that the auditory filter is sensitive to.
This quantity is essentially
the output of the auditory filter
(i.e., steps ???--??? in the MFCC pipeline),
but now represented in
cochleotopically organized spiking neurons.
It is also the $X$ vector in
the inverse discrete cosine transform,
equation~\eqref{idct}.

The \textit{cepstra} ensembles
each represent one cepstral coefficient.
The inverse discrete cosine transform
for that coefficient is computed
through the connections between
the auditory nerve ensembles
and the cepstral ensembles.
Specifically, each connection
between an auditory periphery
and cepstrum ensemble
scales the decoded value of the
cepstrum ensemble by
the corresponding value
the $\mathbf{T}$ matrix.
Since this is a linear transform,
it is done by computing
normal decoding weights
according to equation~\eqref{???}
and scaling the decoders by $T_{i,j}$.
Since the currents coming from
multiple connections to an ensemble
are summed, the cepstral ensembles
end up representing the
dot product between the row of
$T$ that corresponds to its cepstrum
and the $X$ vector,
which is collectively represented
by the auditory periphery layer.

The end result of the networks
is the decoded output
of the \textit{cepstra} ensembles,
which is a continuously varying,
automatically smoothed set of cepstral coefficients.
We hypothesize that these coefficients can be used
as the feature vectors in an
automatic speech recognition system.

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Delta neural cepstral coefficients}

Another common technique in designing frontends
for automatic speech recognition is to
include temporal derivatives
in the feature vector.
Appending MFCC derivatives
to the feature vector
(i.e., MFCC velocities;
called Delta-Cepstral Coefficients [DCC])
improve ASR accuracy.
Appending derivatives of the MFCC derivatives
(i.e., MFCC accelerations;
called Double Delta-Cepstral Coefficients [DDCC])
further improves recognition,
though by a smaller margin than
the improvement from the first derivative
??? find good cite.
??? Kumar et al kumarkimsternica11.pdf
used the derivative of the auditory filter output
instead of the raw auditory filter output
in a MFCC-like pipeline
(called Delta-Spectral Cepstral Coefficients [DSCC])
and found that the ASR system
was more robust to noise and reverberation.

Temporal derivatives in these systems
are typically computed through straightforward
finite differences.
That is, for an MFCC $y_k$
at frame $t$, the corresponding delta is
\begin{equation}
  \label{dcc}
  \delta_k(t) = y_k(t+n) - y_k(t-n),
\end{equation}
where $n$ is a small number of frames
(typically between one and three).
The double delta applies the same operation
to the $\delta_k$ vectors.

% An additional nonlinearity is applied
% for DSCCs,
% as they use the derivative of the power spectrum.
% Using the raw power spectrum derivatives
% results in very small cepstral coefficients,
% so ??? Kim et al proposed a ``Gaussianization''
% step in which the power spectrum
% is made to follow a Gaussian distribution
% by inverting the normal cumulative distribution function.
% ??? more?

\subsection{NDCC and NSCC neural models}

Implementing temporal differentiation
in spiking neural networks
is not as straightforward as implementing
equation~\eqref{dcc} in neurons,
as the neural simulation operates
online in continuous time,
and therefore does not necessarily
have access to the information
from previous timesteps.
However, there are several ways
to build networks that maintain
information from the past
and can therefore implement temporal differentiation.

??? ref Tripp describes six such models,
two of which use feedforward connections,
two use adaptive neuron models,
and two implement linear time-invariant (LTI) filters
through recurrent connections.
All six models successfully
compute a temporal derivative,
so for simplicity we implemented
the two feedforward temporal differentiation models,
with the assumption that
more sophisticated temporal differentiation
models could be used if desired.

??? Figure: both derivs

In the first feedforward differentiation model,
an intermediate ensemble is introduced
between the ensemble representing the input signal
and the ensemble that we want to represent
the derivative of the input signal
(see Figure~???).
The \textit{derivative} ensemble
receives direct positively scaled input from
the \textit{input} ensemble,
and delayed negatively scaled input from
the \textit{intermediate} ensemble.
The slight delay between the positive signal
communicated from the \textit{input}
and the negative signal
from the \textit{intermediate} ensemble
results in the \textit{derivative} ensemble
representing the temporal derivative
of the input signal.

Information transmitted between two ensembles
is slightly delayed
because changes to the input currents
of the first ensemble
must first be integrated for one or more
timesteps before those changes can result in
spikes that influence
the second ensemble's input currents.
The amount of time between
when input signal changes are reflected
in the first and second populations
depends on several factors,
including the simulation timestep,
the neuron model, and the synaptic filter model.
In the feedforward differentiation model
with an intermediate ensemble,
all connections use a lowpass filter
with a relatively long time constant
($\tau=0.1 \text{ s}$)
as the synaptic filter model.\footnote{
  Synaptic filter time constants of around 100 ms
  are consistent with NMDA glutamate receptors,
  which ??? cite 1213.full.pdf have suggested
  are part of differentiator circuits
  underlying respiratory rhythm.
  Additionally, in terms of
  the biological plausibility of the network as a whole,
  ??? Tripp notes that this form of network
  has been observed in \textit{C. elegans} ??? cite Dunn
  and may underlie chemotaxis.}

In the second feedforward differentiation model,
two connections are made between
the \textit{input} and \textit{derivative} ensembles.
The first positively scaled connection
uses a synaptic filter with a fast time constant
($\tau_F=0.005 \text{ s}$),
and the second negatively scaled connection
uses a synaptic filter with a slow time constant
($\tau_s=0.1 \text{ s}$).\footnote{
  Synaptic filter time constants of 5 ms
  are consistent with some types of
  AMPA receptors ??? cite Jonasetal.}
Like the first model,
this approach exploits the delays introduced
by synaptic filtering,
subtracting a more slowly changing signal
from a signal changing quickly,
resulting in a temporal derivative.
While this network has the advantage
of requiring fewer neurons
as no intermediate connection is introduced,
it adds the constraint that
the neurons in the \textit{derivative} ensemble
contain both fast and slow
neurotransmitter receptors.\footnote{
  Since AMPA and NMDA are both glutamate receptors,
  no constraint is placed on the \textit{input} population.
  ??? Tripp notes that a derivative circuit
  in the vestibular system of oyster toadfish
  ??? PNAS-2004-Hols...
  is likely to match the form of the second
  feedforward differentiation model.}

These differentiator networks
are added to the NCC network
shown in Figure~???
to compute the desired derivative.

??? Figure of whole NCC circuit

% ?? discuss whether we can do gaussianization?
% we probably can... but maybe not in a bioplaus manner.
% Depends if it's necessary anyhow

??? make an appendix with a simplified Nengo model
and a table of parameters and typical values

\subsection{Evaluation}

We hypothesize that NCCs and DNCCs
are suitable feature vectors
for traditional ASR systems
and for more biologically plausible
models of the human speech system like Sermo.
In order to evaluate their effectiveness
in these situations,
we compare MFCCs with and without DCCs
and DDCCs to NCCs and DNCCs
on phoneme and word classification tasks.

??? choose an SVM implementation and
explain it here

??? it's not clear if the derivatives will help
since the SVM should manage these,
so if they don't help then I will
just use the derivatives

\section{Syllable production}

\probbox{Once the intention to voice a syllable sequence
  has been formed, the motor system must translate
  a sequence of static conceptual representations of those syllables
  to a continuous trajectory of motor commands
  that will cause muscle activations in the vocal tract.
  How can a neural model like Sermo generate
  a motor command trajectory from
  a sequence of static syllable representation?}

We theorize that the speech motor system
is organized at the level of syllables,
and that motor command trajectories are explicitly stored
for syllables that are frequently voiced.
In this neural model,
we describe how sequences of syllables are represented,
and how those representations
generate motor command trajectories
that can control an articulatory synthesizer.

A sequence of syllables is represented
by a semantic pointer
that tags each syllable with its place
in the list; i.e.,
\begin{equation}
  \text{SEQ} = \text{SYLL1} \circledast \text{POS1} \oplus
    \text{SYLL2} \circledast \text{POS2} \ldots
\end{equation}
For example, the word ``family'' would
be represented as ??? /fam schwa lee/
\begin{equation}
  \text{FAMILY} = \text{FAM} \cirledast \text{POS1} \oplus
    \text{SCHWA} \circledast \text{POS2} \oplus
    \text{LEE} \circledast \text{POS3}
\end{equation}

??? this representation places a limitation
on the number of syllables that can be in a sequence.
From Spaun, that limit is around 7 ??? look up.
most language average between 1 and 3 syllables
per word ??? Pellegrino...pdf
We don't believe that this limit actual limits us,
because the linguistic subsystem would be hierarchically
organized such that it is producing sequences of words,
each of which is its own sequence of syllables
that can be easily represented and queried.
long sequences of syllables would therefore
be a consequences of having word sequences
map onto syllable sequences.

The syllable production model
begins with a syllable sequence
randomly generated semantic pointer
that represents a syllable.\footnote{
  The semantic pointer could reflect
  how the syllable is produced; e.g.,
  it could have a representation like
  $\text{BA}=\text{B}\circledast\text{ONSET}\oplus
  \text{A}\circledast\text{NUCLEUS}$,
  where semantic pointers for the individual phonemes
  and syllable parts are randomly generated.
  However, as we will show in section~???,
  we do not need this information
  recognize syllables using production information.
  Therefore, we deliberately do not impose
  any structure on the syllable semantic pointers
  in this model because we anticipate
  that higher-level linguistic subsystems in Sermo
  will dictate the features of the syllable
  that must be contained
  in the syllable semantic pointer representation.}

??? Do some fine tuning?

??? Should we do a sequence of syllables?
maybe, because it's already done

??? Make a note that this section was done
in collaboration with Bernd

\subsection{Network diagram}

\subsection{Evaluation}

??? compare original VTG -> audio to Nengo -> VTG -> audio

\section{Syllable recognition}

Go from a vocal tract gesture score
to a syllable semantic pointer, in neurons.

\subsection{Network diagram}

\subsection{Evaluation}

\section{Syllable consolidation}

Go from vocal tract gesture score
to audio signal, in neurons.

Learn a syllable recognition
and a syllable production
network from this.

Contrast this to syllable production because it's
an infrequently voiced syllable.

\subsection{Bootstrapped syllable learning}

??? A full picture of speech development involves:
learning vocal tract gestures
through reinforced motor babbling,
learning basic syllables
through mimicry?,
and scaling up to the elements of this model.
While we think that our model
is a useful starting point for such
a developmental model,
as it provides an end-target,
we don't claim to do this kind of structural learning.
However, we believe that error-based learning
could result in this kind of system;
to show that this is a possibility,
we consider a minimal learning situation:
learning voice a new syllable
given a set of existing syllables.
This type of thing probably happens
in second language acquisition,
when novel combinations of phonemes
are encountered,
or even during first language acquisition
as pronunciation is refined over
the course of one's life;
words that were once awkward combinations
of many syllables are compressed into
nearly equivalent sequences of fewer,
more complex syllables.

As opposed to conversational shadowing,
which highlights the high-level strengths
of the integrated speech system,
syllable learning highlights
the low-level strengths of this system.
Syllable learning involves
learning a novel set of gestures
and associated articulator trajectories
that will voice a syllable
that is encountered for the first time.

We call our syllable learning system ``bootstrapped''
because we assume that our system
has an existing repertoire of syllables
that it is already able to voice.
These existing syllables will be
used in learning the new syllable.
Bootstrapped syllable learning contrasts with
the type of syllable learning
done as an infant and toddler,
which uses reinforced speech babbling
to learn novel syllables.
While we believe that learning syllables
from babbling is an important research direction
that can be explored in this system,
it has also been explored in many other systems
(???cite Diva etc),
and so we have chosen to leave this type of learning
for future work.

The bootstrapped syllable learning system
learns new syllables in three steps.

\begin{enumerate}
\item Initialize the new syllable from the most
  similar existing syllable.
  For example, when learning to voice
  the syllable /ba/, the system should
  start from the syllable /fa/ if it is known.
\item Swap compatible speech gestures.
  For example, vowel producing gestures
  would be compatible, allowing for modifying
  a /ba/ to a /bu/, and so on.
  The choice of which gesture to swap and
  how to swap it will be informed by
  ???figure out.
\item Fine-tune the voiced syllable
  until it can be recognized as the
  syllable to be learned.
  ???more
\end{enumerate}

??? NB: the oscillator being learned should exist outside of
the normal speech system so that they can both run
at the same time, but there should be a switch kind of thing
to put the new syllable through.

??? hypothesis (not sure where to put this):
the mapping between phoneme to gesture
is such that is not advantageous
to represent in the synthesis
(maybe also recognition?) system(s).
Therefore, it may be the case that
not all people have phoneme representations.
However, we propose that phonemes
are a useful construct for learning to voice
novel syllables in a second-language learning situation.

These steps require several systems
that have already been implemented
in the recognition and synthesis systems separately;
for example, the ability to compare
voiced syllables to those already known
is one of the primary goals of the recognition system itself,
so it can be leveraged when trying to learn new syllables.
However, these steps also point to new systems
that must be implemented.
First, the system requires a method
to transfer a syllable-producing function
from one ensemble to another.
Second, some knowledge of which gestures
are compatible must be built into the system.
Finally, ???fine-tuning.

It has been shown that in second-language learning,
slowing down the syllable can have
a significant increase in learning effectiveness
???cite.
We believe that our system emulates
how a native speaker of one language
would learn to voice novel syllables in a second language.
Therefore, we predict that slowing down
the speed at which the syllables are heard
and uttered will improve ???learning speed
and / or quality of learned syllable.

??? Mental syllabary (1-s2.0-S009394X...pdf)
supports our architecture

\subsection{Network diagram}

\subsection{Evaluation}
