\chapter{Methods}

Natural speech is not a series of phonemes
recited as clearly as possible
to maximize intelligibility.
Natural speech includes variation in
pitch and volume in order to transfer
non-linguistic information between speakers.
Current automated speech systems
focus solely on the linguistic content
of speech;
by deciphering and reproducing both
linguistic and non-linguistic content,
we aim to produce more natural speech
recognition and production
than the existing state of the art.

??? make a little checklist here of what our end goal
is to motivate the four sections we're looking at

\section{Distributed representations in neural networks}

Issue: localist ANN representations. Solution: NEF

NEF stuff; emphasize that unlike other models,
here representations are distributed,
which makes transformation harder,
so we have that.
Then also as a bonus we get dynamics.

\section{Symbol-like processing with real-valued vectors}

Issue: there are a finite number of syllables
(especially frequent syllables) but there are an infinite
number of syllable strings (e.g., words, sentences).
Need some way to represent temporal information
with discrete symbols.
Solution: SPA

The previous section gives us a method to represent
and manipulate ANN activations (i.e., numerical vectors)
with distributed, possibly spiking, neurons.

??? make links to knowledge representation in CS

??? give general background for the math in methods?

\section{Biologically plausible online learning}

Issue: NEF gives a way to solve for connection weights,
but what about functions we don't apriori,
or learning situations?
Solution: PES

PES for syllable learning

\section{Biologically implausible offline learning}

Issue: PES should be able to learn everything,
but takes a very long time to do so.
Solution: RNNs with GRU (or LSTM, but GRU seems good for now)

Ideally, a method to do an analogous thing with PES
will be possible, but that's for future work.

%%% ??? move rest to implementation section?

\section{Recognition system}

The recognition system is composed of
three layers connected in a feed-forward manner.

??? Note: I think we'll change from phonemes to vocal tract gestures,
for this model. It's possible to learn phonemes and / or words,
but that might not make sense for this particular system;
the ventral stream, for example, might use words,
but for us, gestures make more sense.
See nihms521633.pdf for another account of why phonemes
aren't necessarily the best thing.

???come up with a name maybe

???figure of whole system

\begin{itemize}
\item \textbf{Auditory periphery.} The auditory periphery layer
  takes incoming air pressure waves and converts them
  to a frequency representation,
  mimicking the function of the human ear.
\item \textbf{Auditory preprocessing.} The auditory preprocessing layer
  represents the frequency information from the auditory periphery,
  and through recurrent connections within the layer,
  also represents temporal dynamics of frequency information
  (e.g., time derivatives).
\item \textbf{Features.} The feature layer
  represents speech-relevant features;
  in this work, we represent phonemes, pitch, and volume.
  Features are computed from information
  in the auditory preprocessing layer.
\end{itemize}

This system is similar to many previous systems,
and aside from its feedforward nature,
follows naturally from the organization
of the human speech system (???ref or ???section).
The primary way in which this system
deviates from previous models
is by imposing biological constraints.
Specifically, the system operates
in continuous time,
and only uses locally available information.
Adhering to these constraints differentiates
this system from similar systems,
such as those based on deep learning (???refsection),
in which... ???more, it's discrete
This system is also ???something about NEF

\subsection{Auditory periphery}

???figure of periphery

As summarized in secition ???ref,
there are many models of the auditory periphery
that differ in how many phenomena they capture,
and in their computational costs.
For this system,
the primary selection criteria is
the computational cost;
periphery models that simulate quickly
allow for rapid iterative development
to determine the conditions
under which the model is successful
under which it fails.
We will therefore use
a bank of Gammatone filters as the
auditory periphery model,
as it captures the primary phenomena
of the auditory periphery
and incurs the least computational cost
of the models surveyed.

???talk about the various phenomena
and why it isn't important for this model

Unlike some of the other layers, however,
this layer is modular.
All auditory periphery models expose
generally the same interface;
the input is air pressure waves,
and the output is either action potentials
traveling down the auditory nerve,
or basilar membrane deflections
which can be converted to action potentials
using various spike generation mechanisms.
Should we determine that Gammatone filters
are not sufficient for extracting
the features that we aim to collect,
then we can try other auditory periphery models
with minimal changes to the rest of the system.
In doing so, we would also be able to
determine functional roles for the
unique phenomena captured by the
periphery model that is able to extract particular features.
However, we believe that many
of the phenomena captured
by sophisticated periphery models either
\begin{itemize}
\item replicate unnecessary biological detail
  (i.e., details that come about because biological parts
  are necessarily noisy and imperfect), or
\item become necessary in more complicated
  auditory environments than the ones
  we will use for testing in this work.
\end{itemize}

???question: do we take the envelope here,
or is that later?

\subsection{Auditory preprocessing}

The auditory preprocessing layer
takes frequency information from the auditory periphery,
and computes generic temporal transformations.
Each downstream feature will use a different
subset of these transformations.
For efficiency,
only transformations used by at least one
downstream feature will be
included in the final integrated system,
but for testing the auditory system independently,
many transformations will be computed
to determine the critical set of
transformations needed by each downstream feature.

It is important to be explicit about
why the auditory preprocessing layer is necessary,
as it exemplifies the constraints involved in
using biologically inspired methods.
We use temporal transformations
so that all of the information necessary
to determine the phoneme currently being voiced,
or to determine any other downstream feature,
is available at the current moment in time.
Humans can be trained to look at
audio spectrograms and decipher
the utterance based on how the
frequency information changes over time.
Many speech recognition systems operate
in a similar way, by looking at
???millisecond slices of time
and decoding phonemes with that portion
of the spectrogram ???refs and specifics.
In contrast, humans receive audio
as a constant stream,
using internal processes to remember
parts of the recent audio past
in order to decode linguistic
and non-linguistic information.
Our system also operates in
an online real-time fashion,
receiving a constant stream of audio input,
and using internal processes
to keep around the information
needed to decode speech.
The audio preprocessing layer
is the primary method in which
raw audio information from the recent past
is made available at the current time.

These transformations are expressed as
linear time-invariant filters.
These filters are applied to the incoming
frequency information.
???more about LTIs

???rundown of what transformations are done

???try to have a citation for each transformation

???include equations

A central question that we will explore
is whether features can be decoded
from temporal transformations
of each frequency,
or it is necessary for nearby frequencies
to interact in some transformations.
For example,
equation~???eqref calculates
the derivative of a signal;
i.e., it tells us how the power associated
with a particular frequency has changed recently.
We could instead look at the derivative
of the difference between two frequencies.
???put in equation.
This equation tells us how the relative
power between two frequencies
has changed recently.
???someref indicates that
correctly identifying bigrams
is as essential to phonology
as correctly identifying individual phonemes.
Temporal transformations that

This layer does not contain all possible
temporal transformations;
there are an infinite number of filters
that could be applied to the incoming
frequency information.
The filters above were chosen
because they appear in the literature.
However, additional transformations
can be added if they are needed
by other downstream features.
Other LTI filters could also be added
systematically to discover useful filters
that might form predictions of
filters we might find in the brain.

While the ability to add new LTI filters
does not make the preprocessing layer
as modular and interchangeable
as the auditory periphery,
the procedure for adding
new temporal transformations
is straightforward,
making the preprocessing layer
generic and extensible.


??? Modulation filterbank: implement, talk about

\subsection{Features}

The feature layer takes temporally transformed
frequency information from the preprocessing layer
and represents high-level features relevant
to human speech.
Specifically, we represent phonemes,
pitch, and volume in an attempt
to capture the features that are
psychoacoustically probable
and sufficient to reproduce
natural speech through the synthesis system.

Unlike the preprocessing layer,
the transformations in the feature layer
are computed across feedforward connections.
The exact transformation is unique to each feature.

Phonemes are the most complicated feature
that we represent.
We track
the current vocalic and consonantal phoneme separately
because these two types of phonemes
have very different acoustic properties.
Vowels are produced with an open vocal tract,
and are typically voiced longer than consonants.
For these reasons, the temporal information
necessary to recognize vowels
may be readily available at
the current moment;
however, using temporal information relatively
distant in the past (e.g., ???20-30ms)
may result in better performance since we can
leverage the fact that vowels are voiced for longer.
Another question that we can investigate in this model
is whether diphthongs should be considered phonemes
or a string of two phonemes.
Diphthongs may be easier to recognize as a single unit
by exploiting the temporal information available
in the system;
on the other hand, if the individual components
of the diphthong are easy to recognize,
then there is no need to treat them as a single unit.

Consonants, on the other hand, are produced
through constrictions of the vocal tract,
resulting in short disturbances or stoppages in air flow
in between two vocalic contexts.
Temporal information is likely to be necessary
in the case of consonants
(i.e., the current moment's frequency information
is unlikely to be sufficient),
and the temporal information of interest
is in the recent past (e.g., ???5-10ms).
It also worth noting that ???almost
all languages have more consonants than vowels,
as there are more possible places to constrict
the vocal tract than there are acoustically
differentiable shapes for an open vocal tract.
Because of this, we are likely to need
more neural resources to decode consonants
than vowels
(either purely in terms of the number of neurons,
or in the amount of temporal information conveyed
by the neurons).

Unlike generic LTI systems and other features,
there is unlikely to be a concise mathematical equation
describing how a vowel or consonant is to be recognized;
the true function is likely to be too complicated,
and may differ across people depending on their experience.
Instead, for phoneme recognition
we construct a series of labeled input-output pairs,
and approximate the true function
through a least-squares minimization procedure
(see ???NEF section).
We first test the system
with a corpus of synthesized speech,
which has less variability and should therefore
be easier to decode.
We then test the system
with a corpus of natural speech
???deets.

% synthesized: http://festvox.org/dbs/
% natural...: http://opendata.stackexchange.com/questions/1327/speech-audio-databases-with-phonemes-labelled

Currently, we are using two discrete systems
for vowels and consonants.
It is not clear whether
semivowels, semiconsonants, and glides
should be considered vowels or consonants,
or whether we need another system
for detecting these phonemes.
Alternatively, it could be the case
that there is only one monolithic system
that takes in a large amount of temporal information
and can decode any phoneme.
We do not address this issue here,
and suggest it as an interesting future extension.

Aside from phonemes, we also represent pitch and volume.
Neither of these features are useful
as an absolute quantity---most humans are poor
at judging absolute pitch and volume ???cite---so
we aim to represent relative pitch and volume.
In both of these cases,
we must determine a baseline pitch or volume,
and a method for comparing the current
pitch or volume to the baseline.

In speech, baseline pitch is primarily determined
by speaker identity.
Each speaker has a baseline pitch,
determined in part by the shape of their vocal folds,
so it is likely that we learn
and remember the baseline pitch
of speakers that we interact with frequently.
On the other hand,
changes in baseline pitch
(e.g., through illnesses affecting the vocal tract)
require little to no adaptation period,
so the mechanism through which we determine
a speaker's baseline pitch
is likely to be relatively simple
and flexible.
In this work,
we will not posit a neural mechanism
for determining baseline pitch,
and will instead compute it
from the data offline and provide it as input.

Given the baseline pitch as input,
relative pitch will be computed
primarily through neural inhibition.
See section ???implementation
for how this is accomplished.

Unlike pitch, baseline volume
is not speaker specific;
it is easy to notice when a speaker
talks louder or quieter
than the norm.
???baseline is the overall activity
of a long-timescale filtered version
of the current moment's power spectrum?
???relative volume is that minus a
short-timescale filtered version?
So it's basically just a derivative?
???not sure if we even care about volume
to be honest...
% http://www.sengpielaudio.com/calculator-loudness.htm

We believe that these three features
are sufficient to reproduce natural speech,
including prosodic features implicitly
by virtue of tracking these three features
over the timescale of an utterance.
Many aspects of prosody,
such as the rhythm of an utterance,
can be reproduced by rerunning
the three feature trajectories through time.
However, reproducing prosodic features
does not mean that we are explicitly
representing them.
In more sophisticated systems
connecting with linguistic models,
these prosodic features would
be represented explicitly
and would therefore need to be
determined based on temporal transformation
from the preprocessing layer,
and also from temporal transformations
of the three features represented here.
These prosodic features,
which would be essential for determining
important context clues
like the emotional state of the speaker,
would therefore be represented in
a fourth layer, receiving input from
temporally transformed versions of
the features from the third layer.
These temporal transformations
could be computed with LIT filters
in the same way that the preprocessing layer
transforms frequency information.
However, as our goal is to
extract only the features necessary
to reproduce natural speech,
we leave the decoding of high-level prosodic features
as future work.

??? Possible thing to try: keep track of a global
speed; in general, most people talk at the same pace,
but you can tell if they are speaking in a
marked fast or slow pace.

\subsection{The role of feedback}

The above system is primarily feed-forward
(though feedback is used extensively in the
auditory preprocessing layer).
It is important to note that
a complete auditory processing system
would match the brain (???refs) and have
feedback connections between all layers
in order to refine each layer's ability
to provide useful output downstream layers.
(??? more stuff from refs)

Feedback is an undeniably important
component of a full system that operates
for long periods of time with sensors
that are constantly changing and degrading.
In this work, however, we assume
that the auditory periphery does not change
its ability to process sounds over a long timescale,
and therefore we do not include corrective feedback
signals between layers in the auditory system.
This type of feedback could be added
to this system in future work.

\subsection{Evaluation}

??? Functional evaluation: can we decode downstream features?
How well do we do compared to other systems?
We don't have to do as well as them,
but we have to do well enough that it's not ridiculous.

??? for phonemes: our gold standard is a set of labels
over time with the proper phoneme.
If the system categorizes to that label in that period,
then it's correct. Or maybe what's more important
is that the phonemes are all in the same order,
and we also have some kind of timing information?

??? Neural evaluation: One thing that's commonly measured
in auditory neuroscience is the spectro-temporal receptive field
of auditory neurons. We can make these for our downstream feature
neurons and compare them to published STRFs and see if they look similar.
Note that it would be easy to make neurons that can replicate STRFs,
as we would just use the temporal transformations that matter.
But, we make our choice of transformations based on what would
be best to decode certain features;
being able to also replicate STRFs gives an indication that
we made good choices, and that this system is engineered
in a similar way to how the system developed and evolved (maybe?)

??? should we talk about the experiments from the results section here?

??? Nonsense CVCs: cite Allen 94;

``Typical ASR systems start with a `front-end' that transforms
the speech signal into a `feature vector' which is then processed
by a `back-end' classifier. These systems frequently place a
heavy emphasis on word and language models as a method of
increasing the recognition scores.''

``Because of confusion and misunderstanding based on
coarticulation arguments, only a small amount of research
has been done on the automatic recognition of nonsense CVCs.
From the work of Fletcher and Steinberg, it should be clear
that the real challenge in machine recognition today is
human-like performance for phones and nonsense CVC
under conditions of typical channel distortions.
Since the human performance is well-known under
these conditions [15], [19], nonsense CVCs
represent an excellent database. Decreasing
the error rates for these elementary signals would have
a major impact on overall system performance and robustness.''

\section{Synthesis system}

The synthesis system translates
high-level features describing an utterance
into an audio waveform.
There are two components of the synthesis system:
an articulatory synthesizer and a neural control system.

??? figure of whole system

\begin{itemize}
\item \textbf{Articulatory synthesizer.} The articulatory synthesizer
  generates audio waveforms
  by simulating the airflow through a model of the human vocal tract.
  It consists of a vocal tract model,
  which converts control signals from the neural control system
  into vocal tract geometries,
  and an acoustic model,
  which converts vocal tract geometries into acoustic signals
  (i.e., audio waveforms).
\item \textbf{Neural control system.} The control system
  translates high-level features into control signals.
  While these control signals are specific to the
  synthesizer being controlled,
  the control system is designed to be as generic
  as possible such that it can be modified to control
  any articulatory synthesizer.
\end{itemize}

??? might want to make this more specific after writing
the background section

\subsection{Articulatory synthesizer}

As summarized in ???secref,
articulatory synthesizers are composed
of a vocal tract model and an acoustic model.
While state-of-the art synthesizers aim to
maximize speech intelligibility,
our primary focus is on control,
so computational efficiency
and interoperability are
key characteristics for choosing a synthesizer.

Interoperability refers to the ability
to incorporate the synthesizer
in the main simulation loop,
which in our case is driven by
the Nengo neural simulator
implemented in Python
(see ???nengosec).
Using the Nengo simulator places
two key interoperability restrictions.
First, the synthesizer must
be able to run ``online'';
that is, the synthesizer should maintain
some internal state which updates
based on control parameters
that are updated on each timestep.
Online synthesis contrasts with
batch synthesis, in which
the entire control parameter trajectory
must be known \textit{a priori}.
Second, the synthesizer must
be accessible through the Python
programming language in a performant manner.
While any synthesizer can communicate
with Python through some means
(e.g., foreign function interface
or operating system sockets),
some communication protocols incur
significant computational costs.

None of the synthesizers summarized in
section ???secref
stood out as the best candidate
in terms of efficiency and interoperability.
We also hypothesized that better knowledge
of synthesizer internals would
result in better control methods.
For these reasons,
we chose to implement our own
simple articulatory synthesizer focusing on
efficiency and interoperability with Nengo.

??? vocal tract model

??? acoustic model -- wait until later

\subsection{Neural control system}

??? Rewrite after we make a few sims

As summarized in ???secref,
the ???brain parts
are responsible for effecting
muscle movements that
vibrate the glottis,
deform the vocal tract,
and send air through it,
creating pressure waves
that we perceive as speech.
The neural control system
performs the same function as ???brain parts
in the synthesis system.
While we cannot be certain
what features the human brain
uses to generate control signals,
our model assumes that the
high level features of interest
are a string of phonemes,
a continuous relative pitch signal,
and a continuous relative volume signal.

??? At its core, the neural control system
deals with two types of mappings.
One type of mapping is a simple
input-output relationship
that can be described as a mathematical function,
similar to many of the mappings
done in the recognition system.
The second type of mapping is a temporal mapping.
Unlike the recognition system,
in which the goal was to
transform existing temporal information,
in the temporal mappings in the control system,
we aim to generate temporal information
from discrete inputs.
Essentially, we are mapping from
an input to a continuous sequence of outputs.
The output sequence has finite length,
but could repeat indefinitely.

??? In order to temporally control
the input string of phonemes,
it is initially represented
as a static symbol-like representation,
which we will call a $\text{WORD}$
even though not all strings of phonemes
will necessarily correspond
to a word in the vocabulary of a language.
The $\text{WORD}$ maps to a
sequence of phonemes,
which we will call
$\text{PHONE}(t)$,
where $t$ is time.

??? $$\text{WORD} \mapsto \text{PHONE}(t)$$

??? That is, for each word, we learn
what sequence of phonemes make up that word,
and when those phonemes begin.
Keeping track of when phonemes begin
is necessary because
each phoneme is also a temporal mapping,
from the phone to
a sequence of articulator movements
that will cause the articulatory synthesizer
to voice the target word.

??? $$\text{PHONE}(t) \mapsto \text{ART}(t)$$

??? The amount of time ...

??? or, maybe we should do this by
a function of WORD and current PHONE
that gives the next PHONE

??? we could just map directly from word
to articular movement -- that'd be easier.
But that implausible, you'd have to
essentially relearn how to voice each phone
for each word. that's not right, so we
have the intermediate step to make it
possible to share the phone -> art
function across all instances of that phone
in all words.

??? In the neural control system,
we use functions to map from
articular


??? The transitions that we currently do through
BG should be transferred to highly learned
cortical sequences eventually.
We might expect that the

\subsection{Evaluation}

\section{Integrated speech system}

Essentially, integrating the two systems
involves adapting the output of the recognition system
into a form that can be used by the synthesis system.
While both systems deal with the same features,
there are critical differences
which preclude a simple connection between the two.
???more

Important to note: they can't run simultaneously.
These cannot be hooked up in a supervised learning
situation where they feed into each other
directly because there are too many delays.
???more

??? things that need to be handled:
trajectory generation;
some kind of memory;
think more about this...

The integrated speech system
will be used for two purposes.

\begin{itemize}
\item \textbf{Conversational shadowing.} In conversational shadowing,
  a speaker repeats the words that a speech partner says.
  The system incorporates minor linguistic and prosodic transformations,
  as commonly occur in human conversation.
\item \textbf{Bootstrapped syllable learning.}
  The recognition and synthesis systems
  will be used in concert to learn how to voice a syllable
  that the synthesis system has no prior knowledge of.
\end{itemize}

\subsection{Conversational shadowing}

Conversation shadowing... ???brief intro.

While we use conversational shadowing
as a way to show that the recognition
and synthesis systems can interact
without requiring detailed linguistic structures
that are outside the scope of this thesis,
it is important to note that conversational shadowing
naturally occurs as people acquire their native language.
??? cite Murphey 2001, background

In our conversational shadowing system,
we implement two types of conversational shadowing.
The first is direct parroting, in which
the system attempts to reproduce
an incoming utterance as closely as possible.
The second is question inversion, in which
the system repeats the incoming utterance,
but poses incoming statements as questions,
and responds to incoming questions as statements.
While question inversion is primarily
a prosodic change,
we also make simple syllable substitutions
in order to show that these substitutions are possible,
and to make the system's utterances more natural.
For example, we replace instances of the syllables
/??i'm/ at the beginning of an utterance
with the syllables /???you're/,
such that utterances like
``I'm going to the store''
generate the response
``you're going to the store?''

Both of these are forms of complete shadowing;
selective forms of shadowing require linguistic knowledge
to pick out relevant portions of an utterance,
so we do not implement them here,
though we believe that they
do not require any modifications
to the recognition and synthesis systems.

The primary goal of this system is to show that
the encoding schemes used by
the recognition and synthesis systems
are both compatible and flexible.
Compatibility means that it is possible
to send the encodings used in the recognition system
to the synthesis system such that
recognized utterances can be voiced.
Compatibility is a minimum requirement
for any integrated speech system.
Flexibility means that it is possible
to transform the encodings
in the connection between
the recognition and synthesis systems
such that recognized utterances
can be responded to with different utterances.
Humans use the extreme case of flexibility
in all conversations;
incoming utterances are processed
and completely different utterances
are voiced as a result.
In our case, we use a reduced form
of flexibility such that the incoming utterance
is transformed in a natural way
but contains similar linguistic content
as the incoming utterance.

We believe that our semantic pointer encodings
allow for far greater flexibility
than existing integrated systems.
???more

???maybe an example of each type of shadowing

\subsection{Bootstrapped syllable learning}

??? A full picture of speech development involves:
learning vocal tract gestures
through reinforced motor babbling,
learning basic syllables
through mimicry?,
and scaling up to the elements of this model.
While we think that our model
is a useful starting point for such
a developmental model,
as it provides an end-target,
we don't claim to do this kind of structural learning.
However, we believe that error-based learning
could result in this kind of system;
to show that this is a possibility,
we consider a minimal learning situation:
learning voice a new syllable
given a set of existing syllables.
This type of thing probably happens
in second language acquisition,
when novel combinations of phonemes
are encountered,
or even during first language acquisition
as pronunciation is refined over
the course of one's life;
words that were once awkward combinations
of many syllables are compressed into
nearly equivalent sequences of fewer,
more complex syllables.

As opposed to conversational shadowing,
which highlights the high-level strengths
of the integrated speech system,
syllable learning highlights
the low-level strengths of this system.
Syllable learning involves
learning a novel set of gestures
and associated articulator trajectories
that will voice a syllable
that is encountered for the first time.

We call our syllable learning system ``bootstrapped''
because we assume that our system
has an existing repertoire of syllables
that it is already able to voice.
These existing syllables will be
used in learning the new syllable.
Bootstrapped syllable learning contrasts with
the type of syllable learning
done as an infant and toddler,
which uses reinforced speech babbling
to learn novel syllables.
While we believe that learning syllables
from babbling is an important research direction
that can be explored in this system,
it has also been explored in many other systems
(???cite Diva etc),
and so we have chosen to leave this type of learning
for future work.

The bootstrapped syllable learning system
learns new syllables in three steps.

\begin{enumerate}
\item Initialize the new syllable from the most
  similar existing syllable.
  For example, when learning to voice
  the syllable /ba/, the system should
  start from the syllable /fa/ if it is known.
\item Swap compatible speech gestures.
  For example, vowel producing gestures
  would be compatible, allowing for modifying
  a /ba/ to a /bu/, and so on.
  The choice of which gesture to swap and
  how to swap it will be informed by
  ???figure out.
\item Fine-tune the voiced syllable
  until it can be recognized as the
  syllable to be learned.
  ???more
\end{enumerate}

??? NB: the oscillator being learned should exist outside of
the normal speech system so that they can both run
at the same time, but there should be a switch kind of thing
to put the new syllable through.

??? hypothesis (not sure where to put this):
the mapping between phoneme to gesture
is such that is not advantageous
to represent in the synthesis
(maybe also recognition?) system(s).
Therefore, it may be the case that
not all people have phoneme representations.
However, we propose that phonemes
are a useful construct for learning to voice
novel syllables in a second-language learning situation.

These steps require several systems
that have already been implemented
in the recognition and synthesis systems separately;
for example, the ability to compare
voiced syllables to those already known
is one of the primary goals of the recognition system itself,
so it can be leveraged when trying to learn new syllables.
However, these steps also point to new systems
that must be implemented.
First, the system requires a method
to transfer a syllable-producing function
from one ensemble to another.
Second, some knowledge of which gestures
are compatible must be built into the system.
Finally, ???fine-tuning.

It has been shown that in second-language learning,
slowing down the syllable can have
a significant increase in learning effectiveness
???cite.
We believe that our system emulates
how a native speaker of one language
would learn to voice novel syllables in a second language.
Therefore, we predict that slowing down
the speed at which the syllables are heard
and uttered will improve ???learning speed
and / or quality of learned syllable.

??? Mental syllabary (1-s2.0-S009394X...pdf)
supports our architecture

\subsection{Evaluation}
