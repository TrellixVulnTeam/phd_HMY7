\chapter{Conceptual model}

The long term goal of the research
presented in this thesis
is a complete neurally realized model
of the human speech system,
which is able to maintain
natural conversations.
Implementing such a system
requires the collaborative efforts
of many domain experts.
In this chapter, we present
a conceptual model of the human speech system
that we believe can be implemented
in biologically plausible spiking neural networks.

The model, dubbed Sermo
(\textul{S}peech \textul{e}xecution and \textul{r}ecognition
\textul{m}odel \textul{o}rganism),
is a synthesis of the background material
summarized in the previous chapter.
The goal of Sermo
is to break the larger
intractable problem of human speech
into concrete subproblems
that have existing partial solutions,
or provide a reasonable challenge
for machine learning
and computational neuroscience researchers.
Pragmatically for this thesis,
Sermo provides context for
the subproblems for which
we have implemented concrete solutions
in subsequent chapters.

\section{Sermo description}

??? main figure

Sermo is split into five interacting systems,
as summarized in Figure~???.
\begin{itemize}
  \item \textbf{Auditory feature extraction}
    converts incoming audio waveforms into
    features suitable for linguistic processing
    and sensorimotor integration.
    This system is analogous to the frontend
    of an ASR system,
    and to the auditory periphery and
    early auditory pathways in the human speech system.
  \item \textbf{Non-auditory sensory modalities}
    gather relevant information
    from non-auditory sources
    to inform or correct other systems.
  \item \textbf{Sensorimotor integration}
    uses sensory information to
    recognize and predict the motor intentions
    of speakers, including one's self.
    This system is analogous to the
    dorsal stream in the human speech system.
  \item \textbf{Linguistic processing}
    converts sensory and sensorimotor information
    into sublexical and lexical representations
    suitable for high level linguistic
    and cognitive reasoning.
    This system is anologous to
    the language model in an ASR system,
    and to the ventral stream
    in the human speech system.
  \item \textbf{Speech motor control}
    uses lexical and sensorimotor information
    to generate motor commands
    that drive a simulated vocal tract model,
    which produces audio waveforms.
\end{itemize}

Further details on each of these
systems follow in the subsequent sections.

In addition to the overall organization
of subsystems and how they interact,
we also impose the following constraints
on Sermo in order to make explicit
that it is modeled after a biological system,
and should be able to interact
with biological systems naturally.

\begin{itemize}
  \item Subsystems must operate in a continuous, online fashion.
  \item Subsystems must be implementable in biologically plausible
    spiking neurons.
  \item Learning rules may only use local error information,
    and must compute error signals with spiking neurons.
\end{itemize}

Another way to summarize these constraints
is to emphasize that Sermo
is a mechanistic model
of the human speech system.
We are not only concerned with
classifying and generating speech
with human-like accuracy,
we also wish to gain insight
into what information is required
to classify and generate speech.

Solutions to subproblems that do not
meet these criteria
(e.g., statistical models)
are still a crucial component
of progress in this model.
Some parts of the model are
necessarily statistical,
and the information being transformed
may not be easily explained
by a label like ``lexical representation.''
In these situations,
however, it is still a critical research activity
to adapt statistical models that work in
discrete time, with rate based neuron models,
or with idealized learning rules like backpropagation
to meet the above criteria,
for the following reasons.

\begin{itemize}
  \item Models meeting these criteria can be compared directly
    to experimental data at all levels,
    from single unit activity recordings to behavior.
  \item Models meeting these criteria can be implemented
    in neuromorphic hardware.
\end{itemize}

Direct experimental comparisons enable
iterative development of the model.
As Sermo subsystems are implemented,
they will be used to make testable predictions.
As these predictions are tested,
the model can be invalidated
or updated to incorporate new empirical results.

Neuromorphic hardware
presents a plausible avenue
to real time interaction
between humans and Sermo instances.
Many of the subsystems already implemented
run at many times slower than real time,
despite the use of recent general purpose
CPUs and GPUs,
and reasonably efficient algorithms.
While incremental advances
in these technologies may
bring subsystems closer to real time,
a fully integrated system
with all subsystems interacting online
is intractable for the foreseeable future.
Neuromorphic systems that are designed
to simulate millions of spiking neuron models
in real time
are currently under development
(e.g., ??? cites),
and may make a full Sermo instance
possible in the next few decades.

A final note about
the general architecture of Sermo
and its subsystems
is that we will focus on a view
of Sermo as it would be used
in a conversational setting
that does not require learning.
We treat each subsystem as a separate
module that is defined primarily
by the input it expects,
and the output it provides;
however, in development and other forms
of learning,
there would be a proliferation of
interconnections between modules
providing error information.
Detailing these connections
in each subsystem and across subsystems
would be an important area
for further developing the model,
but we believe that presenting
a static endpoint for learning
is important as a first step
before considering
how that endpoint might come about.
Regardless,
as learning is ubiquitous in speech,
we provide a model that learns
through synaptic plasticity
in Section~??? as
a proof of concept that
the subsystems in Sermo
can be learned if appropriate
error information is computed
as part of the model.

\subsection{Auditory feature extraction}

??? Add input / output box, similar to Implementation probbox?

The auditory feature extraction system
preprocesses audio waveforms
in order to support
linguistic and sensorimotor analysis.
As has been shown through
the gap between
automatic speech recognition
and human speech comprehension,
the temporal characteristics
of speech
make these analysis problems
difficult to solve
with traditional computers.
Digital computation is precise and discrete;
human speech is variable and continuous.
We hypothesize that the
parallel and distributed nature
of biological computation
is central to its
ability to process temporal information.

Sermo defines three subproblems
that must be solved
by the auditory feature extraction system:
\textit{spectral analysis},
\textit{decorrelation},
and \textit{temporal transformation}.
In spectral analysis,
the incoming auditory signal
is projected into frequency space
by analyzing the spectral density
of the signal in the recent past.
The spectral density of the signal
is expected to have high
correlation between nearby frequencies.
Generally, these correlations
make feature extraction more difficult,
so the decorrelation step
projects the spectral density
onto a set of basis functions
that are more orthogonal;
typically, fewer basis functions
are used than frequency components.
Finally, while the information
in the spectral density
and decorrelated vector
contains information from
a window of time defined by
the spectral analysis step,
the temporal characteristic of speech
may require that information
from further in the past is available
at the current time.
Temporal transformations
maintain past information
to filter spectral
and decorrelated spectral information
for better linguistic
and sensorimotor information decoding.

\subsubsection{Spectral analysis}

Sermo theorizes that spectral analysis
is performed by a model
of the auditory periphery.
An auditory periphery model
emulates the human ear
up to the signals
traveling down the auditory nerve.

There are many existing
models of the auditory periphery
that meet the constraints
imposed by Sermo (see Section~???).
Each auditory perhiphery model
captures certain psychoacoustic phenomena
at a particular computational cost.
In general, these two quantities
are often traded off;
the more accurately the ear is modeled,
the most costly the model is to simulate.

It is difficult to assert \textit{a priori}
which psychoacoustical phenomena
are important for speech.
It would be reasonable to assume that
the majority of what the ear does
is important for an individual's survival;
however, we process many types of sound,
not just speech.
An ideal auditory filter model
for the spectral analysis subsystem
in Sermo
would capture all of the phenomena
that are important for speech,
but no additional phenomena
that increase the computational cost
of the model.

\subsubsection{Decorrelation}

Decorrelation techniques like
linear filters and the discrete cosine transform
are widely used in digital computing
for compression ??? cite DCTTR802.pdf.
In compression, a signal is projected
onto a set of basis functions;
only the coefficients are stored,
as an approximation of the original signal
can be recovered later by linearly combining
the basis functions weighted by those coefficients.
A similar argument can be made
to explain why automatic speech recognition systems
often use a decorrelation technique
at the frontend.
By operating on the coefficients of a set
of basis functions,
the set of parameters that must be optimized
by the backend is reduced.

While this line of reasoning
does not guarantee that
the brain also explicitly decorrelates
the frequency representation
provided by the auditory periphery,
it is clear that biological systems
aim to minimize energy expenditure whenever possible.
Learning through synaptic plasticity
takes more energy than does
typical signal transmission.
Since much of the speech system
is developed and refined
through synaptic plasticity,
using decorrelated representations
as early in the speech system as possible
may reduce the brain's energy expenditure
over one's lifetime.
Therefore, we hypothesize
that a system implemented
with a biologically plausible substrate,
as Sermo is,
decorrelates the frequency representation
provided by the auditory periphery.

\subsubsection{Temporal transformation}

The extent to which the recent past
is used to recognize speech
at the current moment
is of crucial importance,
yet is not often studied systematically.
In the brain, spectrotemporal receptive fields
have been measured using
tone ramps and other temporally varying signals,
revealing neurons that appear to be active
hundreds of milliseconds
after activity at
the cell's characteristic frequency,
but it is not clear if those
neurons are involved in speech,
nor is it clear what transformation they are implementing.
In automatic speech recognition,
the length of the audio frame
and how much it increments on each timestep
is often chosen arbitrarily,
or uses values known to work.

Until recently,
the most successful ASR systems
were based on Hidden Markov Models (HMMs),
which are based on the Markov assumption,
which that is that only the information
from the previous state is required
to reason about the current state.
Despite the fact that speech is highly temporally correlated,
these systems were relatively successful.
However, a great deal of engineering
went into the application of HMMs
to speech, primarily because of
its temporal nature
(see e.g., ??? Rabiner tutorial,
some other cite).
One such improvement for HMM-based ASR systems
is to include the first and second temporal derivatives
in the state representation.
Current state-of-the-art systems
are based on hierarchies of bidirectionally connected
recurrent neural networks (RNNs),
which are able to maintain information about
previous states through recurrent connections.
It is reasonable to assume that
a more flexible representation
of the recent past
is part of why these networks
fare better than HMM-based systems
which have had decades of incremental improvements.

Further study into how much temporal information
is maintained by deep RNNs,
and how that temporal information is transformed
would benefit Sermo
and other speech systems.
In particular, expressing the deep RNN's
temporal transformation as a
linear time-invariant (LTI) filter
would be of particular interest,
as LTI filters can be efficiently implemented
with spiking neural networks
(see ??? cite Aaron's stuff).
Alternatively, LTI filters
commonly used in engineering applications
can be experimented with
to investigate whether decoding accuracy
is improved in downstream
linguistic and sensorimotor systems.

The above system is primarily feed-forward
(though feedback is used extensively in the
auditory preprocessing layer).
It is important to note that
a complete auditory processing system
would match the brain (???refs) and have
feedback connections between all layers
in order to refine each layer's ability
to provide useful output downstream layers.
(??? more stuff from refs)

Feedback is an undeniably important
component of a full system that operates
for long periods of time with sensors
that are constantly changing and degrading.
In this work, however, we assume
that the auditory periphery does not change
its ability to process sounds over a long timescale,
and therefore we do not include corrective feedback
signals between layers in the auditory system.
This type of feedback could be added
to this system in future work.

\subsubsection{Evaluation}

Progress in the auditory feature extraction system
is difficult to measure
because it is most useful
in the context of other systems.
It is not readily apparent
what impact
an improved decorrelation network
might have on the sensorimotor integration
system's ability to decode production information,
for example.
Although we can assume that
an output representation with
less autocorrelation
will be easier to decode,
the gains from this improvement
may be modest compared to
changing the time constants on temporal transformations,
or even increasing the number of neurons
used to represent certain pieces of information.

We believe that this difficulty
motivates the development
of large integrated systems
like Sermo
as it enables evaluation
through comparing candidates solutions
to different subproblems
in the context of
known best solutions
to subproblems in the sensorimotor integration
and linguistic processing systems.
In other words,
one of the primary contributions of Sermo
is to propose standardized input and output formats
required by each subsystem.
We will show a proof of concept
that Sermo supports such comparisons
in Section~??? in which we compare
??? auditory periphery models
on a word classification task.

\subsection{Non-auditory sensory modalities}

Biological systems are remarkably adept
at integrating information
from different sensory modalities
in order to guide behavior.
In speech,
we use visual information
from other speakers to improve recognition,
and somatosensory information
from vocal tract articulators
to improve production.
A fully realized version of Sermo
would include these non-auditory sensory systems;
however, the currently implemented
subset of Sermo does not.

\subsection{Sensorimotor integration}

As summarized in Section~???,
the dorsal stream of the human speech system
represents both sensory and motor information
when recognizing and producing speech.
In Sermo,
we hypothesize that
the sensorimotor integration system
is responsible for decoding production information
from sensory information,
and maintaining two types of associations.
First, sensory input,
both auditory and non-auditory,
is used to \textit{decode the production information}
that generated the sensory input.
Second, that decoded production information
is associated with
static syllable representations;
we call this \textit{trajectory classification},
as each syllable requires
a time series of sensory features
on the order of hundreds of milliseconds
to be reliably classified.
Second, static syllable representations
are associated with
trajectories of speech production information;
we call this \textit{trajectory generation},
as a time series of vocal tract movements
are necessary to voice a syllable.

As in the auditory feature extraction system,
a great deal of effort in involved in
dealing with the temporal nature of speech.
Were the sensory and motor representations static,
the associations in the sensorimotor system
could be accomplished
with simple associative memories
(see ??? spaun).
Instead, we are effectively associating
sensory trajectories with motor trajectories.
The key insight
that enables these associations in Sermo
is to use an intermediate syllable representation
that can be transmitted between
brain areas and queried
for the full trajectory.
A convenient side effect of
the intermediate syllable representation
is that it also enables
top-down influence from
other system,
most notably the linguistic processing subsystem.
It is not reasonable to expect
linguistic areas to communicate
production information directly
to motor cortex;
however, it is reasonable to expect
that the lexical representations
in the linguistic system
can be queried for the syllables
that make up those lexical items.

Sermo makes the strong claim
that sensorimotor integration,
and therefore speech motor control,
is organized at the level of syllables.
While there are ??? reasons
to make this claim
??? cite something about mental syllabary?,
our motivation for using syllables
as the primary unit
of speech motor representation
is that we believe
it enables sufficiently flexible speech
with the lowest expenditure of neural resources.

Syllables enable flexible speech
because we can fine-tune the
trajectory generated by any
one syllable representation individually.
For example,
if Sermo can improve its ability
to voice the syllable ??? /ba/,
then the quality of all words
containing the syllable ??? /ba/
improves.
Had we chosen word representations,
then this transfer learning
would not be possible.
On the other hand,
has we chosen phoneme representations,
then it would be difficult
to fine tune trajectories at all,
as the motor trajectory of a phoneme
is dependent on the
previous and next phonemes;
we therefore could not fine tune
the trajectory of each phoneme individually.

Syllables minimize the expenditure of neural resources
because syllables are mostly independent of one another,
and because there are a finite number of syllables.
To illustrate why these reasons are important,
let us assume that a certain amount of
neural resources (neuron and synapses)
are associated with the basic unit of representation.
We will denote this as $n$,
and assume that this amount is the same
regardless of the representation used.
If we choose phonemes as the basic representation,
then we have a small number of basic units;
for the sake of argument,
say around 50 phonemes.
However, because the motor trajectory
of a phoneme is dependent on the
phonemes before and after it,
we need neural resources not only for
the basic unit, but in the worst case,
also for all possible
interactions of those basic units;
i.e., we would need $50^3n$ or $125,000n$ resources.
On the other hand, if we chose
words as the basic unit of representation,
we would not have to worry about
between word interactions,
but the number of words in a language
is much larger than the number of syllables
in a language.
Using estimates for English
from  ??? http://semarch.linguistics.fas.nyu.edu/barker/Syllables/index.txt
and ??? http://www.languagemonitor.com/number-of-words/number-of-words-in-the-english-language-1008879/,
word representations would use approximately
$1,025,109.8n$ resources,
compared to just $15,831n$ resources for syllables.
Syllable representations are clearly the most tractable
given these assumptions.
??? reminder: make sure we use this to argue
that this scales in the results section.

So far, we have not been explicit
about our representation of production information.
As discussed in Section~???,
we could use a discrete representation
of production information
(i.e., vocal tract gestures)
or a continuous representation
of production information
(i.e., vocal tract articulator positions).
Unlike in the choice of
using syllable representations,
the number of possible vocal tract gestures
and the number of possible articulator positions
is similar,
assuming a reasonable parameterization
of the vocal tract articulators
as is done in all articulatory synthesizers.
Therefore, we consider the choice
of production information representation
to be an empirical question
that we aim to answer
with the subset of Sermo that we implement
in this thesis.

\subsubsection{Trajectory generation}

In trajectory generation,
the static syllable representation
is used to generate
a time series of production information.

There are several criteria
that a trajectory generation system
must meet in order for it to be
useful in the sensorimotor integration system.
First, the system must be able to produce
trajectories with high degrees of freedom.
There are over twenty degrees of freedom,
whether we use vocal tract gestures
or articulator positions.
Note that these degrees of freedom
are independent,
so separate trajectory generators
can be employed for each degree of freedom,
but these generators must be
tightly synchronized as
consonantal constrictions can require
the temporal coordination
of several gestures or articulators.
Regardless, whether one large generator
or several smaller generators are used,
the neural resources required by each
must be reasonable,
as there may be many thousands
of these trajectory generators
in an adult speech system.

Second, the trajectory generators
must be flexible with respect to
the timescale at which
they generate a trajectory.
There is significant variability
in how we voice syllables.
During a stressful situation,
all of our syllables may be
voiced quickly;
syllables are often elongated
or shortened for emphasis.
A trajectory generator must be
able to produce appropriate
production information
at speeds that vary
between and within
trajectory generation trials.

Third, the trajectory generators
must be able to operate
in tight succession,
and possibly even overlap in time.
Speech is remarkably continuous;
to robustly segment speech into
syllables often requires
speech to be slowed down
or overenunciated.
Therefore, if our notion of production
requires representation at the level of syllables,
then the motor trajectories
resulting from those syllables
must be able to blend naturally into one another
without obvious pauses between
different syllables,
or even multiple utterances of the same syllable.

\subsubsection{Trajectory classification}

In trajectory classification,
we are essentially solving the inverse problem
of trajectory generation.
Given some time series of production information,
we must infer the static syllable representation
that would produce that trajectory.

As an inverse problem,
the trajectory classification system
has the same challenges
as the trajectory generation system.
The trajectories to be classified
will have relatively high degrees of freedom;
the trajectories may not advance uniformly
through time;
and subsequent syllables will follow
in quick succession,
even possibly overlapping.

\subsubsection{Decoding production information}

In the trajectory classification system,
we assume that the trajectories we are classifying
are trajectories of production information.
It may also be possible to directly classify
sensory information into syllables;
however, we follow the motor theory of speech recognition
which theorizes that production information
is directly inferred from auditory information.
One theoretical reason
why we assume that production information
is decoded from sensory information
is that it allows us
to frame trajectory generation and
classification as inverse problems
of each other.
In doing so, there may be ways
in which the classification and generation systems
can correct each other's errors.

A more practical justification
for decoding production information
is that it provides a site in which to
aggregate production information
gathered from multiple sensory modalities.
The McGurk effect discussed in Section~???
indicates that we use both auditory
and visual information to
determine which syllable is being voiced.
If part of our syllable classification system
if informed by a decoding of production information,
then both auditory and visual systems
can contribute to that decoding.
The speech development systems
embodied in DIVA and the Kr\"{o}ger model
also learn to associate sensory information
with one's own production information
through babbling and imitation stages.

Additionally,
it is possible to vocally imitate
novel syllables.
Novel syllables that are made up of sounds
(and therefore vocal tract gestures)
that are frequently encountered
in one's language can be
repeated readily.
Novel syllables that also feature
novel vocal tract gestures
are more difficult to repeat readily.
Since novel syllables do not have
a learned production information trajectory,
it is reasonable to hypothesize that
the syllable's production trajectory
is decoded from the speech information,
and passed through to the
speech motor control system unchanged.
Repeated repetitions of the syllable
would lead to a production trajectory
being learned and consolidated.
% ??? mention the model if I make it

??? has shown that it is possible to
do this decoding,
which some call auditory-articulatory inversion ???.
Currently, this has only been done
with continuous articulator positions;
determining if discrete vocal tract gestures
can be decoded from speech
would guide further development of Sermo.
We do not solve this problem
in this thesis;
however, we are able to use Sermo
to produce a corpus of time-locked
synthesized auditory information
and vocal tract gestures.
We will make this corpus available
to other researchers
to collaboratively determine
if this decoding process is possible.

\subsubsection{Evaluation}

While a version of Sermo
that explains development
would require many presentations
in order to generalize trajectories
associated with syllables,
the current system
is provided a desired trajectory
for each syllable.
Therefore, we can directly
test the sensorimotor integration system's
ability to classify
and generate known trajectories.
Classification if either done
correctly or incorrectly,
though some classification methods
may provide confidence information
that can be used to further evaluate the system.
Generation, on the other hand,
cannot be easily evaluated
as correct or incorrect.
One method of evaluation would be to
measure how much the generated trajectory
deviates from
(comparing, for example,
the squared jerk of the trajectories;
see ??? http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3470860/).
However, it is more important that
the generated trajectory
is able to produce the desired syllable.
Therefore, a more useful evaluation
of a trajectory generation system
would be to connect it to the
speech motor control system,
and have humans
perceptually evaluate
the generated audio.

\subsection{Linguistic processing}

As summarized in Section~???
the ventral stream of human speech system
represents high-level linguistic features of speech.
In Sermo, we hypothesize that
the linguistic processing system
decodes linguistically relevant information
like phonemes and words
from sensory and production information,
and provides syllable targets
to be voiced to the sensorimotor integration system.

Decoding phonemes and words from speech
(or, more accurately,
classifying phonemes and words from speech)
has been the primary topic of research
in ASR systems for decades.
As such, we are confident that
the methods used in ASR systems
can be implemented in a mechanistic model
suitable for incorporation with Sermo,
as has been done for similar
visual classification systems
(??? cite Hunsberger).
Once classified, lexical and sublexical items
will be used in higher level linguistic
systems that will associate
semantics (as in ??? blouw)
and syntax (as in ??? Stewart)
to these representations.
As these higher level systems
have been developed with
the Semantic Pointer Architecture (SPA),
we hypothesize that the linguistic processing system
produces lexical representations
in the form of semantic pointers.
However, we do not further constrain these representations
as constraints will come from the needs of
higher-level linguistic and cognitive systems,
which are not currently part of Sermo.

The output of these higher-level systems,
however, is constrained by the needs
of the sensorimotor system.
Specifically, Sermo requires that
the output of the
linguistic processing system
be in the form of static syllable representations.
Therefore, for the portion
of the linguistic system connected
to the sensorimotor system,
semantic pointers representing
sentences or words must contain
syllable representations that can be
queried through unbinding.
See Section~??? for more information
on unbinding in the SPA.

\subsection{Speech motor control}

The speech motor control system
uses a continuous trajectory
of speech production information
to drive an articulatory
speech synthesis system,
which produces audio waveforms.
It is composed of two components.
The first is the \textit{motor expansion} system,
which projects the speech production information
into the vector space that
directly controls the articulatory synthesizer.
The second is
the \textit{articulatory synthesizer} itself,
which includes a vocal tract model
and an acoustic model.

\subsubsection{Motor expansion}

To this point, we have discussed
production information abstractly,
either as vocal tract gestures
or vocal tract articulator positions.
In the neurobiological case,
there is only one possible
configuration for the vocal tract,
and so the production information
should be as close as possible to the
form required by the motor system.
While all vocal tracts are differently shaped,
the muscle groups controlling various articulators
should be identical in healthy humans.

In Sermo, however,
there is no perfect choice
for an articulatory synthesizer.
Indeed, it is a long-term goal of Sermo
to provide a unified control system
that can be used to productively
explore and test the quality
of articulatory synthesizers
by providing a control system
that can be used for any synthesizer,
if a motor expansion system
for that synthesizer can be created.
As such, the control signals
that each articulatory synthesizer
expects may differ significantly.
The motor expansion system
maps the production information
produced by the sensorimotor system
into the vector space that
the target articulatory synthesizer
uses as control signals.
It allows the rest of Sermo
to be designed agnostic
to the articulatory synthesizer chosen
because the motor expansion system
will handle the mapping
from production information
to control signals.

The motor expansion system implies
that Sermo should employ
a standardized production representation,
so that only one mapping is required
for each articulatory synthesizer.
Currently, it is not clear
what production representation
should become that standard.
Previously, we have argued
vocal tract gestures may be an
ideal choice for representing production information.
However, we have not yet determined
if vocal tract gestures can be decoded
from acoustic information
(??? has done this for vocal tract parameters),
nor do we have empirical evidence
that vocal tract gesture trajectories
can be generated and classified robustly.
Additionally, there are unfortunately
at least two systems that employ
a disparate set of vocal tract gestures.
The researchers that first proposed
the task dynamics framework,
and the ??? CASY synthesizer
use a set of ??? vocal tract gestures
??? more ??? cite.
??? Kr\"{o}ger and Birkholz,
on the other hand,
use ??? gestures in the
??? me and bernd synthesizer,
and 49 gestures in the VocalTractLab
synthesizer ???cite.
If a vocal tract gesture representation
is shown to be advantageous
for the operations required by Sermo,
then standardizing on a single
vocal tract gesture representation
will be a critical next step.
Similarly,
if it is determined that
continuous production information
in the form of articulator positions
is a more advantageous representation
for Sermo,
then a sufficiently expressive
continuous set of control points
will have to be chosen.

While this motivation for the
motor expansion could be interpreted
as meaning that it does not
have an analogous system in the brain,
the motor expansion system
may also play an important role
in compensating for vocal tract perturbations.
When our vocal tracts are modified in some way,
perhaps by illness or injury,
or by a deliberate manipulation
as in ??? micheal jordan's thing,
we can adapt our speech
such that changes in how we voice a phoneme
transfer to other syllables containing that phoneme.
Since it is unrealistic to think that
each syllable trajectory is modified
when adapting to the perturbation,
it is reasonable to assume that
there is an intermediate representation
in between the production information
produced by the sensorimotor integration system
and the control signal that is sent
to the motor system.
In Sermo, the representations used
in the motor expansion system
would play that role.

\subsubsection{Articulatory synthesizer}

Articulatory synthesizers
generate audio waveforms
by simulating the airflow
through a model of the human vocal tract.
A synthesizer consists of a vocal tract model,
which converts control signals
into vocal tract geometries,
and an acoustic model,
which converts vocal tract geometries
into acoustic signals.
As with auditory periphery models,
Sermo uses existing solutions
to articulatory synthesis,
and aims to be a testbed
in which articulatory synthesizers
can be compared to one another
using the same control system.

From the perspective of Sermo,
the details of the vocal tract
and acoustic models
should be abstracted away
and only considered
in the motor expansion system.
Primarily, we are concerned with
how each synthesizer is controlled,
and whether the synthesizer
can be used online.
However, even synthesizers
that cannot be used online
can still be tested with Sermo
by generating the control signals online
and providing them to the synthesizer offline.
These synthesizers would not be suitable
for real time conversation without
being adapted for online use.

\section{Relation to the DIVA Kr\"{o}ger models}

As discussed in Section~???,
the models most similar to Sermo
and the subsystems modeled in this thesis
are the DIVA model by ??? Guenther et al,
and the Kr\"{o}ger model by ??? kroger et al.

??? more

\section{Subsystems modeled in this thesis}

In the subsequent chapters of this thesis,
we will present mechanistic models
implemented in spiking neurons
spanning several systems
in the conceptual Sermo model.

??? go over the implementation and discuss
what parts they span

\section{Limitations}

??? there are large swaths of the model missing,
most notably in how the ventral linguistic systems
might be implemented.

??? there's also a bunch of prosodic stuff;
this could be added (see comments below)

% Aside from phonemes, we also represent pitch and volume.
% Neither of these features are useful
% as an absolute quantity---most humans are poor
% at judging absolute pitch and volume ???cite---so
% we aim to represent relative pitch and volume.
% In both of these cases,
% we must determine a baseline pitch or volume,
% and a method for comparing the current
% pitch or volume to the baseline.

% In speech, baseline pitch is primarily determined
% by speaker identity.
% Each speaker has a baseline pitch,
% determined in part by the shape of their vocal folds,
% so it is likely that we learn
% and remember the baseline pitch
% of speakers that we interact with frequently.
% On the other hand,
% changes in baseline pitch
% (e.g., through illnesses affecting the vocal tract)
% require little to no adaptation period,
% so the mechanism through which we determine
% a speaker's baseline pitch
% is likely to be relatively simple
% and flexible.
% In this work,
% we will not posit a neural mechanism
% for determining baseline pitch,
% and will instead compute it
% from the data offline and provide it as input.

% Given the baseline pitch as input,
% relative pitch will be computed
% primarily through neural inhibition.
% See section ???implementation
% for how this is accomplished.

% Unlike pitch, baseline volume
% is not speaker specific;
% it is easy to notice when a speaker
% talks louder or quieter
% than the norm.
% ???baseline is the overall activity
% of a long-timescale filtered version
% of the current moment's power spectrum?
% ???relative volume is that minus a
% short-timescale filtered version?
% So it's basically just a derivative?
% ???not sure if we even care about volume
% to be honest...
% % http://www.sengpielaudio.com/calculator-loudness.htm

??? the only productive way around this is to
collaborate on the development of the model,
both conceptually and concretely.
We assert that
the traditional publication system
is just one piece of how science should progress.
Modern tools for visualizing
and disseminating information,
and proposing and discussing changes
are another piece of puzzle.
I have been using these tools
to track my personal progress,
and upon publication
will release these tools
in order to overcome these limitations.
