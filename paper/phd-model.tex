\chapter{Conceptual model}

The long term goal of the research
presented in this thesis
is a complete neurally realized model
of the human speech system,
which is able to maintain
natural conversations.
Implementing such a system
requires the collaborative efforts
of many domain experts.
In this chapter, we present
a conceptual model of the human speech system
that we believe can be implemented
in biologically plausible spiking neural networks.

The model, dubbed Sermo
(\textul{S}peech \textul{e}xecution and \textul{r}ecognition
\textul{m}odel \textul{o}rganism),
is a synthesis of the background material
summarized in the previous chapter.
The goal of Sermo
is to break the larger
intractable problem of human speech
into concrete subproblems
that have existing partial solutions,
or provide a reasonable challenge
for machine learning
and computational neuroscience researchers.
Pragmatically for this thesis,
Sermo provides context for
the subproblems for which
we have implemented concrete solutions
in subsequent chapters.

\section{Sermo description}

??? main figure

Sermo is split into five interacting systems,
as summarized in Figure~???.
\begin{itemize}
  \item \textbf{Auditory feature extraction}
    converts incoming audio waveforms into
    features suitable for linguistic processing
    and sensorimotor integration.
    This system is analogous to the frontend
    of an ASR system,
    and to the auditory periphery and
    early auditory pathways in the human speech system.
  \item \textbf{Non-auditory sensory modalities}
    gather relevant information
    from non-auditory sources
    to inform or correct other systems.
  \item \textbf{Sensorimotor integration}
    uses sensory information to
    recognize and predict the motor intentions
    of speakers, including one's self.
    This system is analogous to the
    dorsal stream in the human speech system.
  \item \textbf{Linguistic processing}
    converts sensory and sensorimotor information
    into sublexical and lexical representations
    suitable for high level linguistic
    and cognitive reasoning.
    This system is anologous to
    the language model in an ASR system,
    and to the ventral stream
    in the human speech system.
  \item \textbf{Speech motor control}
    uses lexical and sensorimotor information
    to generate motor commands
    that drive a simulated vocal tract model,
    which produces audio waveforms.
\end{itemize}

Further details on each of these
systems follows in the subsequent sections.

In addition to the overall organization
of subsystems and how they interact,
we also impose the following constraints
on Sermo in order to make explicit
that it is modeled after a biological system,
and should be able to interact
with biological systems naturally.

\begin{itemize}
  \item Subsystems must operate in a continuous, online fashion.
  \item Subsystems must be implementable in biologically plausible
    spiking neurons.
  \item Learning rules may only use local error information,
    and must compute error signals with spiking neurons.
\end{itemize}

Another way to summarize these constraints
is to emphasize that Sermo
is a mechanistic model
of the human speech system.
We are not only concerned with
classifying and generating speech
with human-like accuracy,
we also wish to gain insight
into what information is required
to classify and generate speech.

Solutions to subproblems that do not
meet these criteria
(e.g., statistical models)
are still a crucial component
of progress in this model.
Some parts of the model are
necessarily statistical,
and the information being transformed
may not be easily explained
by a label like ``lexical representation.''
In these situations,
however, it is still a critical research activity
to adapt statistical models that work in
discrete time, with rate based neuron models,
or with idealized learning rules like backpropagation
to meet the above criteria,
for the following reasons.

\begin{itemize}
  \item Models meeting these criteria can be compared directly
    to experimental data at all levels,
    from single unit activity recordings to behavior.
  \item Models meeting these criteria can be implemented
    in neuromorphic hardware.
\end{itemize}

Direct experimental comparisons enable
iterative development of the model.
As Sermo subsystems are implemented,
they will be used to make testable predictions.
As these predictions are tested,
the model can be invalidated
or updated to incorporate new empirical results.

Neuromorphic hardware
presents a plausible avenue
to real time interaction
between humans and Sermo instances.
Many of the subsystems already implemented
run at many times slower than real time,
despite the use of recent general purpose
CPUs and GPUs,
and reasonably efficient algorithms.
While incremental advances
in these technologies may
bring subsystems closer to real time,
a fully integrated system
with all subsystems interacting online
is intractable for the foreseeable future.
Neuromorphic systems that are designed
to simulate millions of spiking neuron models
in real time
are currently under development
(e.g., ??? cites),
and may make a full Sermo instance
possible in the next few decades.

A final note about
the general architecture of Sermo
and its subsystems
is that we will focus on a view
of Sermo as it would be used
in a conversational setting
that does not require learning.
We treat each subsystem as a separate
module that is defined primarily
by the input it expects,
and the output it provides;
however, in development and other forms
of learning,
there would be a proliferation of
interconnections between modules
providing error information.
Detailing these connections
in each subsystem and across subsystems
would be an important area
for further developing the model,
but we believe that presenting
a static endpoint for learning
is important as a first step
before considering
how that endpoint might come about.
Regardless,
as learning is ubiquitous in speech,
we provide a model that learns
through synaptic plasticity
in Section~??? as
a proof of concept that
the subsystems in Sermo
can be learned if appropriate
error information is computed
as part of the model.

\subsection{Auditory feature extraction}

??? Add input / output box, similar to Implementation probbox?

The auditory feature extraction system
preprocesses audio waveforms
in order to support
linguistic and sensorimotor analysis.
As has been shown through
the gap between
automatic speech recognition
and human speech comprehension,
the temporal characteristics
of speech
make these analysis problems
difficult to solve
with traditional computers.
Digital computation is precise and discrete;
human speech is variable and continuous.
We hypothesize that the
parallel and distributed nature
of biological computation
is central to its
ability to process temporal information.

Sermo defines three subproblems
that must be solved
by the auditory feature extraction system:
\textit{spectral analysis},
\textit{decorrelation},
and \textit{temporal transformation}.
In spectral analysis,
the incoming auditory signal
is projected into frequency space
by analyzing the spectral density
of the signal in the recent past.
The spectral density of the signal
is expected to have high
correlation between nearby frequencies.
Generally, these correlations
make feature extraction more difficult,
so the decorrelation step
projects the spectral density
onto a set of basis functions
that are more orthogonal;
typically, fewer basis functions
are used than frequency components.
Finally, while the information
in the spectral density
and decorrelated vector
contains information from
a window of time defined by
the spectral analysis step,
the temporal characteristic of speech
may require that information
from further in the past is available
at the current time.
Temporal transformations
maintain past information
to filter spectral
and decorrelated spectral information
for better linguistic
and sensorimotor information decoding.

\subsubsection{Spectral analysis}

Sermo theorizes that spectral analysis
is performed by a model
of the auditory periphery.
An auditory periphery model
emulates the human ear
up to the signals
traveling down the auditory nerve.

There are many existing
models of the auditory periphery
that meet the constraints
imposed by Sermo (see Section~???).
Each auditory perhiphery model
captures certain psychoacoustic phenomena
at a particular computational cost.
In general, these two quantities
are often traded off;
the more accurately the ear is modeled,
the most costly the model is to simulate.

It is difficult to assert \textit{a priori}
which psychoacoustical phenomena
are important for speech.
It would be reasonable to assume that
the majority of what the ear does
is important for an individual's survival;
however, we process many types of sound,
not just speech.
An ideal auditory filter model
for the spectral analysis subsystem
in Sermo
would capture all of the phenomena
that are important for speech,
but no additional phenomena
that increase the computational cost
of the model.

\subsubsection{Decorrelation}

Decorrelation techniques like
linear filters and the discrete cosine transform
are widely used in digital computing
for compression ??? cite DCT_TR802.pdf.
In compression, a signal is projected
onto a set of basis functions;
only the coefficients are stored,
as an approximation of the original signal
can be recovered later by linearly combining
the basis functions weighted by those coefficients.
A similar argument can be made
to explain why automatic speech recognition systems
often use a decorrelation technique
at the frontend.
By operating on the coefficients of a set
of basis functions,
the set of parameters that must be optimized
by the backend is reduced.

While this line of reasoning
does not guarantee that
the brain also explicitly decorrelates
the frequency representation
provided by the auditory periphery,
it is clear that biological systems
aim to minimize energy expenditure whenever possible.
Learning through synaptic plasticity
takes more energy than does
typical signal transmission.
Since much of the speech system
is developed and refined
through synaptic plasticity,
using decorrelated representations
as early in the speech system as possible
may reduce the brain's energy expenditure
over one's lifetime.
Therefore, we hypothesize
that a system implemented
with a biologically plausible substrate,
as Sermo is,
decorrelates the frequency representation
provided by the auditory periphery.

\subsubsection{Temporal transformation}

The extent to which the recent past
is used to recognize speech
at the current moment
is of crucial importance,
yet is not often studied systematically.
In the brain, spectrotemporal receptive fields
have been measured using
tone ramps and other temporally varying signals,
revealing neurons that appear to be active
hundreds of milliseconds
after activity at
the cell's characteristic frequency,
but it is not clear if those
neurons are involved in speech,
nor is it clear what transformation they are implementing.
In automatic speech recognition,
the length of the audio frame
and how much it increments on each timestep
is often chosen arbitrarily,
or uses values known to work.

Until recently,
the most successful ASR systems
were based on Hidden Markov Models (HMMs),
which are based on the Markov assumption,
which that is that only the information
from the previous state is required
to reason about the current state.
Despite the fact that speech is highly temporally correlated,
these systems were relatively successful.
However, a great deal of engineering
went into the application of HMMs
to speech, primarily because of
its temporal nature
(see e.g., ??? Rabiner tutorial,
some other cite).
One such improvement for HMM-based ASR systems
is to include the first and second temporal derivatives
in the state representation.
Current state-of-the-art systems
are based on hierarchies of bidirectionally connected
recurrent neural networks (RNNs),
which are able to maintain information about
previous states through recurrent connections.
It is reasonable to assume that
a more flexible representation
of the recent past
is part of why these networks
fare better than HMM-based systems
which have had decades of incremental improvements.

Further study into how much temporal information
is maintained by deep RNNs,
and how that temporal information is transformed
would benefit Sermo
and other speech systems.
In particular, expressing the deep RNN's
temporal transformation as a
linear time-invariant (LTI) filter
would be of particular interest,
as LTI filters can be efficiently implemented
with spiking neural networks
(see ??? cite Aaron's stuff).
Alternatively, LTI filters
commonly used in engineering applications
can be experimented with
to investigate whether decoding accuracy
is improved in downstream
linguistic and sensorimotor systems.

The above system is primarily feed-forward
(though feedback is used extensively in the
auditory preprocessing layer).
It is important to note that
a complete auditory processing system
would match the brain (???refs) and have
feedback connections between all layers
in order to refine each layer's ability
to provide useful output downstream layers.
(??? more stuff from refs)

Feedback is an undeniably important
component of a full system that operates
for long periods of time with sensors
that are constantly changing and degrading.
In this work, however, we assume
that the auditory periphery does not change
its ability to process sounds over a long timescale,
and therefore we do not include corrective feedback
signals between layers in the auditory system.
This type of feedback could be added
to this system in future work.

\subsubsection{Evaluation}

Progress in the auditory feature extraction system
is difficult to measure
because it is most useful
in the context of other systems.
It is not readily apparent
what impact
an improved decorrelation network
might have on the sensorimotor integration
system's ability to decode production information,
for example.
Although we can assume that
an output representation with
less autocorrelation
will be easier to decode,
the gains from this improvement
may be modest compared to
changing the time constants on temporal transformations,
or even increasing the number of neurons
used to represent certain pieces of information.

We believe that this difficulty
motivates the development
of large integrated systems
like Sermo
as it enables evaluation
through comparing candidates solutions
to different subproblems
in the context of
known best solutions
to subproblems in the sensorimotor integration
and linguistic processing systems.
In other words,
one of the primary contributions of Sermo
is to propose standardized input and output formats
required by each subsystem.
We will show a proof of concept
that Sermo supports such comparisons
in Section~??? in which we compare
??? auditory periphery models
on a word classification task.

\subsection{Non-auditory sensory modalities}

Biological systems are remarkably adept
at integrating information
from different sensory modalities
in order to guide behavior.
In speech,
we use visual information
from other speakers to improve recognition,
and somatosensory information
from vocal tract articulators
to improve production.
A fully realized version of Sermo
would include these non-auditory sensory systems;
however, the currently implemented
subset of Sermo does not.

\subsection{Sensorimotor integration}

Input: auditory and non-auditory features

Output: production information: vocal tract gestures,
or vocal tract parameters.

\subsection{Linguistic processing}

??? pass!

The feature layer takes temporally transformed
frequency information from the preprocessing layer
and represents high-level features relevant
to human speech.
Specifically, we represent phonemes,
pitch, and volume in an attempt
to capture the features that are
psychoacoustically probable
and sufficient to reproduce
natural speech through the synthesis system.

Unlike the preprocessing layer,
the transformations in the feature layer
are computed across feedforward connections.
The exact transformation is unique to each feature.

Phonemes are the most complicated feature
that we represent.
We track
the current vocalic and consonantal phoneme separately
because these two types of phonemes
have very different acoustic properties.
Vowels are produced with an open vocal tract,
and are typically voiced longer than consonants.
For these reasons, the temporal information
necessary to recognize vowels
may be readily available at
the current moment;
however, using temporal information relatively
distant in the past (e.g., ???20-30ms)
may result in better performance since we can
leverage the fact that vowels are voiced for longer.
Another question that we can investigate in this model
is whether diphthongs should be considered phonemes
or a string of two phonemes.
Diphthongs may be easier to recognize as a single unit
by exploiting the temporal information available
in the system;
on the other hand, if the individual components
of the diphthong are easy to recognize,
then there is no need to treat them as a single unit.

Consonants, on the other hand, are produced
through constrictions of the vocal tract,
resulting in short disturbances or stoppages in air flow
in between two vocalic contexts.
Temporal information is likely to be necessary
in the case of consonants
(i.e., the current moment's frequency information
is unlikely to be sufficient),
and the temporal information of interest
is in the recent past (e.g., ???5-10ms).
It also worth noting that ???almost
all languages have more consonants than vowels,
as there are more possible places to constrict
the vocal tract than there are acoustically
differentiable shapes for an open vocal tract.
Because of this, we are likely to need
more neural resources to decode consonants
than vowels
(either purely in terms of the number of neurons,
or in the amount of temporal information conveyed
by the neurons).

Unlike generic LTI systems and other features,
there is unlikely to be a concise mathematical equation
describing how a vowel or consonant is to be recognized;
the true function is likely to be too complicated,
and may differ across people depending on their experience.
Instead, for phoneme recognition
we construct a series of labeled input-output pairs,
and approximate the true function
through a least-squares minimization procedure
(see ???NEF section).
We first test the system
with a corpus of synthesized speech,
which has less variability and should therefore
be easier to decode.
We then test the system
with a corpus of natural speech
???deets.

% synthesized: http://festvox.org/dbs/
% natural...: http://opendata.stackexchange.com/questions/1327/speech-audio-databases-with-phonemes-labelled

Currently, we are using two discrete systems
for vowels and consonants.
It is not clear whether
semivowels, semiconsonants, and glides
should be considered vowels or consonants,
or whether we need another system
for detecting these phonemes.
Alternatively, it could be the case
that there is only one monolithic system
that takes in a large amount of temporal information
and can decode any phoneme.
We do not address this issue here,
and suggest it as an interesting future extension.

Aside from phonemes, we also represent pitch and volume.
Neither of these features are useful
as an absolute quantity---most humans are poor
at judging absolute pitch and volume ???cite---so
we aim to represent relative pitch and volume.
In both of these cases,
we must determine a baseline pitch or volume,
and a method for comparing the current
pitch or volume to the baseline.

In speech, baseline pitch is primarily determined
by speaker identity.
Each speaker has a baseline pitch,
determined in part by the shape of their vocal folds,
so it is likely that we learn
and remember the baseline pitch
of speakers that we interact with frequently.
On the other hand,
changes in baseline pitch
(e.g., through illnesses affecting the vocal tract)
require little to no adaptation period,
so the mechanism through which we determine
a speaker's baseline pitch
is likely to be relatively simple
and flexible.
In this work,
we will not posit a neural mechanism
for determining baseline pitch,
and will instead compute it
from the data offline and provide it as input.

Given the baseline pitch as input,
relative pitch will be computed
primarily through neural inhibition.
See section ???implementation
for how this is accomplished.

Unlike pitch, baseline volume
is not speaker specific;
it is easy to notice when a speaker
talks louder or quieter
than the norm.
???baseline is the overall activity
of a long-timescale filtered version
of the current moment's power spectrum?
???relative volume is that minus a
short-timescale filtered version?
So it's basically just a derivative?
???not sure if we even care about volume
to be honest...
% http://www.sengpielaudio.com/calculator-loudness.htm

We believe that these three features
are sufficient to reproduce natural speech,
including prosodic features implicitly
by virtue of tracking these three features
over the timescale of an utterance.
Many aspects of prosody,
such as the rhythm of an utterance,
can be reproduced by rerunning
the three feature trajectories through time.
However, reproducing prosodic features
does not mean that we are explicitly
representing them.
In more sophisticated systems
connecting with linguistic models,
these prosodic features would
be represented explicitly
and would therefore need to be
determined based on temporal transformation
from the preprocessing layer,
and also from temporal transformations
of the three features represented here.
These prosodic features,
which would be essential for determining
important context clues
like the emotional state of the speaker,
would therefore be represented in
a fourth layer, receiving input from
temporally transformed versions of
the features from the third layer.
These temporal transformations
could be computed with LIT filters
in the same way that the preprocessing layer
transforms frequency information.
However, as our goal is to
extract only the features necessary
to reproduce natural speech,
we leave the decoding of high-level prosodic features
as future work.

??? Possible thing to try: keep track of a global
speed; in general, most people talk at the same pace,
but you can tell if they are speaking in a
marked fast or slow pace.

\subsection{Speech motor control}

The synthesis system translates
high-level features describing an utterance
into an audio waveform.
There are two components of the synthesis system:
an articulatory synthesizer and a neural control system.

??? figure of whole system

\begin{itemize}
\item \textbf{Articulatory synthesizer.} The articulatory synthesizer
  generates audio waveforms
  by simulating the airflow through a model of the human vocal tract.
  It consists of a vocal tract model,
  which converts control signals from the neural control system
  into vocal tract geometries,
  and an acoustic model,
  which converts vocal tract geometries into acoustic signals
  (i.e., audio waveforms).
\item \textbf{Neural control system.} The control system
  translates high-level features into control signals.
  While these control signals are specific to the
  synthesizer being controlled,
  the control system is designed to be as generic
  as possible such that it can be modified to control
  any articulatory synthesizer.
\end{itemize}

??? might want to make this more specific after writing
the background section

\subsubsection{Articulatory synthesizer}

As summarized in ???secref,
articulatory synthesizers are composed
of a vocal tract model and an acoustic model.
While state-of-the art synthesizers aim to
maximize speech intelligibility,
our primary focus is on control,
so computational efficiency
and interoperability are
key characteristics for choosing a synthesizer.

Interoperability refers to the ability
to incorporate the synthesizer
in the main simulation loop,
which in our case is driven by
the Nengo neural simulator
implemented in Python
(see ???nengosec).
Using the Nengo simulator places
two key interoperability restrictions.
First, the synthesizer must
be able to run ``online'';
that is, the synthesizer should maintain
some internal state which updates
based on control parameters
that are updated on each timestep.
Online synthesis contrasts with
batch synthesis, in which
the entire control parameter trajectory
must be known \textit{a priori}.
Second, the synthesizer must
be accessible through the Python
programming language in a performant manner.
While any synthesizer can communicate
with Python through some means
(e.g., foreign function interface
or operating system sockets),
some communication protocols incur
significant computational costs.

None of the synthesizers summarized in
section ???secref
stood out as the best candidate
in terms of efficiency and interoperability.
We also hypothesized that better knowledge
of synthesizer internals would
result in better control methods.
For these reasons,
we chose to implement our own
simple articulatory synthesizer focusing on
efficiency and interoperability with Nengo.

??? vocal tract model

??? acoustic model -- wait until later

\subsubsection{Neural control system}

??? Rewrite after we make a few sims

??? This is basically the mapping between VTGs
and the actual motor tract trajectories.
Why is this necessary? It may not be,
but one thought of why it may be
is that we can easily account for
vocal tract perturbations,
and how we compensate works
across all of our utterances.
If we assume that we're just learning
motor trajectories for each syllable,
then we would have to update all of these
to have this kind of transference.
This doesn't make sense.
So, if we're generating vocal tract gesture trajectories,
then all we need to do is
adjust the mapping from gesture trajectories
to the articulator parameters,
and it will work for all syllables.

As summarized in ???secref,
the ???brain parts
are responsible for effecting
muscle movements that
vibrate the glottis,
deform the vocal tract,
and send air through it,
creating pressure waves
that we perceive as speech.
The neural control system
performs the same function as ???brain parts
in the synthesis system.
While we cannot be certain
what features the human brain
uses to generate control signals,
our model assumes that the
high level features of interest
are a string of phonemes,
a continuous relative pitch signal,
and a continuous relative volume signal.

??? At its core, the neural control system
deals with two types of mappings.
One type of mapping is a simple
input-output relationship
that can be described as a mathematical function,
similar to many of the mappings
done in the recognition system.
The second type of mapping is a temporal mapping.
Unlike the recognition system,
in which the goal was to
transform existing temporal information,
in the temporal mappings in the control system,
we aim to generate temporal information
from discrete inputs.
Essentially, we are mapping from
an input to a continuous sequence of outputs.
The output sequence has finite length,
but could repeat indefinitely.

??? In order to temporally control
the input string of phonemes,
it is initially represented
as a static symbol-like representation,
which we will call a $\text{WORD}$
even though not all strings of phonemes
will necessarily correspond
to a word in the vocabulary of a language.
The $\text{WORD}$ maps to a
sequence of phonemes,
which we will call
$\text{PHONE}(t)$,
where $t$ is time.

??? $$\text{WORD} \mapsto \text{PHONE}(t)$$

??? That is, for each word, we learn
what sequence of phonemes make up that word,
and when those phonemes begin.
Keeping track of when phonemes begin
is necessary because
each phoneme is also a temporal mapping,
from the phone to
a sequence of articulator movements
that will cause the articulatory synthesizer
to voice the target word.

??? $$\text{PHONE}(t) \mapsto \text{ART}(t)$$

??? The amount of time ...

??? or, maybe we should do this by
a function of WORD and current PHONE
that gives the next PHONE

??? we could just map directly from word
to articular movement -- that'd be easier.
But that implausible, you'd have to
essentially relearn how to voice each phone
for each word. that's not right, so we
have the intermediate step to make it
possible to share the phone -> art
function across all instances of that phone
in all words.

??? In the neural control system,
we use functions to map from
articular

??? The transitions that we currently do through
BG should be transferred to highly learned
cortical sequences eventually.
We might expect that the

\section{Relation to DIVA and the Kr\"{o}ger model}

\section{Subsystems modeled in this thesis}

\section{Limitations and how to overcome them}

??? there are large swaths of the model missing,
most notably in how the ventral linguistic systems
might be implemented.

??? the only productive way around this is to
collaborate on the development of the model,
both conceptually and concretely.
We assert that
the traditional publication system
is just one piece of how science should progress.
Modern tools for visualizing
and disseminating information,
and proposing and discussing changes
are another piece of puzzle.
I have been using these tools
to track my personal progress,
and upon publication
will release these tools
in order to overcome these limitations.
