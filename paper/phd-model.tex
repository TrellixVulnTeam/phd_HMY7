\chapter{Conceptual model}

The long term goal of the research
presented in this thesis
is a complete neurally realized model
of the human speech system,
which is able to maintain
natural conversations.
Implementing such a system
requires the collaborative efforts
of many domain experts.
In this chapter, we present
a conceptual model of the human speech system
that we believe can be implemented
in biologically plausible spiking neural networks.

The model, dubbed Sermo
(\textul{S}peech \textul{e}xecution and \textul{r}ecognition
\textul{m}odel \textul{o}rganism),
is a synthesis of the background material
summarized in the previous chapter.
The goal of Sermo
is to break the larger
intractable problem of human speech
into concrete subproblems
that have existing partial solutions,
or provide a reasonable challenge
for machine learning
and computational neuroscience researchers.
Pragmatically for this thesis,
Sermo provides context for
the subproblems for which
we have implemented concrete solutions
in subsequent chapters.

\section{Sermo description}

??? main figure

??? break it down: auditory feature extraction,
  linguistic processing (ventral),
  sensorimotor integration (dorsal),
  speech motor control

In addition to the overall organization
of subsystems and how they interact,
we also impose the following constraints
on Sermo in order to make explicit
that it is modeled after a biological system,
and should be able to interact
with biological systems naturally.

\begin{itemize}
  \item Subsystems must operate in a continuous, online fashion.
  \item Subsystems must be implementable in biologically plausible
    spiking neurons.
  \item Learning rules may only use local error information,
    and must compute error signals with spiking neurons.
\end{itemize}

Solutions to subproblems that do not
meet these criteria are still
a crucial component of progress in this model.
Adapting solutions that work in
discrete time, with rate based neuron models,
or with idealized learning rules like backpropagation
to meet these criteria
is a critical research activity,
for the following reasons.

\begin{itemize}
  \item Models meeting these criteria can be compared directly
    to experimental data at all levels,
    from single unit activity recordings to behavior.
  \item Models meeting these criteria can be implemented
    in neuromorphic hardware.
\end{itemize}

Direct experimental comparisons enable
iterative development of the model.
As Sermo subsystems are implemented,
they will be used to make testable predictions.
As these predictions are tested,
the model can be invalidated
or updated to incorporate new empirical results.

Neuromorphic hardware
presents a plausible avenue
to real time interaction
between humans and Sermo instances.
Many of the subsystems already implemented
run at many times slower than real time,
despite the use of recent general purpose
CPUs and GPUs,
and reasonably efficient algorithms.
While incremental advances
in these technologies may
bring subsystems closer to real time,
a fully integrated system
with all subsystems interacting online
is intractable for the foreseeable future.
Neuromorphic systems that are designed
to simulate millions of spiking neuron models
in real time
are currently under development
(e.g., ??? cites),
and may make a full Sermo instance
possible in the next few decades.

\subsection{Auditory feature extraction}

The recognition system is composed of
three layers connected in a feed-forward manner.

??? Note: I think we'll change from phonemes to vocal tract gestures,
for this model. It's possible to learn phonemes and / or words,
but that might not make sense for this particular system;
the ventral stream, for example, might use words,
but for us, gestures make more sense.
See nihms521633.pdf for another account of why phonemes
aren't necessarily the best thing.

???come up with a name maybe

???figure of whole system

\begin{itemize}
\item \textbf{Auditory periphery.} The auditory periphery layer
  takes incoming air pressure waves and converts them
  to a frequency representation,
  mimicking the function of the human ear.
\item \textbf{Auditory preprocessing.} The auditory preprocessing layer
  represents the frequency information from the auditory periphery,
  and through recurrent connections within the layer,
  also represents temporal dynamics of frequency information
  (e.g., time derivatives).
\item \textbf{Features.} The feature layer
  represents speech-relevant features;
  in this work, we represent phonemes, pitch, and volume.
  Features are computed from information
  in the auditory preprocessing layer.
\end{itemize}

This system is similar to many previous systems,
and aside from its feedforward nature,
follows naturally from the organization
of the human speech system (???ref or ???section).
The primary way in which this system
deviates from previous models
is by imposing biological constraints.
Specifically, the system operates
in continuous time,
and only uses locally available information.
Adhering to these constraints differentiates
this system from similar systems,
such as those based on deep learning (???refsection),
in which... ???more, it's discrete
This system is also ???something about NEF

\subsubsection{Auditory periphery}

???figure of periphery

As summarized in secition ???ref,
there are many models of the auditory periphery
that differ in how many phenomena they capture,
and in their computational costs.
For this system,
the primary selection criteria is
the computational cost;
periphery models that simulate quickly
allow for rapid iterative development
to determine the conditions
under which the model is successful
under which it fails.
We will therefore use
a bank of Gammatone filters as the
auditory periphery model,
as it captures the primary phenomena
of the auditory periphery
and incurs the least computational cost
of the models surveyed.

???talk about the various phenomena
and why it isn't important for this model

Unlike some of the other layers, however,
this layer is modular.
All auditory periphery models expose
generally the same interface;
the input is air pressure waves,
and the output is either action potentials
traveling down the auditory nerve,
or basilar membrane deflections
which can be converted to action potentials
using various spike generation mechanisms.
Should we determine that Gammatone filters
are not sufficient for extracting
the features that we aim to collect,
then we can try other auditory periphery models
with minimal changes to the rest of the system.
In doing so, we would also be able to
determine functional roles for the
unique phenomena captured by the
periphery model that is able to extract particular features.
However, we believe that many
of the phenomena captured
by sophisticated periphery models either
\begin{itemize}
\item replicate unnecessary biological detail
  (i.e., details that come about because biological parts
  are necessarily noisy and imperfect), or
\item become necessary in more complicated
  auditory environments than the ones
  we will use for testing in this work.
\end{itemize}

???question: do we take the envelope here,
or is that later?

\subsubsection{Auditory preprocessing}

The auditory preprocessing layer
takes frequency information from the auditory periphery,
and computes generic temporal transformations.
Each downstream feature will use a different
subset of these transformations.
For efficiency,
only transformations used by at least one
downstream feature will be
included in the final integrated system,
but for testing the auditory system independently,
many transformations will be computed
to determine the critical set of
transformations needed by each downstream feature.

It is important to be explicit about
why the auditory preprocessing layer is necessary,
as it exemplifies the constraints involved in
using biologically inspired methods.
We use temporal transformations
so that all of the information necessary
to determine the phoneme currently being voiced,
or to determine any other downstream feature,
is available at the current moment in time.
Humans can be trained to look at
audio spectrograms and decipher
the utterance based on how the
frequency information changes over time.
Many speech recognition systems operate
in a similar way, by looking at
???millisecond slices of time
and decoding phonemes with that portion
of the spectrogram ???refs and specifics.
In contrast, humans receive audio
as a constant stream,
using internal processes to remember
parts of the recent audio past
in order to decode linguistic
and non-linguistic information.
Our system also operates in
an online real-time fashion,
receiving a constant stream of audio input,
and using internal processes
to keep around the information
needed to decode speech.
The audio preprocessing layer
is the primary method in which
raw audio information from the recent past
is made available at the current time.

These transformations are expressed as
linear time-invariant filters.
These filters are applied to the incoming
frequency information.
???more about LTIs

???rundown of what transformations are done

???try to have a citation for each transformation

???include equations

A central question that we will explore
is whether features can be decoded
from temporal transformations
of each frequency,
or it is necessary for nearby frequencies
to interact in some transformations.
For example,
equation~???eqref calculates
the derivative of a signal;
i.e., it tells us how the power associated
with a particular frequency has changed recently.
We could instead look at the derivative
of the difference between two frequencies.
???put in equation.
This equation tells us how the relative
power between two frequencies
has changed recently.
???someref indicates that
correctly identifying bigrams
is as essential to phonology
as correctly identifying individual phonemes.
Temporal transformations that

This layer does not contain all possible
temporal transformations;
there are an infinite number of filters
that could be applied to the incoming
frequency information.
The filters above were chosen
because they appear in the literature.
However, additional transformations
can be added if they are needed
by other downstream features.
Other LTI filters could also be added
systematically to discover useful filters
that might form predictions of
filters we might find in the brain.

While the ability to add new LTI filters
does not make the preprocessing layer
as modular and interchangeable
as the auditory periphery,
the procedure for adding
new temporal transformations
is straightforward,
making the preprocessing layer
generic and extensible.

??? Modulation filterbank: implement, talk about

\subsubsection{Feature detectors}

The feature layer takes temporally transformed
frequency information from the preprocessing layer
and represents high-level features relevant
to human speech.
Specifically, we represent phonemes,
pitch, and volume in an attempt
to capture the features that are
psychoacoustically probable
and sufficient to reproduce
natural speech through the synthesis system.

Unlike the preprocessing layer,
the transformations in the feature layer
are computed across feedforward connections.
The exact transformation is unique to each feature.

Phonemes are the most complicated feature
that we represent.
We track
the current vocalic and consonantal phoneme separately
because these two types of phonemes
have very different acoustic properties.
Vowels are produced with an open vocal tract,
and are typically voiced longer than consonants.
For these reasons, the temporal information
necessary to recognize vowels
may be readily available at
the current moment;
however, using temporal information relatively
distant in the past (e.g., ???20-30ms)
may result in better performance since we can
leverage the fact that vowels are voiced for longer.
Another question that we can investigate in this model
is whether diphthongs should be considered phonemes
or a string of two phonemes.
Diphthongs may be easier to recognize as a single unit
by exploiting the temporal information available
in the system;
on the other hand, if the individual components
of the diphthong are easy to recognize,
then there is no need to treat them as a single unit.

Consonants, on the other hand, are produced
through constrictions of the vocal tract,
resulting in short disturbances or stoppages in air flow
in between two vocalic contexts.
Temporal information is likely to be necessary
in the case of consonants
(i.e., the current moment's frequency information
is unlikely to be sufficient),
and the temporal information of interest
is in the recent past (e.g., ???5-10ms).
It also worth noting that ???almost
all languages have more consonants than vowels,
as there are more possible places to constrict
the vocal tract than there are acoustically
differentiable shapes for an open vocal tract.
Because of this, we are likely to need
more neural resources to decode consonants
than vowels
(either purely in terms of the number of neurons,
or in the amount of temporal information conveyed
by the neurons).

Unlike generic LTI systems and other features,
there is unlikely to be a concise mathematical equation
describing how a vowel or consonant is to be recognized;
the true function is likely to be too complicated,
and may differ across people depending on their experience.
Instead, for phoneme recognition
we construct a series of labeled input-output pairs,
and approximate the true function
through a least-squares minimization procedure
(see ???NEF section).
We first test the system
with a corpus of synthesized speech,
which has less variability and should therefore
be easier to decode.
We then test the system
with a corpus of natural speech
???deets.

% synthesized: http://festvox.org/dbs/
% natural...: http://opendata.stackexchange.com/questions/1327/speech-audio-databases-with-phonemes-labelled

Currently, we are using two discrete systems
for vowels and consonants.
It is not clear whether
semivowels, semiconsonants, and glides
should be considered vowels or consonants,
or whether we need another system
for detecting these phonemes.
Alternatively, it could be the case
that there is only one monolithic system
that takes in a large amount of temporal information
and can decode any phoneme.
We do not address this issue here,
and suggest it as an interesting future extension.

Aside from phonemes, we also represent pitch and volume.
Neither of these features are useful
as an absolute quantity---most humans are poor
at judging absolute pitch and volume ???cite---so
we aim to represent relative pitch and volume.
In both of these cases,
we must determine a baseline pitch or volume,
and a method for comparing the current
pitch or volume to the baseline.

In speech, baseline pitch is primarily determined
by speaker identity.
Each speaker has a baseline pitch,
determined in part by the shape of their vocal folds,
so it is likely that we learn
and remember the baseline pitch
of speakers that we interact with frequently.
On the other hand,
changes in baseline pitch
(e.g., through illnesses affecting the vocal tract)
require little to no adaptation period,
so the mechanism through which we determine
a speaker's baseline pitch
is likely to be relatively simple
and flexible.
In this work,
we will not posit a neural mechanism
for determining baseline pitch,
and will instead compute it
from the data offline and provide it as input.

Given the baseline pitch as input,
relative pitch will be computed
primarily through neural inhibition.
See section ???implementation
for how this is accomplished.

Unlike pitch, baseline volume
is not speaker specific;
it is easy to notice when a speaker
talks louder or quieter
than the norm.
???baseline is the overall activity
of a long-timescale filtered version
of the current moment's power spectrum?
???relative volume is that minus a
short-timescale filtered version?
So it's basically just a derivative?
???not sure if we even care about volume
to be honest...
% http://www.sengpielaudio.com/calculator-loudness.htm

We believe that these three features
are sufficient to reproduce natural speech,
including prosodic features implicitly
by virtue of tracking these three features
over the timescale of an utterance.
Many aspects of prosody,
such as the rhythm of an utterance,
can be reproduced by rerunning
the three feature trajectories through time.
However, reproducing prosodic features
does not mean that we are explicitly
representing them.
In more sophisticated systems
connecting with linguistic models,
these prosodic features would
be represented explicitly
and would therefore need to be
determined based on temporal transformation
from the preprocessing layer,
and also from temporal transformations
of the three features represented here.
These prosodic features,
which would be essential for determining
important context clues
like the emotional state of the speaker,
would therefore be represented in
a fourth layer, receiving input from
temporally transformed versions of
the features from the third layer.
These temporal transformations
could be computed with LIT filters
in the same way that the preprocessing layer
transforms frequency information.
However, as our goal is to
extract only the features necessary
to reproduce natural speech,
we leave the decoding of high-level prosodic features
as future work.

??? Possible thing to try: keep track of a global
speed; in general, most people talk at the same pace,
but you can tell if they are speaking in a
marked fast or slow pace.

\subsubsection{The role of feedback}

The above system is primarily feed-forward
(though feedback is used extensively in the
auditory preprocessing layer).
It is important to note that
a complete auditory processing system
would match the brain (???refs) and have
feedback connections between all layers
in order to refine each layer's ability
to provide useful output downstream layers.
(??? more stuff from refs)

Feedback is an undeniably important
component of a full system that operates
for long periods of time with sensors
that are constantly changing and degrading.
In this work, however, we assume
that the auditory periphery does not change
its ability to process sounds over a long timescale,
and therefore we do not include corrective feedback
signals between layers in the auditory system.
This type of feedback could be added
to this system in future work.

\subsection{Linguistic processing}

??? pass!

\subsection{Sensorimotor integration}

\subsection{Speech motor control}

The synthesis system translates
high-level features describing an utterance
into an audio waveform.
There are two components of the synthesis system:
an articulatory synthesizer and a neural control system.

??? figure of whole system

\begin{itemize}
\item \textbf{Articulatory synthesizer.} The articulatory synthesizer
  generates audio waveforms
  by simulating the airflow through a model of the human vocal tract.
  It consists of a vocal tract model,
  which converts control signals from the neural control system
  into vocal tract geometries,
  and an acoustic model,
  which converts vocal tract geometries into acoustic signals
  (i.e., audio waveforms).
\item \textbf{Neural control system.} The control system
  translates high-level features into control signals.
  While these control signals are specific to the
  synthesizer being controlled,
  the control system is designed to be as generic
  as possible such that it can be modified to control
  any articulatory synthesizer.
\end{itemize}

??? might want to make this more specific after writing
the background section

\subsubsection{Articulatory synthesizer}

As summarized in ???secref,
articulatory synthesizers are composed
of a vocal tract model and an acoustic model.
While state-of-the art synthesizers aim to
maximize speech intelligibility,
our primary focus is on control,
so computational efficiency
and interoperability are
key characteristics for choosing a synthesizer.

Interoperability refers to the ability
to incorporate the synthesizer
in the main simulation loop,
which in our case is driven by
the Nengo neural simulator
implemented in Python
(see ???nengosec).
Using the Nengo simulator places
two key interoperability restrictions.
First, the synthesizer must
be able to run ``online'';
that is, the synthesizer should maintain
some internal state which updates
based on control parameters
that are updated on each timestep.
Online synthesis contrasts with
batch synthesis, in which
the entire control parameter trajectory
must be known \textit{a priori}.
Second, the synthesizer must
be accessible through the Python
programming language in a performant manner.
While any synthesizer can communicate
with Python through some means
(e.g., foreign function interface
or operating system sockets),
some communication protocols incur
significant computational costs.

None of the synthesizers summarized in
section ???secref
stood out as the best candidate
in terms of efficiency and interoperability.
We also hypothesized that better knowledge
of synthesizer internals would
result in better control methods.
For these reasons,
we chose to implement our own
simple articulatory synthesizer focusing on
efficiency and interoperability with Nengo.

??? vocal tract model

??? acoustic model -- wait until later

\subsubsection{Neural control system}

??? Rewrite after we make a few sims

As summarized in ???secref,
the ???brain parts
are responsible for effecting
muscle movements that
vibrate the glottis,
deform the vocal tract,
and send air through it,
creating pressure waves
that we perceive as speech.
The neural control system
performs the same function as ???brain parts
in the synthesis system.
While we cannot be certain
what features the human brain
uses to generate control signals,
our model assumes that the
high level features of interest
are a string of phonemes,
a continuous relative pitch signal,
and a continuous relative volume signal.

??? At its core, the neural control system
deals with two types of mappings.
One type of mapping is a simple
input-output relationship
that can be described as a mathematical function,
similar to many of the mappings
done in the recognition system.
The second type of mapping is a temporal mapping.
Unlike the recognition system,
in which the goal was to
transform existing temporal information,
in the temporal mappings in the control system,
we aim to generate temporal information
from discrete inputs.
Essentially, we are mapping from
an input to a continuous sequence of outputs.
The output sequence has finite length,
but could repeat indefinitely.

??? In order to temporally control
the input string of phonemes,
it is initially represented
as a static symbol-like representation,
which we will call a $\text{WORD}$
even though not all strings of phonemes
will necessarily correspond
to a word in the vocabulary of a language.
The $\text{WORD}$ maps to a
sequence of phonemes,
which we will call
$\text{PHONE}(t)$,
where $t$ is time.

??? $$\text{WORD} \mapsto \text{PHONE}(t)$$

??? That is, for each word, we learn
what sequence of phonemes make up that word,
and when those phonemes begin.
Keeping track of when phonemes begin
is necessary because
each phoneme is also a temporal mapping,
from the phone to
a sequence of articulator movements
that will cause the articulatory synthesizer
to voice the target word.

??? $$\text{PHONE}(t) \mapsto \text{ART}(t)$$

??? The amount of time ...

??? or, maybe we should do this by
a function of WORD and current PHONE
that gives the next PHONE

??? we could just map directly from word
to articular movement -- that'd be easier.
But that implausible, you'd have to
essentially relearn how to voice each phone
for each word. that's not right, so we
have the intermediate step to make it
possible to share the phone -> art
function across all instances of that phone
in all words.

??? In the neural control system,
we use functions to map from
articular

??? The transitions that we currently do through
BG should be transferred to highly learned
cortical sequences eventually.
We might expect that the

\section{Relation to DIVA and the Kr\"{o}ger model}

\section{Subsystems modeled in this thesis}

\section{Limitations and how to overcome them}

??? there are large swaths of the model missing,
most notably in how the ventral linguistic systems
might be implemented.

??? the only productive way around this is to
collaborate on the development of the model,
both conceptually and concretely.
We assert that
the traditional publication system
is just one piece of how science should progress.
Modern tools for visualizing
and disseminating information,
and proposing and discussing changes
are another piece of puzzle.
I have been using these tools
to track my personal progress,
and upon publication
will release these tools
in order to overcome these limitations.
