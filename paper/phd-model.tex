\chapter{Conceptual model}
\label{chapt:model}

The long term goal of the research
presented in this thesis
is a complete neurally realized model
of the human speech system,
which is able to maintain
natural conversations.
Implementing such a system
requires the collaborative efforts
of many domain experts.
In this chapter, we present
a conceptual model of the human speech system
that we believe can be implemented
in biologically plausible spiking neural networks.

The model, dubbed Sermo
(\underline{S}peech \underline{e}xecution and \underline{r}ecognition
\underline{m}odel \underline{o}rganism),
is a synthesis of the background material
summarized in the previous chapter.
The goal of Sermo
is to break the larger
intractable problem of human speech
into concrete subproblems
that have existing partial solutions,
or provide a reasonable challenge
for machine learning
and computational neuroscience researchers.
Pragmatically for this thesis,
Sermo provides context for
the subproblems for which
we have implemented concrete solutions
in subsequent chapters.

\section{Sermo description}

??? main figure

Sermo is split into five interacting systems,
as summarized in Figure~???.
\begin{itemize}
  \item \textbf{Auditory feature extraction}
    converts incoming audio waveforms into
    features suitable for linguistic processing
    and sensorimotor integration.
    This system is analogous to the frontend
    of an ASR system,
    and to the auditory periphery and
    early auditory pathways in the human speech system.
  \item \textbf{Non-auditory sensory modalities}
    gather relevant information
    from non-auditory sources
    to inform or correct other systems.
  \item \textbf{Sensorimotor integration}
    uses sensory information to
    recognize and predict the motor intentions
    of speakers, including one's self.
    This system is analogous to the
    dorsal stream in the human speech system.
  \item \textbf{Linguistic processing}
    converts sensory and sensorimotor information
    into sublexical and lexical representations
    suitable for high level linguistic
    and cognitive reasoning.
    This system is anologous to
    the language model in an ASR system,
    and to the ventral stream
    in the human speech system.
  \item \textbf{Speech motor control}
    uses lexical and sensorimotor information
    to generate motor commands
    that drive a simulated vocal tract model,
    which produces audio waveforms.
\end{itemize}

Further details on each of these
systems follow in the subsequent sections.

In addition to the overall organization
of subsystems and how they interact,
we also impose the following constraints
on Sermo in order to make explicit
that it is modeled after a biological system,
and should be able to interact
with biological systems naturally.

\begin{itemize}
  \item Subsystems must operate in a continuous, online fashion.
  \item Subsystems must be implementable in biologically plausible
    spiking neurons.
  \item Learning rules may only use local error information,
    and must compute error signals with spiking neurons.
\end{itemize}

Another way to summarize these constraints
is to emphasize that Sermo
is a mechanistic model
of the human speech system.
We are not only concerned with
classifying and generating speech
with human-like accuracy,
we also wish to gain insight
into what information is required
to classify and generate speech.

Solutions to subproblems that do not
meet these criteria
(e.g., statistical models)
are still a crucial component
of progress in this model.
Some parts of the model are
necessarily statistical,
and the information being transformed
may not be easily explained
by a label like ``lexical representation.''
In these situations,
however, it is still a critical research activity
to adapt statistical models that work in
discrete time, with rate based neuron models,
or with idealized learning rules like backpropagation
to meet the above criteria,
for the following reasons.

\begin{itemize}
  \item Models meeting these criteria can be compared directly
    to experimental data at all levels,
    from single unit activity recordings to behavior.
  \item Models meeting these criteria can be implemented
    in neuromorphic hardware.
\end{itemize}

Direct experimental comparisons enable
iterative development of the model.
As Sermo subsystems are implemented,
they will be used to make testable predictions.
As these predictions are tested,
the model can be invalidated
or updated to incorporate new empirical results.

Neuromorphic hardware
presents a plausible avenue
to real time interaction
between humans and Sermo instances.
Many of the subsystems already implemented
run at many times slower than real time,
despite the use of recent general purpose
CPUs and GPUs,
and reasonably efficient algorithms.
While incremental advances
in these technologies may
bring subsystems closer to real time,
a fully integrated system
with all subsystems interacting online
is intractable for the foreseeable future.
Neuromorphic systems that are designed
to simulate millions of spiking neuron models
in real time
are currently under development
(e.g., ??? cites),
and may make a full Sermo instance
possible in the next few decades.

A final note about
the general architecture of Sermo
and its subsystems
is that we will focus on a view
of Sermo as it would be used
in a conversational setting
that does not require learning.
We treat each subsystem as a separate
module that is defined primarily
by the input it expects,
and the output it provides;
however, in development and other forms
of learning,
there would be a proliferation of
interconnections between modules
providing error information.
Detailing these connections
in each subsystem and across subsystems
would be an important area
for further developing the model,
but we believe that presenting
a static endpoint for learning
is important as a first step
before considering
how that endpoint might come about.
Regardless,
as learning is ubiquitous in speech,
we provide a model that learns
through synaptic plasticity
in Section~??? as
a proof of concept that
the subsystems in Sermo
can be learned if appropriate
error information is computed
as part of the model.

\subsection{Auditory feature extraction}

% ??? Add input / output box, similar to Implementation probbox?

The auditory feature extraction system
preprocesses audio waveforms
in order to support
linguistic and sensorimotor analysis.
As has been shown through
the gap between
automatic speech recognition
and human speech comprehension,
the temporal characteristics
of speech
make these analysis problems
difficult to solve
with traditional computers.
Digital computation is precise and discrete;
human speech is variable and continuous.
We hypothesize that the
parallel and distributed nature
of biological computation
is central to its
ability to process temporal information.

Sermo defines three subproblems
that must be solved
by the auditory feature extraction system:
\textit{spectral analysis},
\textit{decorrelation},
and \textit{temporal transformation}.
In spectral analysis,
the incoming auditory signal
is projected into frequency space
by analyzing the spectral density
of the signal in the recent past.
The spectral density of the signal
is expected to have high
correlation between nearby frequencies.
Generally, these correlations
make feature extraction more difficult,
so the decorrelation step
projects the spectral density
onto a set of basis functions
that are more orthogonal;
typically, fewer basis functions
are used than frequency components.
Finally, while the information
in the spectral density
and decorrelated vector
contains information from
a window of time defined by
the spectral analysis step,
the temporal characteristic of speech
may require that information
from further in the past is available
at the current time.
Temporal transformations
maintain past information
to filter spectral
and decorrelated spectral information
for better linguistic
and sensorimotor information decoding.

\subsubsection{Spectral analysis}

Sermo theorizes that spectral analysis
is performed by a model
of the auditory periphery.
An auditory periphery model
emulates the human ear
up to the signals
traveling down the auditory nerve.

There are many existing
models of the auditory periphery
that meet the constraints
imposed by Sermo (see Section~???).
Each auditory perhiphery model
captures certain psychoacoustic phenomena
at a particular computational cost.
In general, these two quantities
are often traded off;
the more accurately the ear is modeled,
the most costly the model is to simulate.

It is difficult to assert \textit{a priori}
which psychoacoustical phenomena
are important for speech.
It would be reasonable to assume that
the majority of what the ear does
is important for an individual's survival;
however, we process many types of sound,
not just speech.
An ideal auditory filter model
for the spectral analysis subsystem
in Sermo
would capture all of the phenomena
that are important for speech,
but no additional phenomena
that increase the computational cost
of the model.

\subsubsection{Decorrelation}

Decorrelation techniques like
linear filters and the discrete cosine transform
are widely used in digital computing
for compression ??? cite DCTTR802.pdf.
In compression, a signal is projected
onto a set of basis functions;
only the coefficients are stored,
as an approximation of the original signal
can be recovered later by linearly combining
the basis functions weighted by those coefficients.
A similar argument can be made
to explain why automatic speech recognition systems
often use a decorrelation technique
at the frontend.
By operating on the coefficients of a set
of basis functions,
the set of parameters that must be optimized
by the backend is reduced.

While this line of reasoning
does not guarantee that
the brain also explicitly decorrelates
the frequency representation
provided by the auditory periphery,
it is clear that biological systems
aim to minimize energy expenditure whenever possible.
Learning through synaptic plasticity
takes more energy than does
typical signal transmission.
Since much of the speech system
is developed and refined
through synaptic plasticity,
using decorrelated representations
as early in the speech system as possible
may reduce the brain's energy expenditure
over one's lifetime.
Therefore, we hypothesize
that a system implemented
with a biologically plausible substrate,
as Sermo is,
decorrelates the frequency representation
provided by the auditory periphery.

\subsubsection{Temporal transformation}

The extent to which the recent past
is used to recognize speech
at the current moment
is of crucial importance,
yet is not often studied systematically.
In the brain, spectrotemporal receptive fields
have been measured using
tone ramps and other temporally varying signals,
revealing neurons that appear to be active
hundreds of milliseconds
after activity at
the cell's characteristic frequency,
but it is not clear if those
neurons are involved in speech,
nor is it clear what transformation they are implementing.
In automatic speech recognition,
the length of the audio frame
and how much it increments on each timestep
is often chosen arbitrarily,
or uses values known to work.

Until recently,
the most successful ASR systems
were based on Hidden Markov Models (HMMs),
which are based on the Markov assumption,
which that is that only the information
from the previous state is required
to reason about the current state.
Despite the fact that speech is highly temporally correlated,
these systems were relatively successful.
However, a great deal of engineering
went into the application of HMMs
to speech, primarily because of
its temporal nature
(see e.g., ??? Rabiner tutorial,
other).
One such improvement for HMM-based ASR systems
is to include the first and second temporal derivatives
in the state representation.
Current state-of-the-art systems
are based on hierarchies of bidirectionally connected
recurrent neural networks (RNNs),
which are able to maintain information about
previous states through recurrent connections.
It is reasonable to assume that
a more flexible representation
of the recent past
is part of why these networks
fare better than HMM-based systems
which have had decades of incremental improvements.

Further study into how much temporal information
is maintained by deep RNNs,
and how that temporal information is transformed
would benefit Sermo
and other speech systems.
In particular, expressing the deep RNN's
temporal transformation as a
linear time-invariant (LTI) filter
would be of particular interest,
as LTI filters can be efficiently implemented
with spiking neural networks
(see ??? cite Aaron's stuff).
Alternatively, LTI filters
commonly used in engineering applications
can be experimented with
to investigate whether decoding accuracy
is improved in downstream
linguistic and sensorimotor systems.

The above system is primarily feed-forward
(though feedback is used extensively in the
auditory preprocessing layer).
It is important to note that
a complete auditory processing system
would match the brain (???refs) and have
feedback connections between all layers
in order to refine each layer's ability
to provide useful output downstream layers.
(??? more stuff from refs)

Feedback is an undeniably important
component of a full system that operates
for long periods of time with sensors
that are constantly changing and degrading.
In this work, however, we assume
that the auditory periphery does not change
its ability to process sounds over a long timescale,
and therefore we do not include corrective feedback
signals between layers in the auditory system.
This type of feedback could be added
to this system in future work.

\subsection{Non-auditory sensory modalities}

Biological systems are remarkably adept
at integrating information
from different sensory modalities
in order to guide behavior.
In speech,
we use visual information
from other speakers to improve recognition,
and somatosensory information
from vocal tract articulators
to improve production.
A fully realized version of Sermo
would include these non-auditory sensory systems;
however, the currently implemented
subset of Sermo does not.

\subsection{Sensorimotor integration}

As summarized in Section~???,
the dorsal stream of the human speech system
represents both sensory and motor information
when recognizing and producing speech.
In Sermo,
we hypothesize that
the sensorimotor integration system
is responsible for decoding production information
from sensory information,
and maintaining two types of associations.
First, sensory input,
both auditory and non-auditory,
is used to \textit{decode the production information}
that generated the sensory input.
Second, that decoded production information
is associated with
static syllable representations;
we call this \textit{trajectory classification},
as each syllable requires
a time series of sensory features
on the order of hundreds of milliseconds
to be reliably classified.
Second, static syllable representations
are associated with
trajectories of speech production information;
we call this \textit{trajectory generation},
as a time series of vocal tract movements
are necessary to voice a syllable.

As in the auditory feature extraction system,
a great deal of effort in involved in
dealing with the temporal nature of speech.
Were the sensory and motor representations static,
the associations in the sensorimotor system
could be accomplished
with simple associative memories
(see ??? spaun).
Instead, we are effectively associating
sensory trajectories with motor trajectories.
The key insight
that enables these associations in Sermo
is to use an intermediate syllable representation
that can be transmitted between
brain areas and queried
for the full trajectory.
A convenient side effect of
the intermediate syllable representation
is that it also enables
top-down influence from
other system,
most notably the linguistic processing subsystem.
It is not reasonable to expect
linguistic areas to communicate
production information directly
to motor cortex;
however, it is reasonable to expect
that the lexical representations
in the linguistic system
can be queried for the syllables
that make up those lexical items.

Sermo makes the strong claim
that sensorimotor integration,
and therefore speech motor control,
is organized at the level of syllables.
While there are ??? reasons
to make this claim
??? cite something about mental syllabary?,
our motivation for using syllables
as the primary unit
of speech motor representation
is that we believe
it enables sufficiently flexible speech
with the lowest expenditure of neural resources.

Syllables enable flexible speech
because we can fine-tune the
trajectory generated by any
one syllable representation individually.
For example,
if Sermo can improve its ability
to voice the syllable ??? /ba/,
then the quality of all words
containing the syllable ??? /ba/
improves.
Had we chosen word representations,
then this transfer learning
would not be possible.
On the other hand,
has we chosen phoneme representations,
then it would be difficult
to fine tune trajectories at all,
as the motor trajectory of a phoneme
is dependent on the
previous and next phonemes;
we therefore could not fine tune
the trajectory of each phoneme individually.

Syllables minimize the expenditure of neural resources
because syllables are mostly independent of one another,
and because there are a finite number of syllables.
To illustrate why these reasons are important,
let us assume that a certain amount of
neural resources (neuron and synapses)
are associated with the basic unit of representation.
We will denote this as $n$,
and assume that this amount is the same
regardless of the representation used.
If we choose phonemes as the basic representation,
then we have a small number of basic units;
for the sake of argument,
say around 50 phonemes.
However, because the motor trajectory
of a phoneme is dependent on the
phonemes before and after it,
we need neural resources not only for
the basic unit, but in the worst case,
also for all possible
interactions of those basic units;
i.e., we would need $50^3n$ or 125,000$n$ resources.
On the other hand, if we chose
words as the basic unit of representation,
we would not have to worry about
between word interactions,
but the number of words in a language
is much larger than the number of syllables
in a language.
Using estimates for English
from ??? \url{http://semarch.linguistics.fas.nyu.edu/barker/Syllables/index.txt}
and ??? \url{http://www.languagemonitor.com/number-of-words/number-of-words-in-the-English-language-1008879/},
word representations would use approximately
1,025,109.8$n$ resources,
compared to just 15,831$n$ resources for syllables.
Syllable representations are clearly the most tractable
given these assumptions.
% ??? reminder: make sure we use this to argue
% that this scales in the results section.

So far, we have not been explicit
about our representation of production information.
As discussed in Section~???,
we could use a discrete representation
of production information
(i.e., vocal tract gestures)
or a continuous representation
of production information
(i.e., vocal tract articulator positions).
Unlike in the choice of
using syllable representations,
the number of possible vocal tract gestures
and the number of possible articulator positions
is similar,
assuming a reasonable parameterization
of the vocal tract articulators
as is done in all articulatory synthesizers.
Therefore, we consider the choice
of production information representation
to be an empirical question
that we aim to answer
with the subset of Sermo that we implement
in this thesis.

\subsubsection{Trajectory generation}

In trajectory generation,
the static syllable representation
is used to generate
a time series of production information.

There are several criteria
that a trajectory generation system
must meet in order for it to be
useful in the sensorimotor integration system.
First, the system must be able to produce
trajectories with high degrees of freedom.
There are over twenty degrees of freedom,
whether we use vocal tract gestures
or articulator positions.
Note that these degrees of freedom
are independent,
so separate trajectory generators
can be employed for each degree of freedom,
but these generators must be
tightly synchronized as
consonantal constrictions can require
the temporal coordination
of several gestures or articulators.
Regardless, whether one large generator
or several smaller generators are used,
the neural resources required by each
must be reasonable,
as there may be many thousands
of these trajectory generators
in an adult speech system.

Second, the trajectory generators
must be flexible with respect to
the timescale at which
they generate a trajectory.
There is significant variability
in how we voice syllables.
During a stressful situation,
all of our syllables may be
voiced quickly;
syllables are often elongated
or shortened for emphasis.
A trajectory generator must be
able to produce appropriate
production information
at speeds that vary
between and within
trajectory generation trials.

Third, the trajectory generators
must be able to operate
in tight succession,
and possibly even overlap in time.
Speech is remarkably continuous;
to robustly segment speech into
syllables often requires
speech to be slowed down
or overenunciated.
Therefore, if our notion of production
requires representation at the level of syllables,
then the motor trajectories
resulting from those syllables
must be able to blend naturally into one another
without obvious pauses between
different syllables,
or even multiple utterances of the same syllable.

\subsubsection{Trajectory classification}

In trajectory classification,
we are essentially solving the inverse problem
of trajectory generation.
Given some time series of production information,
we must infer the static syllable representation
that would produce that trajectory.

As an inverse problem,
the trajectory classification system
has the same challenges
as the trajectory generation system.
The trajectories to be classified
will have relatively high degrees of freedom;
the trajectories may not advance uniformly
through time;
and subsequent syllables will follow
in quick succession,
even possibly overlapping.

\subsubsection{Decoding production information}

In the trajectory classification system,
we assume that the trajectories we are classifying
are trajectories of production information.
It may also be possible to directly classify
sensory information into syllables;
however, we follow the motor theory of speech recognition
which theorizes that production information
is directly inferred from auditory information.
One theoretical reason
why we assume that production information
is decoded from sensory information
is that it allows us
to frame trajectory generation and
classification as inverse problems
of each other.
In doing so, there may be ways
in which the classification and generation systems
can correct each other's errors.

A more practical justification
for decoding production information
is that it provides a site in which to
aggregate production information
gathered from multiple sensory modalities.
The McGurk effect discussed in Section~???
indicates that we use both auditory
and visual information to
determine which syllable is being voiced.
If part of our syllable classification system
if informed by a decoding of production information,
then both auditory and visual systems
can contribute to that decoding.
The speech development systems
embodied in DIVA and the Kr\"{o}ger model
also learn to associate sensory information
with one's own production information
through babbling and imitation stages.

Additionally,
it is possible to vocally imitate
novel syllables.
Novel syllables that are made up of sounds
(and therefore vocal tract gestures)
that are frequently encountered
in one's language can be
repeated readily.
Novel syllables that also feature
novel vocal tract gestures
are more difficult to repeat readily.
Since novel syllables do not have
a learned production information trajectory,
it is reasonable to hypothesize that
the syllable's production trajectory
is decoded from the speech information,
and passed through to the
speech motor control system unchanged.
Repeated repetitions of the syllable
would lead to a production trajectory
being learned and consolidated.
% ??? mention the model if I make it

??? has shown that it is possible to
do this decoding,
which some call auditory-articulatory inversion ???.
Currently, this has only been done
with continuous articulator positions;
determining if discrete vocal tract gestures
can be decoded from speech
would guide further development of Sermo.
We do not solve this problem
in this thesis;
however, we are able to use Sermo
to produce a corpus of time-locked
synthesized auditory information
and vocal tract gestures.
We will make this corpus available
to other researchers
to collaboratively determine
if this decoding process is possible.

\subsection{Linguistic processing}

As summarized in Section~???
the ventral stream of human speech system
represents high-level linguistic features of speech.
In Sermo, we hypothesize that
the linguistic processing system
decodes linguistically relevant information
like phonemes and words
from sensory and production information,
and provides syllable targets
to be voiced to the sensorimotor integration system.

Decoding phonemes and words from speech
(or, more accurately,
classifying phonemes and words from speech)
has been the primary topic of research
in ASR systems for decades.
As such, we are confident that
the methods used in ASR systems
can be implemented in a mechanistic model
suitable for incorporation with Sermo,
as has been done for similar
visual classification systems
(??? cite Hunsberger).
Once classified, lexical and sublexical items
will be used in higher level linguistic
systems that will associate
semantics (as in ??? blouw)
and syntax (as in ??? Stewart)
to these representations.
As these higher level systems
have been developed with
the Semantic Pointer Architecture (SPA),
we hypothesize that the linguistic processing system
produces lexical representations
in the form of semantic pointers.
However, we do not further constrain these representations
as constraints will come from the needs of
higher-level linguistic and cognitive systems,
which are not currently part of Sermo.

The output of these higher-level systems,
however, is constrained by the needs
of the sensorimotor system.
Specifically, Sermo requires that
the output of the
linguistic processing system
be in the form of static syllable representations.
Therefore, for the portion
of the linguistic system connected
to the sensorimotor system,
semantic pointers representing
sentences or words must contain
syllable representations that can be
queried through unbinding.
See Section~??? for more information
on unbinding in the SPA.

\subsection{Speech motor control}
\label{sec:model-motorcontrol}

The speech motor control system
uses a continuous trajectory
of speech production information
to drive an articulatory
speech synthesis system,
which produces audio waveforms.
It is composed of two components.
The first is the \textit{motor expansion} system,
which projects the speech production information
into the vector space that
directly controls the articulatory synthesizer.
The second is
the \textit{articulatory synthesizer} itself,
which includes a vocal tract model
and an acoustic model.

\subsubsection{Motor expansion}

To this point, we have discussed
production information abstractly,
either as vocal tract gestures
or vocal tract articulator positions.
In the neurobiological case,
there is only one possible
configuration for the vocal tract,
and so the production information
should be as close as possible to the
form required by the motor system.
While all vocal tracts are differently shaped,
the muscle groups controlling various articulators
should be identical in healthy humans.

In Sermo, however,
there is no perfect choice
for an articulatory synthesizer.
Indeed, it is a long-term goal of Sermo
to provide a unified control system
that can be used to productively
explore and test the quality
of articulatory synthesizers
by providing a control system
that can be used for any synthesizer,
if a motor expansion system
for that synthesizer can be created.
As such, the control signals
that each articulatory synthesizer
expects may differ significantly.
The motor expansion system
maps the production information
produced by the sensorimotor system
into the vector space that
the target articulatory synthesizer
uses as control signals.
It allows the rest of Sermo
to be designed agnostic
to the articulatory synthesizer chosen
because the motor expansion system
will handle the mapping
from production information
to control signals.

The motor expansion system implies
that Sermo should employ
a standardized production representation,
so that only one mapping is required
for each articulatory synthesizer.
Currently, it is not clear
what production representation
should become that standard.
Previously, we have argued
vocal tract gestures may be an
ideal choice for representing production information.
However, we have not yet determined
if vocal tract gestures can be decoded
from acoustic information
(??? has done this for vocal tract parameters),
nor do we have empirical evidence
that vocal tract gesture trajectories
can be generated and classified robustly.
Additionally, there are unfortunately
at least two systems that employ
a disparate set of vocal tract gestures.
The researchers that first proposed
the task dynamics framework,
and the ??? CASY synthesizer
use a particular set of ??? vocal tract gestures..
% ??? more ??? cite.
??? Kr\"{o}ger and Birkholz,
on the other hand,
use ??? gestures in the
??? our synthesizer,
and 49 gestures in the VocalTractLab
synthesizer ???cite.
If a vocal tract gesture representation
is shown to be advantageous
for the operations required by Sermo,
then standardizing on a single
vocal tract gesture representation
will be a critical next step.
Similarly,
if it is determined that
continuous production information
in the form of articulator positions
is a more advantageous representation
for Sermo,
then a sufficiently expressive
continuous set of control points
will have to be chosen.

While this motivation for the
motor expansion could be interpreted
as meaning that it does not
have an analogous system in the brain,
the motor expansion system
may also play an important role
in compensating for vocal tract perturbations.
When our vocal tracts are modified in some way,
perhaps by illness or injury,
or by a deliberate manipulation
as in ??? prism adaptation,
we can adapt our speech
such that changes in how we voice a phoneme
transfer to other syllables containing that phoneme.
Since it is unrealistic to think that
each syllable trajectory is modified
when adapting to the perturbation,
it is reasonable to assume that
there is an intermediate representation
in between the production information
produced by the sensorimotor integration system
and the control signal that is sent
to the motor system.
In Sermo, the representations used
in the motor expansion system
would play that role.

\subsubsection{Articulatory synthesizer}

Articulatory synthesizers
generate audio waveforms
by simulating the airflow
through a model of the human vocal tract.
A synthesizer consists of a vocal tract model,
which converts control signals
into vocal tract geometries,
and an acoustic model,
which converts vocal tract geometries
into acoustic signals.
As with auditory periphery models,
Sermo uses existing solutions
to articulatory synthesis,
and aims to be a testbed
in which articulatory synthesizers
can be compared to one another
using the same control system.

From the perspective of Sermo,
the details of the vocal tract
and acoustic models
should be abstracted away
and only considered
in the motor expansion system.
Primarily, we are concerned with
how each synthesizer is controlled,
and whether the synthesizer
can be used online.
However, even synthesizers
that cannot be used online
can still be tested with Sermo
by generating the control signals online
and providing them to the synthesizer offline.
These synthesizers would not be suitable
for real time conversation without
being adapted for online use.

\section{Evaluation}

As a conceptual model,
Sermo outlines what we believe a fully integrated
speech system would look like macroscopically.
The model provides context
motivating research of each
of the subproblems that must be solved
for the full model to work in real time.
It is important, therefore,
to consider how to evaluate
subsystems in Sermo,
as the fully integrated model
will take many years to develop.

Progress in the auditory feature extraction system
is difficult to measure
because it is most useful
in the context of other systems.
It is not readily apparent
what impact
an improved decorrelation network
might have on the sensorimotor integration
system's ability to decode production information,
for example.
Although we can assume that
an output representation with
less autocorrelation
will be easier to decode,
the gains from this improvement
may be modest compared to
changing the time constants on temporal transformations,
or even increasing the number of neurons
used to represent certain pieces of information.

Fortunately, the decoding systems
that are the primary use
of the feature extraction system
can be evaluated directly
through phoneme and word error rates,
in the case of linguistic decoding,
and through the mean squared error
in the case of sensorimotor decoding.
There are several existing data sets
with full training and testing
input-output pairs,
such as TIMIT for linguistic decoding
??? cite and
mngu0 and X-ray microbeam ???cites
for continuous sensorimotor decoding.
Currently, there are no data sets
for discrete sensorimotor decoding
(i.e., inferring vocal tract gestures
from acoustic information).
We plan to synthesize a
vocal tract gesture
data set to catalyze progress
in this area.

These data sets not only offer a method
to evaluate decoding methods,
but allow us to evaluate choices made
in the feature extraction system.
In general, we see the strength
of large integrated systems like Sermo
as allowing for candidates solutions
to different subproblems
to be evaluated in the context of
known best solutions
to subproblems in connected systems.
In other words,
one of the primary contributions of Sermo
is to propose standardized input and output formats
required by each subsystem.
We will show a proof of concept
that Sermo supports such comparisons
in Section~??? in which we compare
??? auditory periphery models
on a phoneme classification task.

The strongest evaluation
of the sensorimotor integration
and speech motor control systems
is for listeners to perceptually evaluate
synthesized speech.
If a desired sequence of syllables
is identified by all listeners
as the intended sequence,
then these systems can be considered successful.
However, en route to such an experiment,
the trajectory generation and classification systems
can compare their outputs to known trajectories.
While a version of Sermo
that explains development
would require many presentations
in order to generalize trajectories
associated with syllables,
the current system
is provided a desired trajectory
for each syllable.
Therefore, we can directly
test the sensorimotor integration system's
ability to classify
and generate known trajectories.
Classification if either done
correctly or incorrectly,
though some classification methods
may provide confidence information
that can be used to further evaluate the system.
Generation, on the other hand,
cannot be easily evaluated
as correct or incorrect.
One method of evaluation would be to
measure how much the generated trajectory
deviates from the target trajectory
(comparing, for example,
the squared jerk of the trajectories;
see ??? \url{http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3470860/}).
However, a stronger evaluation
is to use the generated trajectories
to drive the speech motor control system,
and perceptually evaluate generated audio.

\section{Relation to the DIVA and Kr\"{o}ger models}

As discussed in Section~???,
the models most similar to Sermo
and the subsystems modeled in this thesis
are the DIVA model by ??? Guenther et al,
and the Kr\"{o}ger model by ??? kroger et al.
Unlike Sermo, both of these models
focuses on the development of the speech system,
rather than attempting to model
an adult speech system directly.
Nevertheless, there are significant overlaps
between these models and Sermo.

DIVA (see Figure~???) primarily models
the trajectory generation step
in Sermo's sensorimotor integration system.
Activation of a cell in the ``speech sound map''
generates feedforward control signals
that drive an articulatory synthesizer.
It also includes auditory somatosensory feedback systems
analogous to Sermo's auditory feature extraction
and non-auditory sensory systems.

The primary strength of DIVA
that is not currently captured in Sermo
is an account of the development
of feedforward control signal trajectories
through repeated trials in which
auditory and somatesensory feedback
corrects parts of the trajectory
in which the produced sound differs
from the expected sound.
We believe that Sermo can be adapted
to learn trajectories online in a similar way,
using biologically plausible learning rules
(e.g., ??? cite macneil, bekolay).

There are several critical differences
between how Sermo and DIVA view
the overlapping systems.
First, while Sermo asserts that
the motor system is organized at the level
of syllables in order to ensure
realistic scaling to adult vocabularies,
DIVA allows its speech sound cells to
correspond to phonemes, syllables, words,
or even short phrases.
Additionally, in DIVA
the motor trajectory is initiated
by activating a speech sound cell,
(??? Guenther notes that a fully realized
model would use a distributed representation),
and it is not clear how sequences
of sound cells would be temporally coordinated.
In Sermo, considerable effort is taken
to create reasonable representations
for the speech target
(which in Sermo are always syllables),
and to temporally coordinate
sequences of syllables.

Second, the form of auditory feedback
differs significantly.
Sermo sees auditory processing as part of
the model, and therefore uses
a biologically inspired auditory filter
and statistical methods that can be
performed by spiking neurons.
DIVA processes audio waveforms offline,
and provides the model with
representations that are well suited
for the operations performed by the model;
specifically, they provide the model with
the first three formant frequencies
at each moment in time,
though they have used log formant ratios
and wavelet-based transformations
with similar results ???cite.
Regardless, this ideal offline auditory processing
gives DIVA a significant advantage
in terms of being able to
associate auditory feedback
with control signals,
which is the primary function of the model.

Finally, despite positioning itself as
a biologically plausible neural network model
of speech production
(the ``most thoroughly defined and tested''
as of 2006; ???cite)
it performs many operations
that are not easily implementable
in a biological system.
Most notably,
while simulated neurons communicate
by sending signals through
synaptic connection weight matrices,
perfect representation and communication
are assumed.
In other words, there is no neural
or synaptic model in DIVA;
model activations are defined as
differential equations
which are computed directly.
The state variables in these equations
are multiplied by connection weights,
but signals propagate immediately
and with no degradation or noise.
In order to represent temporal effects,
signals are perfectly delayed
by arbitrary amounts of time
(??? cite DIVA manual).
While it may be possible to approximate
the computations that
DIVA assumes are possible
with biologically plausible spiking neural networks,
it is not clear whether
neural approximations will
be sufficient to produce similar behavior
as the idealized DIVA model.

The Kr\"{o}ger model also
focuses on learning and development,
rather than a full adult speech system.
The primary system modeled
is the sensorimotor trajectory generation system,
which is learned using
auditory and somatosensory information.
This model also adds
a linguistic processing system,
allowing the motor system to be driven
by syllable representations.
Unlike DIVA, the Kr\"{o}ger model
assumes speech motor organization
at the level of syllables,
which matches Sermo's motor organization.
The Kr\"{o}ger model also
introduces an intermediate representation
between the motor plan (i.e., trajectory)
and the articulatory synthesizer,
playing a similar role as Sermo's
motor expansion system.
Thus, in terms of gross structure, then,
the Kr\"{o}ger model covers a larger subset
of Sermo compared to DIVA.

% The Kr\"{o}ger model also uses
% a more sophisticated neural network model,
% the growing self-organized map (GSOM).
% ??? summarize from Cao.

In terms of whether the operations
of the model can be realized in spiking neurons,
the outlook is more promising
for the Kr\"{o}ger model.
The auditory representation used
is the power spectrum of the audio signal,
which is computed as a bark-scaled spectrogram.
While this is an ideal mathematical transform,
it performs the same function as the
auditory periphery models used in Sermo.
However, while the neural network model
is an improvement over DIVA,
it still uses an overly simplistic
representation scheme.
Words in the phonemic map
are represented with one neuron each,
and syllables in the phonetic map
are also represented with one neuron;
this representation scheme is unrealistic
and  does not scale to adult vocabularies
(??? cite Crawford).
% ??? transition
% there is still no guarantee
% that a spiking neural approximation
% of GSOMs will be accurate enough
% to reproduce the results
% of the model.

\section{Subsystems modeled in this thesis}

In the subsequent chapters of this thesis,
we will present mechanistic models
implemented in spiking neurons
spanning several systems
in the conceptual Sermo model
(see Figure~???).

??? Figure with model areas highlighted

The first model presents an implementation
of the auditory feature extraction system
that performs similar operations as
the frontend in an automatic speech recognition system,
but does so in an online fashion
using an auditory periphery model
and a spiking neural network.
The resulting representations are called
neural cepstral coefficieints,
and we will compare their ability
to classify speech with
existing approaches used in ASR.

The second model implements
a syllable sequencing model
as a stand-in for a
full linguistic processing system,
and generates production information trajectories
from those syllables.
The production information will be
used to synthesize speech samples
with the VocalTractLab articulatory synthesizer.
We will compare the generated trajectories
to target trajectories,
and make qualitative observations
of the synthesized speech.

The third model implements
the trajectory classification portion
of the sensorimotor integration system.
Production information trajectories
will be generated
by the trajectory generation system
and provided to a trajectory classifier.
We will evaluate the system by
comparing the classified syllable
to the intended target syllable.

% ??? fourth model, learning?

\section{Limitations}

Two primary issues limit the Sermo model
as presented in this chapter.
First, it is clear that the subset of Sermo
that is implemented in this thesis
is more throughly defined than
the parts that are not modeled in detail,
such as the linguistic processing system
and non-auditory sensory modalities.
However, we envision the Sermo model
as continually growing and adapting
to incorporate new modeling ideas
and new empirical results confirming,
refuting, or constraining parts of the model.
The current presentation of Sermo
was developed independently,
primarily informed by the literature
summarized in Chapter~???.
We hope that upon presentation,
feedback from researchers will
fill in missing pieces of the model,
and clarify existing pieces,
making the development of Sermo
a collaborative effort.

However, it should be noted that
organizing a collaborative effort
like Sermo brings additional challenges.
To enable easier collaboration,
we will use modern Internet-based tools
for visualizing
and disseminating information,
in addition to the traditional
scientific publication system.
These tools provide mechanisms
for proposing and discussing changes
to a shared repository of materials,
which will include text, figures, and code.

A second issue that directly opposes
the idea that this model must be
developed through a large collaborative effort
is that Sermo's insistence on
using mechanistic models that
can be implemented with spiking neural networks
limits the number of researchers
that have the background necessary
to contribute models that implement
Sermo subsystems.
However, we believe that with the growing
interest in neuromorphic hardware (??? cite)
and the development of deep learning approaches
in spiking neural networks (??? cite),
interest and expertise in spiking neural networks
will steadily develop in the coming decades.
In the same vein, it may be argued that
the constraints placed on candidate Sermo models
are too stringent given our current understanding
of the human speech system,
and spiking neural networks in general;
we are ``not there yet,'' to put it colloquially.
I believe that the models presented
in the rest of this thesis
will serve as evidence that
progress can be made with
the tools currently available.
As interest and expertise in
the modeling techniques used
in the rest of the thesis grows,
so will the possibility
that all of the components in Sermo
can be implemented efficiently.
