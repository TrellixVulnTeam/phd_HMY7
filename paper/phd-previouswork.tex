\chapter{Previous work}

In this chapter, we review
existing complete or partial solutions
to the three problems
solved by the models
presented in the subsequent chapters.

\section{Auditory feature extraction}

As summarized in Section~??? model section,
the auditory feature extraction system
is based on the feature extraction pipeline
used as the frontend
of automatic speech recognition (ASR) systems.
The pipeline pictured in Figure~???
is similar across most systems.

\subsection{Mel-frequency cepstral coefficients (MFCCs)}

The most common feature extracted
and used in ASR systems
is called Mel-frequency cepstral coefficients (MFCCs).
It has been widely used
in both hidden Markov model-based (HMM; ??? cites)
and deep learning-based
(??? cites) ASR systems,
including those that achieve
the lowest error rates
on the popular corpus TIMIT.
It has also been shown that
MFCCs are well suited
for both speech and music inputs
(??? loganpaper.pdf).

Mel-frequency coefficients are inspired
by the human auditory system,
in the sense that they perform
frequency decomposition
of the audio signal
in order to determine the relative power
at frequencies distributed over the mel scale.
A simple algorithm for computing an
mel-frequency cepstral vector
for a frame of audio is as follows.

\begin{enumerate}
  \item Compute the discrete Fourier transform
    of the audio frame.
  \item Take the log of the power spectrum.
  \item Convolve the power spectrum
    with triangular filters distributed
    according to the Mel scale.
  \item Apply the inverse discrete cosine transform (iDCT)
    to the triangular filter outputs.
\end{enumerate}
The resulting coefficients obtained from
the iDCT are called ``cepstral'' coefficients.\footnote{
  The term ``cepstrum'' comes from
  the word ``spectrum'' with the first four letters reversed,
  as the spectrum is obtained with the Fourier transform
  and the cepstrum is obtained with the inverse Fourier transform.
  Similarly, the domain of the cepstrum is not frequency,
  but ``quefrency.''}

Recall from Section~??? that the Mel scale
describes the relationship between
absolute frequency and perceived pitch;
it is a logarithm of the frequency.
The triangular filters are spaced
equidistantly on the Mel scale,
resulting in more filters
at lower frequencies than at higher frequencies.
Typically, at least twenty triangular filters
are used in order to ensure that all frequencies
are captured in more than one filter
(i.e., there is overlap between adjacent filters).
The inverse discrete cosine transform
is designed to decorrelate the signal,
which results in the same amount
of information being represented
with fewer coefficients;
typically, the first thirteen coefficients
are used in the feature vector
of ASR systems.

\subsection{Delta MFCCs}

In addition to the thirteen MFCCs,
many ASR systems,
both HMM-based (??? cites)
and deep learning-based (??? cites),
also append the first,
and sometimes the second,
temporal derivatives of the MFCC
to the feature vector.

The justification for including derivatives
in the feature vector is typically
a practical one,
in that most ASR systems
achieve higher accuracy with
derivative information than without.
Theoretically,
most sources justify time derivatives
by noting that the derivatives
incorporate dynamics into the state representation.
However,
even recurrent deep learning systems
use MFCC derivatives
despite the fact that
state information from many previous frames
is available at the current timestep
(??? cite).
It therefore seems likely that
a sufficiently sophisticated machine learning algorithm
would learn that temporal derivatives
are a useful feature;
incorporating it into the feature vector
is not necessary,
but it effectively bootstraps the learning process
by providing it a feature
that would otherwise
have to be learned.

Temporal derivatives are not the only
MFCC transformation that are done in ASR frontends.
Some systems apply an additional transformation
analogous to low-pass filtering
called ``liftering'' that emphasizes
the lower part of the cepstrum.
However, while we are confident
that these other transforms
can be computed with spiking neural networks,
we limit our model to producing
MFCC and delta-MFCC-like features
in spiking neural networks.

\subsection{Spectral analysis with auditory periphery models}

While the analogy
between the frontend of ASR systems
and the human auditory system
is usually just an analogy,
many systems have experimented with
more physiologically accurate
auditory periphery models
to replace the idealized spectral analysis step
in the feature extraction pipeline.

??? tchorz1999.pdf
??? DimitrioMar.pdf,
??? 10.1.1.298.4746.pdf,
and ??? 10.1.1.151.1803.pdf
have separately proposed variants of MFCCs
that use Gammatone filters
to do the spectral analysis step in an ASR system.
% ??? tchorz1999.pdf
% ??? Dimitri ...
% ??? 10.1.1.298.4746.pdf ...
% and ??? 10.1.1.151.1803.pdf .... ? brief explanation of each.
All three of these studies
found that using Gammatone filters
lowered word or phone error rates
on recognition tasks in which
noise was added to speech samples.
Other studies has achieved similar results;
see ??? ICSIhearingis12.pdf for a review.

While all of these systems
have been applied successfully
for noisy ASR tasks,
they are not suitable
for inclusion in Sermo in their current form.
None of the methods currently available,
to our knowledge, produce spiking behavior
that could
be easily integrated with the rest of Sermo.
Additionally, these networks compute
functions of the filter output that
may be difficult for neurons to implement.
??? Tchorz, for example,
implements short-term adaption
through loops that perform
a lowpass filter and a division.
Division is, in general,
difficult to approximate with spiking neural networks.
A full neural implementation
may be possible,
but not trivial.

On the other hand,
silicon cochlea models face the opposite problem.
% ??? more, summarize some papers
These systems produce spikes
that emulate the spikes
traveling down the auditory nerve,
and since they are implemented in hardware,
they run much quicker than software solutions.
However, because they produce spikes,
they have not been used
as the frontend to any ASR systems,
to our knowledge,
and therefore we cannot
be certain that they are useful
for word and phone recognition.

\section{Syllable sequencing and production}

Currently,
we are aware of only two existing models
explaining how sequences of syllables
might be represented
and translated to trajectories of
production information in the brain.
However, there have been many attempts
to solve similar problems
and could be applicable to speech;
specifically,
models of song generation in songbirds
and serial working memory.

\subsection{Song generation in songbirds}

The avian song system
exhibits remarkable similarities
to the human speech system
(??? cite twitter evolution paper).
As such, models of song sequencing
and generation in birds
may be applicable to models of speech.

??? 1224.full.pdf presented a model
of birdsong sensorimotor learning
in which song sequencing is broken
into two subproblems:
syllable learning,
in which the system learns
to associate an ensemble of neurons
with a top-down syllable representation,
and sequence learning,
in which the activation
of an ensemble of neurons
associated with a particular syllable
is linked to the next syllable
in the learned sequence.
The end result of the model
that activating a particular
ensemble of syllable-specific neurons
begins a sequence of activations
representing a particular syllable sequence
associated with a target song.

The model uses a
simple associative learning rule
to learn sensory predictions
of motor representations,
and motor predictions
of sensory representations.
The sequence of syllables is produced
by a motor representation
making a prediction of the
sound that will be produced,
which is associated with
the next motor action to be produced,
which activates a sensory prediction,
and so on.

??? figure of all the sequences being the same?

While the learning method
may be useful for future iterations of Sermo,
the rest of the model is not suitable for Sermo
because it does allow for
flexible temporal dynamics
in the motor trajectories within and between syllables,
which is one of the hallmarks of human speech.
In ??? cite, all syllables are assumed
to take the same amount of time,
and activate all of the neurons
within the ensemble for the entirety
of the syllable activation.
The time taken for each syllable
is not easily modifiable since
it is defined by the amount of time
it takes for the motor action
to activate the sensory prediction
and switch to the next motor action,
which is an intrinsic property
of the synapse connecting the neuron models.

??? 2697.full.pdf presented a model
which learns to associate specific neural ensembles
to particular sensory inputs,
similar to ??? prev,
but also to sequences of sensory inputs.
For example, given syllables A and B,
some neurons would activate
when syllable A is presented,
some when syllable B is presented,
and some only when syllable B is presented
immediately following syllable A.
Since the temporal sequence is
coded in the connections between
these ensembles,
??? authors hypothesized that
sequences could be generated,
rather than recognized,
but delivering a generic timing pulse
which emulated hearing all possible sensory
inputs at once.
The first time the pulse is delivered,
ensembles sensitive to a single sound
would activate.
On the next time the pulse is delivered,
ensembles representing sequences of length two
would activate, and so on for longer sequences
(??? see Figure~???).
Ensembles representing sequences of length one
are not active on the second timing pulse
due to the intrinsic properties
of the neurons in the ensembles;
specifically, after spiking
for the previous timing pulse,
they enter a refractory period
in which they become insensitive to further input.

??? figure 7 from papaer?

While this approach is more temporally flexible
than ??? prev because
the timing pulse could arrive at any moment,
it only moves the responsibility for flexible timing
from the neurons involved in the sequence
to whatever mechanism generates the timing pulse.
In the paper, the timing pulse is provided
by the experimenter;
they note that the spacing of syllables
can be controlled by varying the frequency
of timing pulses,
but do not provide any mechanism
for generating the pulses
or for how frequency could be varied.
Additionally, the length of the sequence
is extremely limited,
both in terms of the total length of time
and the number of syllables.
The refractory period of each neuron
is only long enough to be insensitive
to the next timing pulse;
therefore, for sequence of length three,
??? authors added inhibitory connections
from sequence-specific ensembles
to non-sequence-specific ensembles.
The connectivity patterns required
for sequences of longer lengths
are not obvious,
and depends on the refractory period,
which is abnormally long.
We therefore do not think that
this model is useful for Sermo.

While other models for songbird generation exist
(e.g., ??? Fee et al),
none exhibit the temporal flexibility
that is required for speech.
As shown in ??? 362.full.pdf,
cooling a part of the avian brain
that projects to motor cortex
results in slowing the trajectory
in proportion to the temperature.
Therefore, birds may not possess
the ability to flexibly time their songs
in the same manner that humans time speech,
meaning we must look elsewhere for
more temporally flexible models.

\subsection{Serial working memory}

We assume that a sequence of syllables
is likely to be represented
in a similar manner
as sequences of other static representations.
Therefore, a useful paradigm for studying
how humans represent sequences
is to investigate serial working memory tasks;
e.g., a subject is asked to remember
a list of numbers,
and then later recall that list,
or elements from that list.
Several models have been proposed
to solve these tasks
in biologically plausible ways.
The most sophisticated such model
is the ordinal serial encoding model,
which has been incorporated into
Spaun (??? cite Spaun, Xuan thesis).

The model takes inspiration from
earlier serial working memory models,
namely CADAM, TODAM, and TODAM2
(see ??? xuan thesis for more details),
which are all based on
vector symbolic architectures
(see ??? methods sect).
Unlike these models,
??? Choo's model
is able to remember and recall
lists of up to seven items,
and exhibits primacy and recency effects,
meaning that items at the beginning
and end of a list are more likely
to be recalled.
His model is also implemented
in a spiking neural network,
making it applicable to Sermo.
Interestingly,
the OSE model does not exhibit
the primacy and recency effects
that are seen in humans
unless the model
is implemented in spiking neurons.
% rate-based models or direct simulation
% of the model's differential equations
% ??? ask Xuan what happens.

The ordinal serial encoding model,
then, is well suited for representing
discrete sequences of syllables in Sermo,
which typically have few elements
if we assume that most syllable sequences
correspond to words in one's lexicon.
However, the model is limited to
discrete sequences,
and therefore cannot be used
for detailed trajectory generation.
Additionally,
while the model is temporally flexible,
in that it can be queried for
a list element at any time,
there is a slight lag between
when the next element is queried
and when it has been recalled.
Syllables, however, are voiced in quick succession,
sometimes even blending into one another slightly.
In Sermo, we will use a separate but interacting model
for trajectory generation,
and will present a solution
to the issue of pauses between items
in Section~???.

\subsection{Task dynamics}

A line of research encompassing work by
several investigators at Haskins Laboratories
developed a set of techniques
under the name Task Dynamics
for generating temporal sequences
of production information
and using that information to drive
an articulatory synthesizer
??? cite TADA.

The key insight in Task Dynamics
is to dissociate the task state
from the motor trajectory
along that state.
By doing this, the dynamics of the task state
can be considered separately from motor trajectories,
while trajectories can be implemented
as a function of the task state.
In the initial publication of task dynamics
(??? SR076.pdf),
two types of task dynamics are presented.
Point attractor dynamics,
modeled by critically damped mass-spring systems,
are useful for one-time actions.
Cyclic attractor dynamics,
modeled by harmonic oscillators
with a nonlinear escapement function,
are useful for repetitive actions.
Initially, these task dynamics were then
related to articulator trajectories
for a two degree-of-freedom arm model
by mapping the task space
to body-centered coordinates,
joint coordinates,
and finally articulator positions,
called the task network.
In a subsequent publication,
??? SR099.pdf
applied task dynamics to speech production
using a simplified mapping,
from task space
to gesture space,
and then to articular space,
similar to that described in Sermo
(see Section~???).
Realistic articulatory trajectories
were achieved
by associating a task dynamic network
with point attractor dynamics
with each speech gesture.
Each gesture defines
the point which the task dynamic network
is attracted to.
The state of each gesture network
influences one or more articulator positions.
The final articulator trajectory
is the combined effect
of all of the gesture networks
on all of the articulators.

Given a complete gesture trajectory
for an utterance,
the task dynamic approach
generate articular position trajectories
which can be synthesized
by the ??? HLSyn synthesizer, cite.
While this model has not been
modified since its introduction in ??? year,
the same group has done significant work
in automatically generating gesture scores
with a similar approach.
In this extension,
instead of knowing \textit{a priori}
when each gesture should occur,
the point attractors associated
with each gesture are coupled
to one another,
such that the timing of each gesture
is controlled by the state
of the gestures to which it is coupled
(??? 10.1.1.307.1650.pdf).
Task dynamic gestural timing
is able to capture precise timing
in syllables with complex onsets
and codas (??? p15.2253.pdf).
Perhaps more importantly,
it allows for the articulatory synthesizer
??? HLSyn to accept text as input,
which is converted to a gesture score
by looking up a syllabification
of the word in a database
and using gestural coupling rules
defined by linguists
to create a system of coupled oscillators
whose activities represent a gestural score
??? cite goldsteinetalbeijing, TADA pub.

In all, the task dynamic approach
to inter-gestural and inter-articulator timing
is the most temporally flexible
syllable sequencing and production system
currently published
(to our knowledge).
The dynamical systems approach
also makes it a natural fit for Sermo,
as it is defined in continuous time,
and can be readily implemented
in spiking neural networks,
though we are not aware of any
neural implementations currently available.

\subsection{REACH}

The final model informing
the Sermo syllable sequencing
and production model presented
in this thesis is the
Recurrent Error-driven Adaptive Control Hierarchy (REACH)
model ??? cite dewolf.
In particular, we use REACH's
neural implementation
of dynamic movement primitives (DMPs).

The REACH model is a general motor control
model implemented in spiking neurons.
It uses DMPs to generate trajectories
for the system to follow;
these trajectories are mapped into
motor space using operational space control,
and unexpected changes in system dynamics
are accounted for online
using nonlinear adaptive control
??? cite.
The model is able to control
a nonlinear three-link arm model
in handwriting and reaching tasks,
even when an unknown force field
is applied to the end effector.

DMPs are a general method for generating motor trajectories;
they will be discussed in more detail
in Section~???.
DMPs are similar to Task Dynamics in many respects.
Both define methods to generate trajectories
for one-time and rhythmic actions.
Both dissociate the temporal dynamics
of the task from trajectories
in motor space,
allowing them to advance the system state
at variable rates,
and compute the trajectory
as a function of the system state.

The primary difference between DMPs and Task Dynamics
is how each system is implemented.
While both are useful for high degree-of-freedom
control problems,
they go about this differently.
In Task Dynamics,
a separate attractor is assigned
to each degree-of-freedom;
when the timing of different
dimensions depends on other dimensions,
the states are coupled,
as was done in
the inter-gestural planning model
described above.
In DMPs,
it is assumed that the system state
is shared by all dimensions,
but the function computed
from the system state can
have arbitrary dimensionality.
While this assumption makes it
difficult to time actions
as a function of other actions,
it does not limit the temporal flexibility
of DMPs, as actions that should occur
simultaneously or in quick succession
can do so by defining the temporal relationship
in the function computed from the system state,
since both dimensions share the same system state.
For example,
if the glottis should begin phonating
right after the lips are closed,
Task Dynamics would couple these two gestures.
With DMPs, the function specifying the gestures
would effect the glottal phonation
a short time after the lip closure.

Another difference between the two is that
for one-time actions,
DMPs use an integrator (i.e., ramp signal)
as the system state,
rather than a point attractor.
Both, however, use cyclic attractors
for rhythmic actions.
A final important difference for Sermo
is that DMPs have been implemented
in spiking neural networks successfully
(??? cite thesis).
We will explain DMPs in more detail
in Section~??? and present a model
using rhythmic DMPs for syllable production
in ???.

\section{Syllable recognition}

Syllable recognition, in general,
is a task that can be solved
by most ASR systems
using a labeled acoustic data set.
In the sensorimotor integration system
in Sermo, however,
we aim to classify syllables
based only on production information
which is decoded from acoustic information.
It should be noted that we do not expect
this system to perform perfectly,
as the dominant speech decoding system
will be linguistic;
however, we are nevertheless able
to hear infrequent syllables
and voice them.
The ability to differentiate
between frequent and infrequent syllables
may depend on whether they
can be classified
on the basis of production information.

One of the few attempts to classify speech
solely on the basis production information
is ??? p3041mitra.pdf.
??? authors were able to
decode continuous production information
from auditory information
using MFCCs and Gammatone filter-based
cepstral features (??? normalized.pdf),
and were able to use a combined feature vector
consisting of MFCCs and production information
to lower word error rates
in various noisy environments.
However, word error rates
when using only continuous production information
as the feature vector
were high ($\sim$70\% with no noise).

The apparent difficulty
in classifying sounds based on
production information alone
could be due to several factors.
For one, the details of the decoding mechanism
in ??? p3041.pdf are not clear.
The authors note that they use a
``deep neural network''
with as many as six hidden layers;
however, the choice of neural activation function,
optimization procedure,
and many other hyperparameters
can affect how well the network learns.
In particular, it does not seem as though
the network has recurrent connections,
which are commonly used in
current state-of-the-art ASR systems.

Alternatively, the statistical approach
used in most ASR systems
may not be well suited to
trajectories of production information.
In order to broaden our search
for other types of techniques,
we investigated general solutions
for trajectory classification
which are used in applications
such as gesture recognition
and automatic video analysis.

\subsection{Trajectory classification}

% ??? list some desirable qualities of a traj classifier
% from later on; e.g.,
% speed invariance (works regardless of how fast
% the traj is playing out),
% online classification, ...

Unsurprisingly, many of the existing
solutions for trajectory classification
are based on generative statistical models,
in particular Hidden Markov Models
as has been dominant in speech recognition
(see ??? paper1.pdf, TIP-traj...pdf
and ??? survey.handrecog.pdf,
Mitra - gesture.pdf
and RR-7212.pdf for reviews).
Given the failure of the statistical approach
in ??? p3041,
we instead focused on systems
that recognize trajectories
through tracking the trajectory
either in comparison to some known template,
or as the state in a dynamical system.
We identify three such systems
that provide inspiration
for the trajectory classification model
described in Section~???.

??? kilibozgud.pdf proposed
a gesture recognition system
for 2D trajectories
using a finite state machine approach.
Finite state machines are composed of
discrete states and a set of state-specific
functions that transition between states.
During a learning phase,
a target trajectory is played
several times,
and one or more finite state machines
are constructed such that
the target trajectory
causes the state machine
to transition to a final accepting state.
During recognition,
the continuous input trajectory
is presented to all finite state machines
in the system;
the first to reach the accepting state is recognized.
The system attains a 73\% accuracy rate
in a real-world user study.
As the system operates continuously online,
part of the system is applicable to Sermo;
however, the discrete state space and
gesture sequence representation
cannot be easily adapted to speech.
Each timestep in the gesture trajectory
is represented by a string denoting whether the
end effector has moved significantly
in the $x$ or $y$ direction since the last frame;
the overall trajectory is represented
by a regular expression generalized from
the strings seen in the learning phase.
It is not clear that this representation scheme
would scale to $n$-dimensional spaces,
as is required for production information trajectories.

??? documentocompleto.pdf
proposed a system called competitive neural classifiers (CNC)
that can recognize arabic number hand gestures
using small training sets (three examples per gesture).
CNC uses a collection of sub-classifiers
that compete in order to collectively
classify the overall trajectory.
The trajectory is segmented
into a set of subtrajectories,
each of which is evaluated by
a sub-classifier neural network
whose output neural activations
represent the probability that
the subtrajectory is produced by
the gesture associated with that output neuron.
The overall classification aggregates the results
across of all sub-classifiers,
producing the correct classification
in 98\% of test cases.
However, the system's impressive results
are partly due to an elaborate preprocessing step
in which a recorded trajectory
is normalized and resampled
such that the actual trajectory used as input
has a fixed number of sample points
uniformly distributed over
the total length of the trajectory
(see Figure~???).
This type of preprocessing
requires knowledge of the whole trajectory,
and therefore could not be implemented
in an online manner.
Additionally, the operations done
on the neural network outputs
are difficult to implement
with spiking neural networks
(e.g., division, argmax),
making this system unsuitable
for use in Sermo.

??? figure 1 from documentocompleto.pdf

Finally,
??? caramiaux.pdf
presented an algorithm called the
Gesture Variation Follower (GFV),
based on an online HMM-based
trajectory tracking technique
called Gesture Following (GF)
??? cite 02bfe50e.pdf,
10.1.1.401.4611.pdf.
The goal of the algorithm is to match
an input gesture sequence
with a set of predefined template gestures.
Unlike the HMM-based GF algorithm,
GFV views the trajectory as a dynamical system,
and uses a Particle Filtering algorithm
to learn a set of weights
that denote the importance of
each randomly generated particle
(i.e., dynamical system state)
to overall recognition accuracy.

The overall flow of the algorithm
is as follows.
First, a set of predefined example trajectories
are defined in the system.
Second, a set of particles
are randomly generated
in an $n$-dimensional space
with a uniform distribution,
and a set of weights are initialized
with each particle weighted equally.
In the main loop of the algorithm,
a random sample is drawn
according to each particle's position
in state space,
and the weights associated with each particle
are updated based on the distance
between the particle's sample
and the observed input.
If too many particles have small weights
associated with them,
then a resampling procedure is done
based on the current weights.

What differentiates this algorithm from
other particle filtering algorithms
is the structure of the state space.
Briefly, the state space that each particle is in
encodes the target gesture that this particle
is associated with,
and a probability distribution
over the phase of the trajectory
(i.e., how far along the trajectory
we currently are),
and the speed of the trajectory
(i.e., how far along the trajectory
do we expect to be on the next timestep).
The state evolves over time according to
predefined state transition functions,
and on each timestep,
each particle emits an observation
according to a possibly non-linear function.

A useful analogy for this algorithm
is that it implements an online version of an HMM.
Like HMMs, it maintains some internal representation
that can be used to estimate the probability
of a particular sequence
by forming a prediction
of the next observation.
The system is queried by providing
an observation,
which changes the internal representation
such that the next observation
is processed in the context of
the sequence of past observations.

GVF has been used successfully
in music generation systems
in which the music sample,
volume, and speed are
controlled by gestures
that are tracked by the GVF algorithm.
It achieves over 98\% accuracy
in a 2D gesture recognition task
with 16 possible gestures,
and was employed successfully
in a 3D hand gesture user study
with 10 participants.

While GVF is one of the most promising
algorithms for doing trajectory recognition
in Sermo,
it has two main limitations in its current formulation.
First, while the classifier can be used online,
it is formulated in discrete time;
this weakness should be possible to overcome,
however, as continuous time particle filtering
has been done in the past
??? ctpf.pdf.
Second, while the dynamical system
at the core of GVF is well suited
to be implemented in spiking neurons,
the resampling process is not.
On each step of the algorithm,
each particle represents a probability
distribution over the system state,
which is sampled
(sampling has been shown to be
possible with spiking neurons;
see ??? plotbuesing.pdf).
However, when the importance weights
of a sufficient number of particles
is below a particular threshold,
a new set of particles
is randomly generated
to replace those with low importance weights.
This procedure would translate to
a significant reorganization
of the neurons implementing
the sampling procedure,
which we believe
would not have evolved if
procedures that do not require
reorganization exist.

We will propose a trajectory classification technique
in Section~??? that shares many of the positive
characteristics of the GVF algorithm,
but can be implemented in a spiking neural network.
Like GVF, it is also based on the idea
of inferring internal dynamical system state
based on observations;
however, it performs the inference
in a manner that we relate to
DMPs (see Section~???).
