\chapter{Previous work}

In this chapter, we review
existing complete or partial solutions
to the ???three/four problems
solved by the models
presented in the subsequent chapters.

\section{Auditory feature extraction}

As summarized in Section~??? model section,
the auditory feature extraction system
is based on the feature extraction pipeline
used as the frontend
of automatic speech recognition (ASR) systems.
The pipeline pictured in Figure~???
is similar across most systems.

\subsection{Mel-frequency cepstral coefficients (MFCCs)}

The most common feature extracted
and used in ASR systems
is called Mel-frequency cepstral coefficients (MFCCs).
It has been widely used
in both hidden Markov model-based (HMM; ??? cites)
and deep learning-based
(??? cites) ASR systems,
including those that achieve
the lowest error rates
on the popular corpus TIMIT.
It has also been shown that
MFCCs are well suited
for both speech and music inputs
(??? loganpaper.pdf).

Mel-frequency coefficients are inspired
by the human auditory system,
in the sense that they perform
frequency decomposition
of the audio signal
in order to determine the relative power
at frequencies distributed over the mel scale.
A simple algorithm for computing an
mel-frequency cepstral vector
for a frame of audio is as follows.

\begin{enumerate}
  \item Compute the discrete Fourier transform
    of the audio frame.
  \item Take the log of the power spectrum.
  \item Convolve the power spectrum
    with triangular filters distributed
    according to the Mel scale.
  \item Apply the inverse discrete cosine transform (iDCT)
    to the triangular filter outputs.
\end{enumerate}
The resulting coefficients obtained from
the iDCT are called ``cepstral'' coefficients.\footnote{
  The term ``cepstrum'' comes from
  the word ``spectrum'' with the first four letters reversed,
  as the spectrum is obtained with the Fourier transform
  and the cepstrum is obtained with the inverse Fourier transform.
  Similarly, the domain of the cepstrum is not frequency,
  but ``quefrency.''}

Recall from Section~??? that the Mel scale
describes the relationship between
absolute frequency and perceived pitch;
it is a logarithm of the frequency.
The triangular filters are spaced
equidistantly on the Mel scale,
resulting in more filters
at lower frequencies than at higher frequencies.
Typically, at least twenty triangular filters
are used in order to ensure that all frequencies
are captured in more than one filter
(i.e., there is overlap between adjacent filters).
The inverse discrete cosine transform
is designed to decorrelate the signal,
which results in the same amount
of information being represented
with fewer coefficients;
typically, the first thirteen coefficients
are used in the feature vector
of ASR systems.

\subsection{Delta MFCCs}

In addition to the thirteen MFCCs,
many ASR systems,
both HMM-based (??? cites)
and deep learning-based (??? cites),
also append the first,
and sometimes the second,
temporal derivatives of the MFCC
to the feature vector.

The justification for including derivatives
in the feature vector is typically
a practical one,
in that most ASR systems
achieve higher accuracy with
derivative information than without.
Theoretically,
most sources justify time derivatives
by noting that the derivatives
incorporate dynamics into the state representation.
However,
even recurrent deep learning systems
use MFCC derivatives
despite the fact that
state information from many previous frames
is available at the current timestep
(??? cite).
It therefore seems likely that
a sufficiently sophisticated machine learning algorithm
would learn that temporal derivatives
are a useful feature;
incorporating it into the feature vector
is not necessary,
but it effectively bootstraps the learning process
by providing it a feature
that would otherwise
have to be learned.

Temporal derivatives are not the only
MFCC transformation that are done in ASR frontends.
Some systems apply an additional transformation
analogous to low-pass filtering
called ``liftering'' that emphasizes
the lower part of the cepstrum.
However, while we are confident
that these other transforms
can be computed with spiking neural networks,
we limit our model to producing
MFCC and delta-MFCC-like features
in spiking neural networks.

\subsection{Spectral analysis with auditory periphery models}

While the analogy
between the frontend of ASR systems
and the human auditory system
is usually just an analogy,
many systems have experimented with
more physiologically accurate
auditory periphery models
to replace the idealized spectral analysis step
in the feature extraction pipeline.

??? tchorz1999.pdf
??? DimitrioMar...pdf,
??? 10.1.1.298.4746.pdf,
and ??? 10.1.1.151.1803.pdf
have separately proposed variants of MFCCs
that use Gammatone filters
to do the spectral analysis step in an ASR system.
??? tchorz1999.pdf
??? Dimitrio ...
??? 10.1.1.298.4746.pdf ...
and ??? 10.1.1.151.1803.pdf .... ? brief explanation of each.
All three of these studies
found that using Gammatone filters
lowered word or phone error rates
on recognition tasks in which
noise was added to speech samples.
Other studies has achieved similar results;
see ??? ICSIhearingis12.pdf for a review.

While all of these systems
have been applied successfully
for in ASR,
they are not suitable
for inclusion in Sermo in their current form.
None of the methods currently available,
to our knowledge, produce spiking behavior
that could therefore
be easily integrated with the rest of Sermo.
Additionally, these networks compute
functions of the filter output that
may be difficult for neurons to implement.
??? Tchorz, for example,
implements short-term adaption
through loops that perform
a lowpass filter and a division.
Division is, in general,
difficult to approximate with spiking neural networks.
A full neural implementation
may be possible,
but not trivial.

On the other hand,
silicon cochlea models face the opposite problem.
??? more, summarize some papers
These systems produce spikes
that emulate the spikes
traveling down the auditory nerve,
and since they are implemented in hardware,
they run much quicker than software solutions.
However, because they produce spikes,
they have not been used
as the frontend to any ASR systems,
to our knowledge,
and therefore we cannot
be certain that they are useful
for word and phone recognition.

\section{Syllable sequencing and production}

\subsection{Serial working memory}

\subsection{Trajectory generation}

??? note that many art. synths consider this part of their
synthesizer (control model).
But we will consider it separately because
it is a primary contribution of this thesis.

??? Saltzman stuff on task dynamics
is highly related to what we want to do.
But with SPA stuff on top.

??? also hosung nam's work

??? also mention near the end that people haven't
yet connected the Saltzman task dynamics stuff
to the brain; that'll be one of our contributions

\subsection{Articulatory speech synthesis}

??? summary table with different vocal tract models

??? summary table with different acoustic models

??? in the tables, also note available implementations
(so we can justify writing our own).
Include programming language in this

??? include online vs batch in table

\section{Syllable recognition}

\subsection{Trajectory classification}

% \section{Syllable consolidation}
