\chapter{Previous work}
\label{chapt:previouswork}

In this chapter, we review
existing complete or partial solutions
to the three problems
solved by the models
presented in the subsequent chapters.

\section{Auditory feature extraction}

As summarized in Section~\ref{sec:model-ncc},
the auditory feature extraction system
is based on the feature extraction pipeline
used as the frontend
of automatic speech recognition (ASR) systems.
The pipeline pictured in Figure~\ref{fig:asr}
is similar across most systems.

\subsection{Mel-frequency cepstral coefficients (MFCCs)}
\label{sec:prev-mfcc}

The most common feature extracted
and used in ASR systems
is called Mel-frequency cepstral coefficients (MFCCs).
It has been widely used
in both hidden Markov model-based
(HMM; \citealp{hain1999,gales2008,gaikwad2010,alam2013})
and deep learning-based
\citep{graves2006,graves2008,fernandez2008} ASR systems,
including those that achieve
the lowest error rates
on the popular corpus TIMIT
\citep{garofolo1993,lopes2011}.
It has also been shown that
MFCCs are well suited
for both speech and music inputs
\citep{logan2000}.

Mel-frequency coefficients are inspired
by the human auditory system,
in the sense that they perform
frequency decomposition
of the audio signal
in order to determine the relative power
at frequencies distributed over the Mel scale.
A simple algorithm for computing a
Mel-frequency cepstral vector
for a frame of audio is as follows.

\begin{enumerate}
  \item Compute the discrete Fourier transform
    of the audio frame.
  \item Take the log of the power spectrum.
  \item Convolve the power spectrum
    with triangular filters distributed
    according to the Mel scale.
  \item Apply the inverse discrete cosine transform (iDCT)
    to the triangular filter outputs.
\end{enumerate}
The resulting coefficients obtained from
the iDCT are called ``cepstral'' coefficients.\footnote{
  The term ``cepstrum'' comes from
  the word ``spectrum'' with the first four letters reversed,
  as the spectrum is obtained with the Fourier transform
  and the cepstrum is obtained with the inverse Fourier transform.
  Similarly, the domain of the cepstrum is not frequency,
  but ``quefrency.''}

Recall from Section~\ref{sec:psychoacoustics}
that the Mel scale
describes the relationship between
absolute frequency and perceived pitch;
it is a logarithmic function of frequency.
The triangular filters are spaced
equidistantly on the Mel scale,
resulting in more filters
at lower frequencies than at higher frequencies.
Typically, at least twenty triangular filters
are used in order to ensure that all frequencies
are captured in more than one filter
(i.e., there is overlap between adjacent filters).
The inverse discrete cosine transform
is designed to decorrelate the signal,
which results in the same amount
of information being represented
with fewer coefficients;
typically, the first thirteen coefficients
are used in the feature vector
of ASR systems.

\subsection{Delta MFCCs}

In addition to the thirteen MFCCs,
many ASR systems,
both HMM-based \citep{hain1999,gales2008}
and deep learning-based
\citep{graves2006,graves2008,fernandez2008},
also append the first
and (sometimes) second
temporal derivatives of the MFCC
to the feature vector.

The justification for including derivatives
in the feature vector is typically
a practical one,
in that most ASR systems
achieve higher accuracy with
derivative information than without.
Theoretically,
most sources justify time derivatives
by noting that the derivatives
incorporate dynamics into the state representation.
However,
even recurrent deep learning systems
like \citet{graves2008}
use MFCC derivatives
despite the fact that
state information from many previous frames
is available at the current timestep.
It therefore seems likely that
a sufficiently sophisticated machine learning algorithm
would learn that temporal derivatives
are a useful feature;
incorporating it into the feature vector
is not necessary,
but effectively bootstraps the learning process
by providing it a feature
that it would have learned regardless.

Temporal derivatives are not the only
MFCC transformation that are done in ASR frontends.
Some systems apply an additional transformation
analogous to low-pass filtering
called ``liftering'' that emphasizes
the lower part of the cepstrum.
However, while we are confident
that these other transforms
can be computed with spiking neural networks,
we limit our model to producing
MFCC and delta-MFCC-like features
in spiking neural networks.

\subsection{Spectral analysis with auditory periphery models}

While the analogy
between the frontend of ASR systems
and the human auditory system
is usually just an analogy,
many systems have experimented with
more physiologically accurate
auditory periphery models
to replace the idealized spectral analysis step
in the feature extraction pipeline described above.

\citet{tchorz1999,dimitriadis2005,schluter2007,shao2009}
have separately proposed variants of MFCCs
that use Gammatone filters
to do the spectral analysis step in an ASR system.
All four of these studies
found that using Gammatone filters
lowered word or phone error rates
on recognition tasks in which
noise was added to speech samples.
Other studies has achieved similar results;
see \citet{stern2012} for a review.

While all of these systems
have been applied successfully
for noisy ASR tasks,
they are not suitable
for inclusion in Sermo in their current form.
To my knowledge,
none of the methods currently available
produce spiking behavior that could
be easily integrated with the rest of Sermo.
Additionally, these networks compute
functions of the filter output that
may be difficult for neurons to implement.
\citet{tchorz1999}, for example,
implement short-term adaption
through loops that perform
a lowpass filter and a division.
Division is, in general,
difficult to approximate with spiking neural networks.
A full neural implementation
may be possible,
but not trivial.

On the other hand,
silicon cochlea models face the opposite problem.
Silicon cochlea models are hardware implementations
of auditory filter models
designed to interact with other neuromorphic systems,
or to be directly implanted in
patients with hearing loss
in order to partially recover the sense of hearing.
Existing silicon cochlea models include
\citet{chan2007,hamilton2008,wen2009,karuppuswamy2013}
(see \citealt{liu2010} for a review of
silicon cochleas
and other neuromorphic sensory systems).
These systems produce spikes
that emulate the spikes
traveling down the auditory nerve,
and since they are implemented in hardware,
they run much faster than most software systems.
However, because they produce spikes,
they have not been used
as the frontend to any ASR systems,
to my knowledge,
as it is not straightforward to
construct feature vectors
out of spike trains.

\section{Syllable sequencing}
\label{sec:prev-sequencing}

Currently,
we are aware of few existing models
explaining how syllables
might be represented
and temporally sequenced in the brain.
However, there have been many attempts
to solve similar problems
and could be applicable to speech;
specifically,
models of song generation in songbirds
and serial working memory.

A notable omission in this section
is the WEAVER++ model
previously discussed in
Sections~\ref{sec:syll-words}
and \ref{sec:sermo-linguistic}.
While a neural implementation of WEAVER++
would be an important contribution to Sermo,
WEAVER++ does not solve
the syllable sequencing problem.
At its lowest level,
it produces syllable targets over time,
but those syllable targets are not temporally
coordinated with the voicing of
other syllables.
As such, a neural implementation
of WEAVER++ would still require
a syllable sequencing model
like the ones described in this section.

\subsection{Song generation in songbirds}

The avian song system
exhibits remarkable similarities
to the human speech system
(see \citealt{bolhuis2010}).
As such, models of song sequencing
and generation in birds
may be applicable to models of speech.

\citet{troyer2000} presented a model
of birdsong sensorimotor learning
in which song sequencing is broken
into two subproblems:
syllable learning,
in which the system learns
to associate an ensemble of neurons
with a top-down syllable representation,
and sequence learning,
in which the activation
of an ensemble of neurons
associated with a particular syllable
is linked to the next syllable
in the learned sequence.
The end result of the model
is that activating a particular
ensemble of syllable-specific neurons
begins a sequence of activations
representing a particular syllable sequence
associated with a target song.

The model uses a
simple associative learning rule
to learn sensory predictions
of motor representations,
and motor predictions
of sensory representations.
The sequence of syllables is produced
by a motor representation
making a prediction of the
sound that will be produced,
which is associated with
the next motor action to be produced,
which activates a sensory prediction,
and so on.

\fig{troyer}{0.6}{Illustration of model results from \citet{troyer2000}.}{
  Illustration of model results from \citet{troyer2000}.
  Through learning, neurons in avian brain area RA
  activate sequentially,
  which is theorized to underlie song sequencing.
  In the first learning stage,
  neurons that are part of the same syllable
  are associated together (middle).
  In the second learning stage,
  sensorimotor predictions enable
  sequence learning such that
  song syllables sequentially activate
  in the correct order.
  Reproduced from \citet{troyer2000}.}

While the learning method
may be useful for future iterations of Sermo,
the rest of the model is not suitable for Sermo
because it does not allow for
flexible temporal dynamics
in the motor trajectories within and between syllables,
which is one of the hallmarks of human speech.
In \citet{troyer2000},
all syllables are assumed
to take the same amount of time,
and activate all of the neurons
within the ensemble for the entirety
of the syllable activation
(see Figure~\ref{fig:troyer}).
The time taken for each syllable
is not easily modifiable since
it is defined by the amount of time
it takes for the motor action
to activate the sensory prediction
and switch to the next motor action,
which is an intrinsic property
of the synapse connecting the neuron models.

\citet{drew2003} presented a model
which learns to associate specific neural ensembles
to particular sensory inputs,
similar to \citet{troyer2000},
but also to sequences of sensory inputs.
For example, given syllables A and B,
some neurons would activate
when syllable A is presented,
some when syllable B is presented,
and some only when syllable B is presented
immediately following syllable A.
Since the temporal sequence is
coded in the connections between
these ensembles,
\citeauthor{drew2003} hypothesized that
sequences could be generated,
rather than recognized,
by delivering a generic timing pulse
which emulated hearing all possible sensory
inputs at once.
The first time the pulse is delivered,
ensembles sensitive to a single sound
would activate.
On the next time the pulse is delivered,
ensembles representing sequences of length two
would activate, and so on for longer sequences
(see Figure~\ref{fig:drew}).
Ensembles representing sequences of length one
are not active on the second timing pulse
due to the intrinsic properties
of the neurons in the ensembles;
specifically, after spiking
for the previous timing pulse,
they enter a refractory period
in which they become insensitive to further input.

\fig{drew}{0.9}{Illustration of model results from \citet{drew2003}.}{
  Illustration of model results from \citet{drew2003}.
  A timing pulse is used to control when the next element
  in the sequence should be activated.
  See text for more details.
  Reproduced from \citet{drew2003}.}

While this approach is more temporally flexible
than \citet{troyer2000} because
the timing pulse could arrive at any moment,
it only moves the responsibility for flexible timing
from the neurons involved in the sequence
to whatever mechanism generates the timing pulse.
In the paper, the timing pulse is provided
by the experimenter;
they note that the spacing of syllables
can be controlled by varying the frequency
of timing pulses,
but do not provide any mechanism
for generating pulses
or varying the frequency of pulses.
Additionally, the length of the sequence
is extremely limited,
both in terms of the total length of time
and the number of syllables.
The refractory period of each neuron
is only long enough to be insensitive
to the next timing pulse;
therefore, for a sequence of length three,
\citeauthor{drew2003} added inhibitory connections
from sequence-specific ensembles
to non-sequence-specific ensembles.
The connectivity patterns required
for sequences of longer lengths
are not obvious,
and depends on the refractory period,
which can quickly become abnormally long.
We therefore do not think that
this model is useful for Sermo.

While other models for songbird generation exist
(e.g., \citealt{fee2004}),
none exhibit the temporal flexibility
that is required for speech.
As shown in \citet{fee2010},
cooling a part of the avian brain
that projects to motor cortex
results in slowing the trajectory
in proportion to the temperature.
Therefore, birds may not possess
the ability to flexibly time their songs
in the same manner that humans time speech,
meaning we must look elsewhere for
more temporally flexible models.

\subsection{Serial working memory}

We assume that a sequence of syllables
is likely to be represented
in a similar manner
as sequences of other static representations.
Therefore, a useful paradigm for studying
how humans represent syllable sequences
is to investigate serial working memory tasks;
e.g., a subject is asked to remember
a list of numbers,
and then later recall that list,
or elements from that list.
Several models have been proposed
to solve these tasks
in biologically plausible ways.
The most sophisticated such model
is the ordinal serial encoding model
(OSE; \citealp{choo2010}),
which has been incorporated into
Spaun \citep{eliasmith2012}.

The model takes inspiration from
earlier serial working memory models,
namely CADAM, TODAM, and TODAM2
(see \citealt{choo2010} for more details),
which are all based on
vector symbolic architectures
(see Section~\ref{sec:methods-spa}
for details on vector symbolic architectures).
Unlike these models,
\citeauthor{choo2010}'s model
is able to remember and recall
lists of up to seven items,
and exhibits primacy and recency effects,
meaning that items at the beginnings
and ends of lists are more likely
to be recalled.
\citeauthor{choo2010}'s model is also implemented
in a spiking neural network,
making it applicable to Sermo.
Interestingly,
the OSE model does not exhibit
the primacy and recency effects
that are seen in humans
unless the model
is implemented in neurons;
direct simulation of the differential equations
behind the model show
poor performance for the second item
even though it should be easily remembered
due to primacy.

The ordinal serial encoding model,
then, is well suited for representing
discrete sequences of syllables in Sermo,
which typically have few elements
if we assume that prosodification
operates only slightly faster
than speech production.
However, the model is limited to
discrete sequences,
and therefore cannot be used
for detailed trajectory generation.
Additionally,
while the model is temporally flexible,
in that it can be queried for
a list element at any time,
there is a slight lag between
when the next element is queried
and when it has been recalled.
Syllables, however, are voiced in quick succession,
sometimes even blending into one another slightly.
In Sermo, we will extend the OSE model
to do syllable sequencing
(see Section~\ref{sec:impl-prod-neuralmodel}).

\section{Trajectory generation}

In trajectory generation,
we must generate a sequence
of production information
corresponding to a syllable target.
We will review two robust trajectory generation techniques,
one primarily applied to speech
(Task Dynamics),
and one used for general motor control
(REACH).

\subsection{Task Dynamics}

A line of research encompassing work by
several investigators at Haskins Laboratories
developed a set of techniques
under the name Task Dynamics
for generating temporal sequences
of production information
and using that information to drive
an articulatory synthesizer
\citep{saltzman1989,nam2004}.

The key insight in Task Dynamics
is to dissociate the task state
from the motor trajectory
along that state.
By doing this, the dynamics of the task state
can be considered separately from motor trajectories,
while trajectories can be implemented
as a function of the task state.
In the initial publication of task dynamics
\citep{saltzman1987},
two types of task dynamics are presented.
Point attractor dynamics,
modeled by critically damped mass-spring systems,
are useful for one-time actions.
Cyclic attractor dynamics,
modeled by harmonic oscillators
with a nonlinear escapement function,
are useful for repetitive actions.
Initially, these task dynamics were
related to articulator trajectories
for a two degree-of-freedom arm model
by mapping the task space
to body-centered coordinates,
joint coordinates,
and finally articulator positions,
called the task network.
In a subsequent publication,
\citet{saltzman1989}
applied task dynamics to speech production
using a simplified mapping,
from task space
to gesture space,
and then to articular space,
similar to that described in Sermo
(see Section~\ref{sec:model-sm}).
Realistic articulatory trajectories
were achieved
by associating a task dynamic network
with point attractor dynamics
for each speech gesture.
Each gesture defines
the point which the task dynamic network
is attracted to.
The state of each gesture dynamical system
influences one or more articulator positions.
The final articulator trajectory
is the combined effect
of all of the gesture systems
on all of the articulators.

Given a complete gesture trajectory
for an utterance,
the task dynamic approach
generates articular position trajectories
which can be synthesized
by the CASY synthesizer \citep{iskarous2003}.
While the Task Dynamics model
for inter-articulator coordination has not been
modified significantly since its introduction in 1989,
the same group has done significant work
in automatically generating gesture scores
with a similar approach.
In this extension,
instead of knowing \textit{a priori}
when each gesture should occur,
the point attractors associated
with each gesture are coupled
to one another,
such that the timing of each gesture
is controlled by the state
of the gestures to which it is coupled
\citep{saltzman2000}.
Task dynamic gestural timing
is able to capture precise timing
in syllables with complex onsets
and codas \citep{nam2003,goldstein2006}.
Perhaps more importantly,
it allows for the articulatory synthesizer
CASY to accept orthographic text as input,
which is converted to a gesture score
by looking up a syllabification
of the word in a database
and using gestural coupling rules
defined by linguists
to create a system of coupled oscillators
whose activities represent a gestural score
\citep{nam2004,goldstein2009}.

In all, the task dynamic approach
to inter-gestural and inter-articulator timing
is the most temporally flexible
syllable production system
currently published
(to my knowledge).
The dynamical systems approach
also makes it a natural fit for Sermo,
as it is defined in continuous time,
and should be readily implementable
in spiking neural networks,
though we are not aware of any
neural implementations currently available.

\subsection{REACH}

The final model informing
Sermo's trajectory generation model is the
Recurrent Error-driven Adaptive Control Hierarchy (REACH)
model \citep{dewolf2015}.
The REACH model is a general motor control
model implemented in spiking neurons.
It uses dynamic movement primitives (DMPs)
to generate trajectories
for the system to follow;
these trajectories are mapped into
motor space using operational space control,
and unexpected changes in system dynamics
are accounted for online
using nonlinear adaptive control
\citep{slotine1987}.
The model is able to control
a nonlinear three-link arm model
in handwriting and reaching tasks,
even when an unknown force field
is applied to the end effector.

DMPs are a general method for generating motor trajectories,
and are similar to Task Dynamics in many respects.
Both define methods to generate trajectories
for one-time and rhythmic actions.
Both use point attractor dynamics
for one-time actions
and cyclic attractor dynamics
for rhythmic actions.
Both dissociate the temporal dynamics
of the task from trajectories
in motor space,
allowing them to advance the system state
at variable rates,
and compute the trajectory
as a function of the system state.

The primary difference between DMPs and Task Dynamics
is that Task Dynamics generates
the trajectory as a function
of the system state,
while DMPs generates the trajectory
as the system state
plus a separate function
of another dynamical system.
Dissociating the system state
from the nonlinear task-specific function
makes DMPs more flexible and general.
Another important difference for Sermo
is that DMPs have been implemented
in spiking neural networks successfully
\citep{dewolf2015}.
We will explain DMPs in more detail
in Section~\ref{sec:methods-dmp} and present a model
using rhythmic DMPs for syllable production
in Section~\ref{sec:impl-prod-neuralmodel}.

\section{Syllable recognition}

Syllable recognition, in general,
is a task that can be solved
by most ASR systems
using a labeled acoustic data set.
In the sensorimotor integration system
in Sermo, however,
we aim to recognize syllables
based only on production information
which is decoded from acoustic information.
It should be noted that we do not expect
this system to perform perfectly,
as the dominant speech decoding system
will be linguistic;
however, humans are nevertheless able
to recognize infrequent syllables
and voice them.
The ability to differentiate
between frequent and infrequent syllables
may depend on whether they
can be classified
on the basis of production information.

To my knowledge,
the only attempt to recognize speech
based solely on production information
is \citet{mitra2014}.
\citeauthor{mitra2014} were able to
decode continuous production information
from auditory information
using MFCCs and Gammatone filter-based
cepstral features \citep{mitra2012},
and were able to use a combined feature vector
consisting of MFCCs and production information
to lower word error rates
in various noisy environments.
However, word error rates
when using only continuous production information
as the feature vector
were high ($\sim$70\% with no noise).

The apparent difficulty
in recognizing words based on
production information alone
could be due to several factors.
For one, the details of the decoding mechanism
in \citet{mitra2014} are not clear.
The authors note that they use a
deep neural network
with as many as six hidden layers;
however, the choice of neural activation function,
optimization procedure,
and many other hyperparameters
can affect how well the network learns.
In particular, it does not seem as though
the network has recurrent connections,
which are commonly used in
current state-of-the-art ASR systems.

Alternatively, the statistical approach
used in most ASR systems
may not be well suited to
trajectories of production information.
In order to broaden our search
for other types of techniques,
we investigated general solutions
for trajectory classification
which are used in applications
such as gesture recognition
and automatic video analysis.

\subsection{Trajectory classification}
\label{sec:prev-classification}

Unsurprisingly, many of the existing
solutions for trajectory classification
are based on generative statistical models,
particularly Hidden Markov Models (HMMs),
as has been dominant in speech recognition
(see \citealt{mlich2008,nascimento2010}
and \citealt{mitra2007,weinland2011,rautaray2015}
for reviews).
Given the failure of the statistical approach
in \citet{mitra2014},
we focus here on systems
that recognize trajectories
through tracking the trajectory
either in comparison to some known template,
or as the state in a dynamical system.
We identify three such systems
that provide inspiration
for the trajectory classification model
described in Section~\ref{sec:impl-recog}.

\citet{kiliboz2015} proposed
a gesture recognition system
for 2D trajectories
using a finite state machine approach.
Finite state machines are composed of
discrete states and a set of state-specific
functions that transition between states.
During a learning phase,
a target trajectory is played
several times,
and one or more finite state machines
are constructed such that
the target trajectory
causes the state machine
to transition to a final accepting state.
During recognition,
the continuous input trajectory
is presented to all finite state machines
in the system;
the first to reach the accepting state is recognized.
The system attains a 73\% accuracy rate
in a real-world user study.
As the system operates continuously online,
part of the system is applicable to Sermo;
however, the discrete state space and
gesture sequence representation
cannot be easily adapted to speech.
Each timestep in the gesture trajectory
is represented by a string denoting whether the
end effector has moved significantly
in the $x$ or $y$ direction since the last frame;
the overall trajectory is represented
by a regular expression generalized from
the strings seen in the learning phase.
It is not clear that this representation scheme
would scale to $n$-dimensional spaces,
as is required for production information trajectories.

\citet{quiroga2013}
proposed a system called competitive neural classifiers (CNC)
that can recognize hand gestures
using small training sets (three examples per gesture).
CNC uses a collection of sub-classifiers
that compete in order to collectively
classify the overall trajectory.
The trajectory is segmented
into a set of subtrajectories,
each of which is evaluated by
a sub-classifier neural network
whose output neural activations
represent the probability that
the subtrajectory is produced by
the gesture associated with that output neuron.
The overall classification aggregates the results
across of all sub-classifiers,
producing the correct classification
in 98\% of test cases.
However, the system's impressive results
are partly due to an elaborate preprocessing step
in which a recorded trajectory
is normalized and resampled
such that the actual trajectory used as input
has a fixed number of sample points
uniformly distributed over
the total length of the trajectory
(see Figure~\ref{fig:quiroga}).
This type of preprocessing
requires knowledge of the whole trajectory,
and therefore could not be implemented
in an online manner.
Additionally, the operations done
on the neural network outputs
are difficult to implement
with spiking neural networks
(e.g., division, argmax),
making this system unsuitable
for use in Sermo.

\fig{quiroga}{0.55}{Resampling in \citet{quiroga2013}.}{
  Example of the resampling procedure in \citet{quiroga2013}.
  The original trajectory (left) has more sample points
  at the beginning and end of the trajectory.
  The resampled trajectories (middle, right)
  have sample points distributed to maximize the probability
  of a successful classification,
  which cannot be done in a continuous online system.
  Adapted from \citet{quiroga2013}.}

Finally, \citet{caramiaux2014}
presented an algorithm called the
Gesture Variation Follower (GVF),
based on an online HMM-based
trajectory tracking technique
called Gesture Following (GF)
\citep{bevilacqua2010,bevilacqua2011}.
The goal of the algorithm is to match
an input gesture sequence
with a set of predefined template gestures.
Unlike the HMM-based GF algorithm,
GVF views the trajectory as a dynamical system,
and uses a particle filtering algorithm
to learn a set of weights
that denote the importance of
each randomly generated particle
(i.e., dynamical system state)
to overall recognition accuracy.

The overall flow of the algorithm
is as follows.
First, a set of predefined example trajectories
are defined in the system.
Second, a set of particles
are randomly generated
in an $n$-dimensional space
with a uniform distribution,
and a set of weights are initialized
with each particle weighted equally.
In the main loop of the algorithm,
a random sample is drawn
according to each particle's position
in state space,
and the weights associated with each particle
are updated based on the distance
between the particle's sample
and the observed input.
If too many particles have small weights
associated with them,
then particles are resampled
based on their current weights.

What differentiates this algorithm from
other particle filtering algorithms
is the structure of the state space.
Briefly, the state space that each particle is in
encodes the target gesture that this particle
is associated with,
and a probability distribution
over the phase of the trajectory
(i.e., how far along the trajectory
we currently are)
and the speed of the trajectory
(i.e., how far along the trajectory
do we expect to be on the next timestep).
The state evolves over time according to
predefined state transition functions,
and on each timestep,
each particle emits an observation
according to a possibly nonlinear function.

A useful analogy for this algorithm
is that it implements an online version of an HMM.
Like an HMM, it maintains some internal representation
that can be used to estimate the probability
of a particular sequence
by forming a prediction
of the next observation.
The system is queried by providing
an observation,
which changes the internal representation
such that the next observation
is processed in the context of
the sequence of past observations.

GVF has been used successfully
in music generation systems
in which the music sample,
volume, and speed are
controlled by gestures
that are tracked by the GVF algorithm.
It achieves over 98\% accuracy
in a 2D gesture recognition task
with 16 possible gestures,
and was employed successfully
in a 3D hand gesture user study
with 10 participants
\citep{caramiaux2014}.

While GVF is one of the most promising
algorithms for doing trajectory recognition
in Sermo,
it has two main limitations in its current formulation.
First, while the classifier can be used online,
it is formulated in discrete time;
this weakness should be possible to overcome,
however, as continuous time particle filtering
has been done in the past
\citep{ng2005}.
Second, while the dynamical system
at the core of GVF should be
implementable in spiking neurons,
the resampling procedure is likely not.
On each step of the algorithm,
each particle represents a probability
distribution over the system state,
which is sampled
(sampling has been shown to be
possible with spiking neurons;
see \citealt{buesing2011}).
However, when the importance weights
of a sufficient number of particles
are below a particular threshold,
a new set of particles
are randomly generated
to replace those with lower importance weights.
This procedure would translate to
a significant reorganization
of the neurons implementing
the sampling procedure,
which I believe
would not have evolved if
procedures that did not require
reorganization existed.

I will propose a trajectory classification technique
in Section~\ref{sec:impl-recog}
that shares many of the positive
characteristics of the GVF algorithm,
but can be implemented in a spiking neural network.
Like GVF, it is also based on the idea
of inferring internal dynamical system state
based on observations;
however, it performs the inference
in a manner that we relate to
DMPs (see Section~\ref{sec:impl-recog-overview}).
