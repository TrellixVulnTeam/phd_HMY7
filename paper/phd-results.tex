\chapter{Evaluation and results}

%% ~15-30 pages

%% - adequacy, efficiency, productiveness, effectiveness
%%   (choose your criteria, state them clearly and justify them)
%% - be careful that you are using a fair measure, and that you are
%%   actually measuring what you claim to be measuring
%% - if comparing with previous techniques those techniques
%%   must be described in Chapter 2
%% - be honest in evaluation
%% - admit weaknesses

% For each model, have an initial section where subparts are
% tested to determine parameters for the full experiments

Here,
we present evaluation metrics for the three models
described in Chapter~???,
gather those metrics
for several experimental conditions,
and present the results.

\section{Neural cepstral coefficients}

We hypothesize that
neural cepstral coefficients (NCCs)
are a suitable feature vector representation
for traditional ASR systems,
and biologically grounded systems like Sermo.
In order to evaluate
its suitability for speech recognition tasks,
we compare NCCs to the most common
feature vector used currently,
Mel-frequency cepstral coefficients.

\subsection{Evaluation}

While different ASR systems
use the feature vector in different ways,
the most important quality of the feature vector
is that audio samples corresponding
to a particular label
(e.g., a phone or word)
takes a similar trajectory
in the feature vector space
as other sample corresponding to the same label.
All samples corresponding to other labels
should take trajectories that are
far enough away in the vector space
to provide a basis for labeling
a test sample correctly.
As such, we compare MFCCs and NCCs
on a purely statistical level,
rather than using them as a frontend
in an ASR system,
which would bias the results
toward the aspects of the
feature vector trajectories
that the ASR system's backend
is most suitable for detecting.

We therefore compare MFCCs and NCCs
using a linear support vector machine (SVM)
that is trained on labeled
examples of feature vector trajectories
corresponding to phones
in the TIMIT training corpus,
and tested on trajectories
corresponding to phones
in the TIMIT testing corpus.

TIMIT ??? cite is a corpus of read speech
with time-aligned phone and word transcriptions.
The corpus contains ten speech samples
from 630 speakers across eight dialects
of American English,
resulting in 6300 utterances in total
(5.4 hours of continuous speech).
The utterances are separated into
a training set
(3696 sentences; 3.14 hours of speech)
and a testing set
(1344 sentences; 0.81 hours of speech),
allowing for apples-to-apples
comparisons of ASR systems.
TIMIT was released in 1990,
and is still a standard data set used
for testing ASR systems
due to the high quality
manual phone transcriptions,
diversity of speakers,
and convenient size,
as it is small enough to be
computationally tractable
but large enough to
provide a meaningful comparison
between ASR approaches.

We will focus on classifying
phones in the TIMIT data set.
Phones are voiced
for a shorter time than words,
and have high variability
as phone sounds can be affected
by the previous and next phones voiced,
making phone classification
a more difficult task than word classification;
(??? word error rate vs phone error rate?).
Since we are not solving the full
speech recognition problem,
and are instead classifying
pre-segmented speech samples,
we anticipate that word classification
would be too simple to meaningfully
compare MFCCs and NCCs.

Additionally, classifying phones
is advantageous because phones
are voiced for a similar length of time
compared to words.
Another way in which our statistical comparison
differs from the full speech recognition problem
is that we must stretch
feature vector trajectories
such that all trajectories are the same length,
due to the constraints of SVM classifiers;
therefore,
we aim to minimize the amount
of stretching done.

We stretch feature vector trajectories
to be as long as the longest trajectory
in the training and testing samples
using simple linear interpolation.
An alternative method
for stretching the trajectory
would be to use dynamic time warping
??? cite DTW paper.
However, dynamic time warping
involves aligning each speech sample
to a representative example,
which would necessitate manually
determining a suitable example
for each phone for each experiment.
Additionally, the manual effort may not
provide any concrete benefits;
??? showed that linearly interpolating
time series data
produced classification accuracy rates
statistically indistinguishable
from accuracy rates on time series data
aligned using dynamic time warping.

TIMIT defines a set of 61 phones,
which include vowels, consonants,
consonantal closures, pauses and silences.
??? leehon.pdf identified
allophones and
other situation where two phones
are used interchangeably,
collapsing the full set of phones
to a set of 39 phones,
which are typically used
instead of the full set.
Many studies improve error rates
by further categorizing
the phones with similar properties;
e.g., grouping together all
plosives, fricatives, etc.
??? cite InTech.
We will use the reduced set of 39 phones.\footnote{
  The set of 39 phones includes a ``silence'' phone,
  which
}
Because vowels are typically voiced
longer than consonants,
we will investigate whether
separating them is necessary
since we lengthen all samples
to the longest sample.

Support vector machines are a standard
supervised learning technique
often applied to classification tasks,
and are particularly well suited
to high-dimensional data.
The SVM algorithm learns a hyperplane
that separates data
in one class from other classes,
which it can subsequently use
to classify data
by evaluating where the data point
lies in vector space compared to the hyperplane.
When dealing with more than two classes,
hyperplanes can be learned to separate
each combination of two classes
(``one-against-one'' classification)
or each class from all other classes
(``one-vs-the-rest'' classification).
When data is relatively low dimensional,
the kernel trick can be applied,
which projects the input data
into a higher dimensional space
based on a kernel function.

In the case of speech feature vector trajectories,
we have relatively many samples,
classes and features,
so we use a one-vs-the-rest
support vector machine classifier
that does not use the kernel trick
(i.e., a linear SVM).
We use LibLinear's fast implementation
of linear SVMs (??? cite linblinear.pdf),
exposed to Python through
the scikit-learn package
(??? cite scikit-learn).  % http://scikit-learn.org/stable/about.html#citing-scikit-learn

The metric that we collect
in the experiments below
is classification correctness,
either in absolute terms,
or NCC classification correctness
relative to the baseline correctness
obtained with NFCCs.
In the ASR literature,
phone error rates are often
reported on TIMIT.
The phone error rate
takes into account
incorrect classifications
(substitutions),
a lack of classification
when there should be one
(deletions)
and erroneous classifications
where there should not be one
(insertions).
However, since we pre-segment
the speech samples into
those that correspond to some phone,
we cannot have deletions or insertions,
only substitutions,
which corresponds to classification correctness.
Since we use correctness
rather than accuracy,
we can only compare our approach
to some reported results
??? cite InTech.pdf,
but even then the comparison is flawed.
Hence, we generate both MFCCs
and NCCs for the same speech samples
in order to verify that
NCCs generate feature vector trajectories
that are at least as statistically separable
as the equivalent MFCC trajectories.

\subsection{Experiments and results}

In each experiment,
we isolate the TIMIT speech samples
corresponding to the phones
of interest,
and generate a feature vector trajectory
(i.e., MFCC or NCC)
for that sample.
MFCCs are generated using frames
of 25 ms,
which advance 10 ms on each timestep,
as is standard in many ASR systems.
NCCs are generated
by running the model described in
Section~??? with a 1 ms simulation timestep.
Because NCC generation takes several minutes,
and because the SVM fitting procedure
is quadratic in the number of samples,
we limit our experiments to
one region of the TIMIT dataset
(the ``army brat'' region,
which consists of a training set
of 220 utterances across 22 speakers,
and a test set of of 110 utterances
across 11 different speakers),
but run each experiment
10 times with different random seeds
to generate confidence intervals
for each experimental condition.
In the MFCC case,
the feature vectors are the same
on every trial;
the random seed controls the
stochastic nature of
the supervised learning procedure
used in the SVM.
In the NCC case,
the random seed affects both
the generation of model parameters
that are sampled from random distributions,
and the random seed used by the SVM.

All trajectories are
made to be the same length,
as is required by the SVM algorithm.
The number of frames used
for all samples is
the number of frames in
the longest MFCC trajectory
in the training set.
Samples that are too short
(e.g., most MFCC trajectories)
are lengthened to match the
target number of frames
using linear interpolations.
Samples that are too long
(e.g., all NCC trajectories)
are shortened to match the
target number of frames
by splitting the trajectory
into equally spaced time bins
and taking the average
for each bin.
See Figure~??? for
an illustration of temporal scaling,
and example MFCC and NCC trajectories.

??? figure: scaling; plot a bunch of
pcolormeshes of the same phoneme
in a set, some MFCCs lengthened,
some NCCs shortened

Each trajectory is then flattened
to produce a vector of length
$n_{\text{cepstra}} \times n_{\text{frames}}$.
Each trajectory is associated with
the phone label associated with
the original speech sample,
producing an input-output pair
that can be used to do supervised learning
in the linear SVM.

Finally, the same procedure is done
with the test set to produce another
set of input-output pairs.
The input is fed to the linear SVM
to generate predicted phone labels,
which are compared to the actual phones
to yield a correctness measure.

Wherever not specified,
the parameters used for each model
are as listed in Table~???.

??? table of default parameters

\subsubsection{Derivatives and z-scoring}

Two important choices for a feature vector
are whether to include derivatives,
and whether to normalize the resulting vector.
Typically, ASR systems include at least
one derivative, and perform normalization
through z-scoring
(i.e., removing the mean
and normalizing to unit variance).
We investigated whether these
choices were important for NCCs.

??? z-score figure

Figure~??? shows
the training and testing accuracy
for ten trials
with and without z-scoring.
For MFCCs,
z-scoring greatly reduces the variance
of the training and testing accuracy;
the mean accuracy values
are indistinguishable for
vowel phonemes,
and may be slightly lower
for consonant phonemes.
For NCCs,
z-scoring sigiificantly
raises training accuracy,
but lowers testing accuracy
(this trend is similar for MFCCs,
but is obscured by the
highly variable results
when not z-scoring).

We interpret these results
as meaning that z-scoring is essential
for MFCCs because without it,
the SVM fitting procedure
varies significantly for
different random seeds
(hence the high variability without z-scoring).
NCCs, on the other hand,
have low variance in either condition.
Since a higher test accuracy indicates
better generalization,
we believe that z-scoring
does more harm than good for NCCs.
Therefore, for future simulations,
we will z-score the MFCC feature vector,
but not the NCC feature vector.

??? derviative figure

Figure~??? shows
the same experiment
varying the number of derivatives used
while always z-scoring MFCCs,
and never z-scoring NCCs.
Note that using derivative means
that the feature vector has
length 26,
where the first 13 elements
are cepstral coefficients,
and the last 13 are derivatives
of the cepstral coefficients;
using two derivatives means that
the vector has length 39,
with the derivative of the
derivative of the cepstral coefficient
appended to vector with 26 elements.

For MFCCs, adding a single derivative
improves testing accuracy
by almost double
(??? 0deriv to 1deriv),
but adding a second derivative
has very little effect.
For NCCs, adding derivatives
improves training accuracy,
but slightly reduces testing accuracy.

??? derivative time figure

Unlike with z-scoring,
we believe it is important
for the MFCC and NCC features to
be the same length,
so for subsequent experiments,
both feature vectors
will contain the first derivative
(and therefore have length 26).
We use one derivative
because it puts MFCCs and NCCs
on nearly equal footing;
MFCCs are greatly improved
with derivatives,
while NCCs are only slightly worsened.
Adding derivatives
has a significant effect
on the amount of time taken to
generate NCCs
(see Figure~???),
so we add only one derivative
to keep the experiments tractable.

Note that in both of these experiments,
it is accuracy is generally higher
for vowel phones versus consonant phones.
Therefore, in subsequent experiments,
we will test only with consonant phones
as they should pose a greater challenge
to the model.

\subsubsection{Number of neurons}

Ideally,
we would like to reduce the amount of time
that the NCC generation process takes.
As shown in Figure~???,
generating NCCs for all
of the speech samples is expensive.
Reducing the number of neurons
used in the NCC model
can speed it up,
but at the cost of performance.
In these experiments,
we vary the number of neurons used
in different layers of the model
in order to see their impact
on speed and performance.

??? figure: periph figure

Figure~??? shows the results of
varying the number of neurons
in the auditory periphery layer.
In this layer,
each auditory filter projects
to an ensemble of neurons
of the size varied in this experiment.
Since the representation here
is a simple one
(a positive scalar value),
the number of neurons required
should not be very large,
so we vary between
1 and 32 neurons with steps
at each power of two.
The number of neurons used
in the feature layers
is fixed at 20 per feature.
Instead of plotting the absolute accuracy,
in this and subsequent experiments,
we plot the accuracy of the NCC model
relative to the mean accuracy
of the same number
of MFCC model instance accuracies.

As seen in Figure~???,
training accuracy
does not change significantly
whether we use
1 or 32 neurons neurons per filter,
thought variance is higher
with fewer neurons.
Testing accuracy,
however, improves up to
using 8 neurons per filter,
but then plateaus,
or may even decline
(possibly due to overfitting).
Therefore, in all
subsequent experiments,
we use 8 neurons to represent
the output of each auditory filter.

??? Figure: feature figure

Figure~??? shows the results
of varying the number of neurons
in the feature layer,
which includes the ensembles
representing the cepstral coefficients,
the ensembles representing
the derivatives of those coefficients,
and any additional ensembles required
to compute the derivative.
The number of neurons for each filter
in the periphery is fixed at 8.

A very different trend is evident
when varying the number of neurons
used in the feature layer.
For very small number of neurons,
training accuracy is worse
than MFCC training accuracy.
As the number of neurons increases,
accuracy improves up to
using 8 neurons per ensemble,
then steadily worsens.
Testing accuracy, however,
is highly variable until
we use 8 neurons per ensemble,
at which point performance
steadily increases up to
the highest number of neurons tested,
64 neurons per ensemble.

??? Figure: feature times

As with the number of derivatives,
the number of neurons in the feature layer
has a big impact on the amount of time
each experiment takes;
see Figure~???.
The time taken for each trial
is the same up to 12 neurons per ensemble,
at which point
the time taken sharply increases.
Since the accuracy using 12 neurons per ensemble
is significantly higher than
the mean accuracy of the equivalent MFCC feature vector,
we believe that using 12 neurons per ensemble
is sufficiently accurate,
and so in subsequent experiments,
we fix the number of neurons
in the feature layer to contain
12 neurons per ensemble.

\subsubsection{NCC averaging}

Before the eventual goal of
varying the auditory periphery model used,
we address concerns about the validity
of our comparison.

One possible concern is that
the NCC model runs at a lower timestep,
giving it some numerical advantages
over the MFCC model;
similarly, the shortening and lengthening
procedures may affect the results.
To examine the validity of these concerns,
we set up an experiment
with a fixed number of frames
per audio sample,
and varied the amount that the frame window
advanced on each timestep.
When the frame window moves at the
same rate as the Nengo simulation timestep,
the two models are manipulated
in the same way by the
shortening algorithm,
which averages over a time windo.

??? Figure: timewindow

Figure~??? shows the consonant accuracy
as a function of the
frame window advance in seconds.
While using a lower timestep results in
significantly better training accuracy,
testing accuracy is actually better
with a longer timestep,
indicating that the increased temporal
resolution may be causing overfitting.
It is possible that this overfitting
also occurs with the NCC model,
but running with a high simulation timestep
would result in an abundance of
synchronous spiking activity
and lowered decoding accuracy.

\subsubsection{Phone groups}

Finally, to this point we have
evaluated all of the experiments using
the consonant phones
because we believe that they are
more difficult than vowel phones.
However, it may be the case that
using all phones together
is a more advantageous situation
for MFCCs compared to NCCs.
For that reason,
we compared the relative accuracy
of NCCs when using
vowel phones, consonant phones,
and all phones together
(including the silent phone).

??? Figure: phones

Figure~??? presents the results.
All conditions have relative accuracy
significantly above one,
meaning that NCC features performed
better than MFCC features.
Additionally, ??? more

??? figure: time taken

As seen in Figure~???,
using all phones increases the
amount of time to run experiments significantly.
Not only do NCCs take a long time to generate,
the increased number of samples
causes the SVM fitting procedure
to take a long time
for both MFCCs and NCCs.
Interestingly, fitting for NCCs
takes ??? almost half
the time as fitting for MFCCs,
despite the number of samples
and number of features being identical.
This result indicates that it is
easier to find linearly separating hyperplanes
for NCCs than MFCCs.

In sum, we believe that our choice to
only test with consonant phones
was justified,
and results in the closest
comparison between MFCCs and NCCs.

\subsubsection{Auditory periphery model}

This final experiment varies the auditory periphery model
used at the beginning of the NCC pipeline.

6 models, with and without outer/middle ear, and with and without adaptive LIF;
18 conditions in all

\subsection{Scaling}

Give main metrics of the network (number of neurons, etc)
and discuss scaling

\subsection{Summary}

??? main results

\section{Syllable sequencing and production}

The goal of the syllable sequencing and production model
is to generate a continuous trajectory
of production information
that can be used to control
an articulatory synthesizer,
given a static representation
of a sequence of syllables.
As the quality of generated speech
is difficult to quantify,
we evaluate this model
by generating a gesture score
for the decoded production information trajectory,
and comparing it
to the gesture score that was provided
to the model as input.

\subsection{Evaluation}

While the syllable sequencing and production model
can be used to control any articulatory synthesizer,
we use the VocalTractLab synthesizer
because of its high quality speech
and available API ??? cite.
The vocal tract gestures representation
in VocalTractLab is based on a set of
predefined articular positions.
Most gestures correspond to
a particular shape,
and the overall vocal tract shape
is based on linearly interpolating
between the shapes of
all currently active gestures.
Taking into account all of the
possible shapes and glottal parameters,
there are 48 possible vocal tract gestures
(see Table~???) despite having only
??? articulator sets
controlling 22??? articulators.\footnote{
  There is also an f0 gesture which is not included here,
  as it controls the pitch of the utterance.
  Since German is not a tonal language,
  the pitch is not required to voice a syllable,
  and instead is used for generating
  varied prosody;
  we do not consider prosody in this model,
  though including it would be straightforward.}

??? table of vocal tract gestures

As discussed in Section~???,
the input to the model
is a semantic pointer
representing a sequence of syllables.
Each syllable has an associated DMP
which is generated using
a gesture score corresponding to
a German syllable;
the gesture scores were provided by
Bernd Kr\"{o}ger and are available at
\url{http://www.phonetik.phoniatrie.rwth-aachen.de/bkroeger/research.htm}.\footnote{
  These German syllable gesture scores are among
  the few publicly available gesture scores.
  We were not able to find any gesture scores
  for English syllables.}
Each gesture score in VocalTractLab
is a collection of
gestures that are parameterized by target value,
onset time, gesture length,
and a time constant that determines
how quickly the gesture becomes active.

For each experiment described below,
we chose a random set of syllables
as the frequent syllables
that make up the vocabulary
of the model
(i.e., syllables in the mental syllabary).
A sequence of syllables of a given length
are randomly chosen and assigned a speed
sampled from $\mathcal{U}(1, 6)$ Hz,
which corresponds to syllable trajectories of
167--1000 ms in length.
Depending on the number of syllables
and the randomly sampled lengths,
in each trial we end up with a syllable sequence
generating several seconds
of production information trajectories,
which in the case of these experiments,
are 48-dimensional trajectories
where each dimension is a gesture.

The model is provided the static syllable sequence input,
and is run for the amount of time that the sequence
should take to voice (determined by the
randomly assigned syllable speeds),
plus a small amount of time to account
for variability in each model instance.
The output of the temporal output associative memory
is recorded.

That output is then converted into a gesture score
by estimating the gesture parameters
from the associative memory output.
Individual gestures are determined by
taking the absolute value of the temporal derivative
across each gesture;
time slices which have high derivatives
are indicative of the onset or offset of a gesture.
We make a new gesture for each
time slice with high derivative
with the beginning of the gesture as
the beginning of the first time slice,
and the end of the gesture
as the end of the next time slice;
the last time slice ends at the end
of the utterance.
The time constant associated
with the gesture
is the length of
the onset slice with high derivative.
The value of the gesture is the
average value between the two
time slices with high derivatives.
Every gesture has a neutral value
representing that the gesture
is not active;
identified gestures with values
sufficiently close to the neutral gestures
are removed from the gesture score.

??? figure with zoom-in showing start and end of gesture

The gesture score resulting from the model simulation
is then compared to the original gesture score
on three criteria.
First, we calculate the accuracy
of the reconstructed gesture score
with the same equation as phoneme classification accuracy;
specifically,
\begin{equation}
  \text{Acc} = \frac{N_g - S - D - I}{N_g},
\end{equation}
Therefore, we must find the number of
substitutions, deletions, and insertions
in our gesture score compared to
a gesture score composed of
the original gestures scores
for the target syllables
concatenated together.
In order to determine the number of
insertions, deletions and substitutions,
we first convert the original
and reconstructed gesture scores
to a sequence of characters
by assigning a character to each gesture
and determining their order
within each articulator set,
then concatenating them.
Neutral gestures are recorded
and denoted with the ``\texttt{0}'' character;
numerical gestures are assigned
a digit from \texttt{0} to \texttt{9}
depending on the relative value
in the gesture's range.
We then do a global pairwise alignment
of the string determined
from the target gesture
and the reconstructed gesture,
which is then evaluated
according to equation~???.
In order to evaluate whether
accuracy scores are good
(since we know of no comparable
speech trajectory generation techniques)
we will compare the accuracy
of the reconstructed gesture scores
to a baseline calculated as
the mean accuracy of all the
non-matching syllables in the
repository of syllables.
In other words, we will calculate
the accuracy of all combinations
of syllables in the syllable repository
(except for matching syllables)
and set the baseline as the mean
of those accuracy measures.

Since the accuracy measure only tells us
that the gesture score contains
the right gestures in the right order,
we also aim to quantify
how the temporal characteristics
of the reconstructed gesture score
differ from the target gesture score.
For each correctly aligned gesture
in the alignment computed
for the accuracy measure,
we record the difference
between the gesture duration,
and report both the mean and variance.
The mean tells us about
overall temporal difference,
indicating that gesture durations
differ in predictable ways;
e.g., a high mean difference
indicates that
the reconstructed gesture score
is slower than thee target gesture score,
but may still be able to
produce good speech.
High variance, however,
indicates that
gesture durations vary in unpredictable ways,
which is likely to be detrimental
to natural speech.

It is critically important
for speech that certain gestures
either begin or end at the same time;
we will call these co-occurring gestures,
though they may end or start at different times.
Therefore, for each pair
of co-occurring gestures in the
target gesture score,
we record the difference
in the reconstructed gesture score's
corresponding co-occurrence pair,
if those gestures exist.
We report the average absolute
difference between the start or end times
of reconstructed gestures
that co-occur in the target gesture.

Finally, we synthesize a speech sample
using VocalTractLab,
and perform a qualitative evaluation
of its intelligibility.
Speech corresponding to the
original syllable sequence is compared
to that generated by the model.
We do not obscure the label
of the synthesized speech,
as this measure is reported primarily
to reinforce the other two measures;
it is not the primary measure
by which the model should be judged.
As such, we evaluate each sample
as either completely silent,
unintelligible, partially intelligible,
or intelligible.

\subsection{Experiments and results}

??? Hyperopt the main parameters: nneurons

??? how much can we crank up the syllable speed?

??? Does adding more syllables to the vocabulary affect anything?

??? How long can the sequences get before things break down?

\subsection{Scaling}

% Give main metrics of the network (number of neurons, etc)
% and discuss scaling

% - Take the control system and look at how much cortex (neurons, synapses)
%   is taken up by each element (word, syllable, phone, etc).

% - Extrapolate to human sized vocabularies, make sure it'll scale

% - Show that if we had a separate oscillator / population
%   for each word or syllable that this wouldn't scale

% ??? important: scaling. Show that adding new syllables
% to the syllabary doesn't affect the voicing
% of a given sequence; show how many neurons are added.

\subsection{Summary}

% ??? main results

\section{Syllable recognition}

The goal of the syllable recognition system
is to temporally classify syllables
given a continuous trajectory
of production information.
We will evaluate the system
based on its ability to emit
correct syllable classifications
at reasonable times,
and whether those classifications
are available in working memory
between classifications.

\subsection{Evaluation}

The infrastructure for evaluating
the recognition model
is similar to that of the production model.
We use the same library
of German syllable gesture scores,
from which we create
48-dimensional gesture trajectories.
On each trial,
we generate a trajectory
by concatenating the trajectories
of a randomly chosen sequence
of trajectories together
(i.e., we do not use the
syllable production model
to generate input for the model).
That trajectory is provided
to all of the iDMPs as input,
and we record the output of the
temporal input associative memory
and the memory module.

From the data collected,
we compute three metrics
to evaluate performance
in different situations.
First, we calculate
classification accuracy
using the same formula
as is used to calculate
phoneme classification accuracy ??? cite intech.
Specifically,
we generate a list of all of the
syllables classified by the model,
and compare that list to the
originally specified list
using the formula
\begin{equation}
  \text{Acc} = \frac{N_g - S - D - I}{N_g},
\end{equation}
where $N_g$ is the number of syllables
in the trajectory,
$S$ is the number of substitutions,
$D$ is the number of deletions,
and $I$ is the number of insertions.
A substitution occurs when
the syllable is misclassified.
A deletion occurs when
a syllable has occurred,
but the model does not emit a classification.
An insertion occurs
when the model emits a classification
when no syllable has occurred
(e.g., it classifies the same syllable twice).

Second, we record the times at which
syllable classification occur,
and compare those times
to the end of the actual syllable trajectory.
Here, we are attempting to determine
if the model can classify syllables
while they are being voiced,
or if the entire syllable must complete
before the classification can be made.

Third, we analyze the contents
of the memory module
in between syllable presentations.
Specifically,
we find the semantic pointer
with the maximum similarity
at the midpoint between
the start and the end of a syllable
for all syllables in the trajectory.
The memory representation of the trial
is perfect if the
previously classified syllable
is in memory at the midpoint
of a syllable.

\subsection{Experiments and results}

??? Hyperopt the main parameters

??? similar to prod: how much can we crank syllable speed?
??? will have to modify scale depending on speed... tweaky!

??? does adding more trajectories into vocab affect anything?

??? how long can sequences get? Probably forever, try that

\subsection{Scaling}

% Give main metrics of the network (number of neurons, etc)
% and discuss scaling

\subsection{Summary}

% ??? main results
