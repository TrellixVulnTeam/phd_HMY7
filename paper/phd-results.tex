\chapter{Evaluation and results}

%% ~15-30 pages

%% - adequacy, efficiency, productiveness, effectiveness
%%   (choose your criteria, state them clearly and justify them)
%% - be careful that you are using a fair measure, and that you are
%%   actually measuring what you claim to be measuring
%% - if comparing with previous techniques those techniques
%%   must be described in Chapter 2
%% - be honest in evaluation
%% - admit weaknesses

% For each model, have an initial section where subparts are
% tested to determine parameters for the full experiments

Here,
we present evaluation metrics for the three models
described in Chapter~???,
gather those metrics
for several experimental conditions,
and present the results.

\section{Neural cepstral coefficients}

We hypothesize that
neural cepstral coefficients (NCCs)
are a suitable feature vector representation
for traditional ASR systems,
and biologically grounded systems like Sermo.
In order to evaluate
its suitability for speech recognition tasks,
we compare NCCs to the most common
feature vector used currently,
Mel-frequency cepstral coefficients.

\subsection{Evaluation}

While different ASR systems
use the feature vector in different ways,
the most important quality of the feature vector
is that audio samples corresponding
to a particular label
(e.g., a phone or word)
takes a similar trajectory
in the feature vector space
as other sample corresponding to the same label.
All samples corresponding to other labels
should take trajectories that are
far enough away in the vector space
to provide a basis for labeling
a test sample correctly.
As such, we compare MFCCs and NCCs
on a purely statistical level,
rather than using them as a frontend
in an ASR system,
which would bias the results
toward the aspects of the
feature vector trajectories
that the ASR system's backend
is most suitable for detecting.

We therefore compare MFCCs and NCCs
using a linear support vector machine (SVM)
that is trained on labeled
examples of feature vector trajectories
corresponding to phones
in the TIMIT training corpus,
and tested on trajectories
corresponding to phones
in the TIMIT testing corpus.

TIMIT ??? cite is a corpus of read speech
with time-aligned phone and word transcriptions.
The corpus contains ten speech samples
from 630 speakers across eight dialects
of American English,
resulting in 6300 utterances in total
(5.4 hours of continuous speech).
The utterances are separated into
a training set
(3696 sentences; 3.14 hours of speech)
and a testing set
(1344 sentences; 0.81 hours of speech),
allowing for apples-to-apples
comparisons of ASR systems.
TIMIT was released in 1990,
and is still a standard data set used
for testing ASR systems
due to the high quality
manual phone transcriptions,
diversity of speakers,
and convenient size,
as it is small enough to be
computationally tractable
but large enough to
provide a meaningful comparison
between ASR approaches.

We will focus on classifying
phones in the TIMIT data set.
Phones are voiced
for a shorter time than words,
and have high variability
as phone sounds can be affected
by the previous and next phones voiced,
making phone classification
a more difficult task than word classification;
(??? word error rate vs phone error rate?).
Since we are not solving the full
speech recognition problem,
and are instead classifying
pre-segmented speech samples,
we anticipate that word classification
would be too simple to meaningfully
compare MFCCs and NCCs.

Additionally, classifying phones
is advantageous because phones
are voiced for a similar length of time
compared to words.
Another way in which our statistical comparison
differs from the full speech recognition problem
is that we must stretch
feature vector trajectories
such that all trajectories are the same length,
due to the constraints of SVM classifiers;
therefore,
we aim to minimize the amount
of stretching done.

We stretch feature vector trajectories
to be as long as the longest trajectory
in the training and testing samples
using simple linear interpolation.
An alternative method
for stretching the trajectory
would be to use dynamic time warping
??? cite DTW paper.
However, dynamic time warping
involves aligning each speech sample
to a representative example,
which would necessitate manually
determining a suitable example
for each phone for each experiment.
Additionally, the manual effort may not
provide any concrete benefits;
??? showed that linearly interpolating
time series data
produced classification accuracy rates
statistically indistinguishable
from accuracy rates on time series data
aligned using dynamic time warping.

TIMIT defines a set of 61 phones,
which include vowels, consonants,
consonantal closures, pauses and silences.
??? leehon.pdf identified
allophones and
other situation where two phones
are used interchangeably,
collapsing the full set of phones
to a set of 39 phones,
which are typically used
instead of the full set.
Many studies improve error rates
by further categorizing
the phones with similar properties;
e.g., grouping together all
plosives, fricatives, etc.
??? cite InTech.
We will use the reduced set of 39 phones.\footnote{
  The set of 39 phones includes a ``silence'' phone,
  which
}
Because vowels are typically voiced
longer than consonants,
we will investigate whether
separating them is necessary
since we lengthen all samples
to the longest sample.

Support vector machines are a standard
supervised learning technique
often applied to classification tasks,
and are particularly well suited
to high-dimensional data.
The SVM algorithm learns a hyperplane
that separates data
in one class from other classes,
which it can subsequently use
to classify data
by evaluating where the data point
lies in vector space compared to the hyperplane.
When dealing with more than two classes,
hyperplanes can be learned to separate
each combination of two classes
(``one-against-one'' classification)
or each class from all other classes
(``one-vs-the-rest'' classification).
When data is relatively low dimensional,
the kernel trick can be applied,
which projects the input data
into a higher dimensional space
based on a kernel function.

In the case of speech feature vector trajectories,
we have relatively many samples,
classes and features,
so we use a one-vs-the-rest
support vector machine classifier
that does not use the kernel trick
(i.e., a linear SVM).
We use LibLinear's fast implementation
of linear SVMs (??? cite linblinear.pdf),
exposed to Python through
the scikit-learn package
(??? cite scikit-learn).  % http://scikit-learn.org/stable/about.html#citing-scikit-learn

The metric that we collect
in the experiments below
is classification correctness,
either in absolute terms,
or NCC classification correctness
relative to the baseline correctness
obtained with NFCCs.
In the ASR literature,
phone error rates are often
reported on TIMIT.
The phone error rate
takes into account
incorrect classifications
(substitutions),
a lack of classification
when there should be one
(deletions)
and erroneous classifications
where there should not be one
(insertions).
However, since we pre-segment
the speech samples into
those that correspond to some phone,
we cannot have deletions or insertions,
only substitutions,
which corresponds to classification correctness.
Since we use correctness
rather than accuracy,
we can only compare our approach
to some reported results
??? cite InTech.pdf,
but even then the comparison is flawed.
Hence, we generate both MFCCs
and NCCs for the same speech samples
in order to verify that
NCCs generate feature vector trajectories
that are at least as statistically separable
as the equivalent MFCC trajectories.

\subsection{Experiments and results}

In each experiment,
we isolate the TIMIT speech samples
corresponding to the phones
of interest,
and generate a feature vector trajectory
(i.e., MFCC or NCC)
for that sample.
MFCCs are generated using frames
of 25 ms,
which advance 10 ms on each timestep,
as is standard in many ASR systems.
NCCs are generated
by running the model described in
Section~??? with a 1 ms simulation timestep.

All trajectories are
made to be the same length,
as is required by the SVM algorithm.
The number of frames used
for all samples is
the number of frames in
the longest MFCC trajectory
in the training set.
Samples that are too short
(e.g., most MFCC trajectories)
are lengthened to match the
target number of frames
using linear interpolations.
Samples that are too long
(e.g., all NCC trajectories)
are shortened to match the
target number of frames
by splitting the trajectory
into equally spaced time bins
and taking the average
for each bin.
See Figure~??? for
an illustration of temporal scaling,
and example MFCC and NCC trajectories.

??? figure: scaling; plot a bunch of
pcolormeshes of the same phoneme
in a set, some MFCCs lengthened,
some NCCs shortened

Each trajectory is then flattened
to produce a vector of length
$n_{\text{cepstra}} \times n_{\text{frames}}$.
Each trajectory is associated with
the phone label associated with
the original speech sample,
producing an input-output pair
that can be used to do supervised learning
in the linear SVM.

Finally, the same procedure is done
with the test set to produce another
set of input-output pairs.
The input is fed to the linear SVM
to generate a predicted phone labels,
which are compared to the actual phones
to yield a correctness measure.

\subsubsection{Number of neurons}

??? for Nengo model, ramp up the number of neurons
in each population to see where it plateaus;
set that

\subsubsection{Phone groups}

??? Compare consonants and vowels separate vs. together

\subsubsection{Derivatives and z-scoring}

??? compare with deriv to without

??? compare with zscore to without

for subsequent, use the one that was best (number of derivs
and zscore)

??? Compare different auditory filters, with middle ear and without

??? Compare adaptive neurons to normal LIF

\subsubsection{NCC averaging}

Maybe this worked because the NCCs operate
at a lower timestep and average.
Do a run with 0.001 ms MFCCs
to show that it doesn't matter

\subsection{Scaling}

Give main metrics of the network (number of neurons, etc)
and discuss scaling

\subsection{Summary}

??? main results

\section{Syllable sequencing and production}

The goal of the syllable sequencing and production model
is to generate a continuous trajectory
of production information
that can be used to control
an articulatory synthesizer,
given a static representation
of a sequence of syllables.
As the quality of generated speech
is difficult to quantify,
we evaluate this model
by generating a gesture score
for the decoded production information trajectory,
and comparing it
to the gesture score that was provided
to the model as input.

\subsection{Evaluation}

While the syllable sequencing and production model
can be used to control any articulatory synthesizer,
we use the VocalTractLab synthesizer
because of its high quality speech
and available API ??? cite.
The vocal tract gestures representation
in VocalTractLab is based on a set of
predefined articular positions.
Most gestures correspond to
a particular shape,
and the overall vocal tract shape
is based on linearly interpolating
between the shapes of
all currently active gestures.
Taking into account all of the
possible shapes and glottal parameters,
there are 48 possible vocal tract gestures
(see Table~???) despite having only
??? articulator sets
controlling 22??? articulators.\footnote{
  There is also an f0 gesture which is not included here,
  as it controls the pitch of the utterance.
  Since German is not a tonal language,
  the pitch is not required to voice a syllable,
  and instead is used for generating
  varied prosody;
  we do not consider prosody in this model,
  though including it would be straightforward.}

??? table of vocal tract gestures

As discussed in Section~???,
the input to the model
is a semantic pointer
representing a sequence of syllables.
Each syllable has an associated DMP
which is generated using
a gesture score corresponding to
a German syllable;
the gesture scores were provided by
Bernd Kr\"{o}ger and are available at
\url{http://www.phonetik.phoniatrie.rwth-aachen.de/bkroeger/research.htm}.\footnote{
  These German syllable gesture scores are among
  the few publicly available gesture scores.
  We were not able to find any gesture scores
  for English syllables.}
Each gesture score in VocalTractLab
is a collection of
gestures that are parameterized by target value,
onset time, gesture length,
and a time constant that determines
how quickly the gesture becomes active.

For each experiment described below,
we chose a random set of syllables
as the frequent syllables
that make up the vocabulary
of the model
(i.e., syllables in the mental syllabary).
A sequence of syllables of a given length
are randomly chosen and assigned a speed
sampled from $\mathcal{U}(1, 6)$ Hz,
which corresponds to syllable trajectories of
167--1000 ms in length.
Depending on the number of syllables
and the randomly sampled lengths,
in each trial we end up with a syllable sequence
generating several seconds
of production information trajectories,
which in the case of these experiments,
are 48-dimensional trajectories
where each dimension is a gesture.

The model is provided the static syllable sequence input,
and is run for the amount of time that the sequence
should take to voice (determined by the
randomly assigned syllable speeds),
plus a small amount of time to account
for variability in each model instance.
The output of the temporal output associative memory
is recorded.

That output is then converted into a gesture score
by estimating the gesture parameters
from the associative memory output.
Individual gestures are determined by
taking the absolute value of the temporal derivative
across each gesture;
time slices which have high derivatives
are indicative of the onset or offset of a gesture.
We make a new gesture for each
time slice with high derivative
with the beginning of the gesture as
the beginning of the first time slice,
and the end of the gesture
as the end of the next time slice;
the last time slice ends at the end
of the utterance.
The time constant associated
with the gesture
is the length of
the onset slice with high derivative.
The value of the gesture is the
average value between the two
time slices with high derivatives.
Every gesture has a neutral value
representing that the gesture
is not active;
identified gestures with values
sufficiently close to the neutral gestures
are removed from the gesture score.

??? figure with zoom-in showing start and end of gesture

The gesture score resulting from the model simulation
is then compared to the original gesture score
on three criteria.
First, we calculate the accuracy
of the model's gesture score
% using the same formula
% that is used for phoneme classification accuracy;
% specifically,
% \begin{equation}
%   \text{Acc} = \frac{N_g - S - D - I}{N_g},
% \end{equation}
% where $N_g$ is the number of gestures
% in the original gesture score,
% $S$ is the number of substitutions,
% $D$ is the number of deletions,
% and $I$ is the number of insertions.

% In the case of phonemes,
% it is easy to calculate accuracy,
% as we represent the actual and classified labels
% as strings.
% For gesture scores,
% each gesture has a start time and a length;
% in some cases, gestures should co-occur,
% so their temporal order is unimportant.
% For other gestures ...

% try some stuff and then update the commented section.

Second, we calculate the temporal variability
of co-occurring gestures.
For most syllables,
there are one or more pairs of gestures
that have either begin or end
at the same time.
The co-occurring gestures
are critical to producing
recognizable speech.
Therefore, for each pair
of co-occurring gestures in the
original gesture score,
we record the difference
in the simulated gesture score's
corresponding co-occurrence pair.

Finally, we synthesize a speech sample
using VocalTractLab,
and perform a qualitative evaluation
of its intelligibility.
Speech corresponding to the
original syllable sequence is compared
to that generated by the model.
We do not obscure the label
of the synthesized speech,
as this measure is reported primarily
to reinforce the other two measures;
it is not the primary measure
by which the model should be judged.
As such, we evaluate each sample
as either completely silent,
unintelligible, partially intelligible,
or intelligible.

\subsection{Experiments and results}

??? Hyperopt the main parameters: nneurons

??? how much can we crank up the syllable speed?

??? Does adding more syllables to the vocabulary affect anything?

??? How long can the sequences get before things break down?

\subsection{Scaling}

% Give main metrics of the network (number of neurons, etc)
% and discuss scaling

% - Take the control system and look at how much cortex (neurons, synapses)
%   is taken up by each element (word, syllable, phone, etc).

% - Extrapolate to human sized vocabularies, make sure it'll scale

% - Show that if we had a separate oscillator / population
%   for each word or syllable that this wouldn't scale

% ??? important: scaling. Show that adding new syllables
% to the syllabary doesn't affect the voicing
% of a given sequence; show how many neurons are added.

\subsection{Summary}

% ??? main results

\section{Syllable recognition}

The goal of the syllable recognition system
is to temporally classify syllables
given a continuous trajectory
of production information.
We will evaluate the system
based on its ability to emit
correct syllable classifications
at reasonable times,
and whether those classifications
are available in working memory
between classifications.

\subsection{Evaluation}

The infrastructure for evaluating
the recognition model
is similar to that of the production model.
We use the same library
of German syllable gesture scores,
from which we create
48-dimensional gesture trajectories.
On each trial,
we generate a trajectory
by concatenating the trajectories
of a randomly chosen sequence
of trajectories together
(i.e., we do not use the
syllable production model
to generate input for the model).
That trajectory is provided
to all of the iDMPs as input,
and we record the output of the
temporal input associative memory
and the memory module.

From the data collected,
we compute three metrics
to evaluate performance
in different situations.
First, we calculate
classification accuracy
using the same formula
as is used to calculate
phoneme classification accuracy ??? cite intech.
Specifically,
we generate a list of all of the
syllables classified by the model,
and compare that list to the
originally specified list
using the formula
\begin{equation}
  \text{Acc} = \frac{N_g - S - D - I}{N_g},
\end{equation}
where $N_g$ is the number of syllables
in the trajectory,
$S$ is the number of substitutions,
$D$ is the number of deletions,
and $I$ is the number of insertions.
A substitution occurs when
the syllable is misclassified.
A deletion occurs when
a syllable has occurred,
but the model does not emit a classification.
An insertion occurs
when the model emits a classification
when no syllable has occurred
(e.g., it classifies the same syllable twice).

Second, we record the times at which
syllable classification occur,
and compare those times
to the end of the actual syllable trajectory.
Here, we are attempting to determine
if the model can classify syllables
while they are being voiced,
or if the entire syllable must complete
before the classification can be made.

Third, we analyze the contents
of the memory module
in between syllable presentations.
Specifically,
we find the semantic pointer
with the maximum similarity
at the midpoint between
the start and the end of a syllable
for all syllables in the trajectory.
The memory representation of the trial
is perfect if the
previously classified syllable
is in memory at the midpoint
of a syllable.

\subsection{Experiments and results}

??? Hyperopt the main parameters

??? similar to prod: how much can we crank syllable speed?
??? will have to modify scale depending on speed... tweaky!

??? does adding more trajectories into vocab affect anything?

??? how long can sequences get? Probably forever, try that

\subsection{Scaling}

% Give main metrics of the network (number of neurons, etc)
% and discuss scaling

\subsection{Summary}

% ??? main results
