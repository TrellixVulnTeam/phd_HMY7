\chapter{Evaluation and results}
\label{chapt:results}

%% ~15-30 pages

%% - adequacy, efficiency, productiveness, effectiveness
%%   (choose your criteria, state them clearly and justify them)
%% - be careful that you are using a fair measure, and that you are
%%   actually measuring what you claim to be measuring
%% - if comparing with previous techniques those techniques
%%   must be described in Chapter 2
%% - be honest in evaluation
%% - admit weaknesses

% For each model, have an initial section where subparts are
% tested to determine parameters for the full experiments

Here,
we present evaluation metrics for the three models
described in Chapter~\ref{chapt:implementation},
gather those metrics
for several experimental conditions,
and present the results.

\section{Neural cepstral coefficients}

We hypothesize that
neural cepstral coefficients (NCCs)
are a suitable feature vector representation
for traditional ASR systems,
and biologically grounded systems like Sermo.
In order to evaluate
its suitability for speech recognition tasks,
we compare NCCs to the most common
feature vector used currently,
Mel-frequency cepstral coefficients.

\subsection{Evaluation}

While different ASR systems
use the feature vector in different ways,
the most important quality of the feature vector
is that audio samples corresponding
to a particular label
(e.g., a phone or word)
takes a similar trajectory
in the feature vector space
as other sample corresponding to the same label.
All samples corresponding to other labels
should take trajectories that are
far enough away in the vector space
to provide a basis for labeling
a test sample correctly.
As such, we compare MFCCs and NCCs
on a purely statistical level,
rather than using them as a frontend
in an ASR system,
which would bias the results
towarvvd the aspects of the
feature vector trajectories
that the ASR system's backend
is most suitable for detecting.

We therefore compare MFCCs and NCCs
using a linear support vector machine (SVM)
that is trained on labeled
examples of feature vector trajectories
corresponding to phones
in the TIMIT training corpus,
and tested on trajectories
corresponding to phones
in the TIMIT testing corpus.

TIMIT \cite{garofolo1993} is a corpus of read speech
with time-aligned phone and word transcriptions.
The corpus contains ten speech samples
from 630 speakers across eight dialects
of American English,
resulting in 6300 utterances in total
(5.4 hours of continuous speech).
The utterances are separated into
a training set
(3696 sentences; 3.14 hours of speech)
and a testing set
(1344 sentences; 0.81 hours of speech),
allowing for apples-to-apples
comparisons of ASR systems.
TIMIT was released in 1990,
and is still a standard data set used
for testing ASR systems
due to the high quality
manual phone transcriptions,
diversity of speakers,
and convenient size,
as it is small enough to be
computationally tractable
but large enough to
provide a meaningful comparison
between ASR approaches.

We will focus on classifying
phones in the TIMIT data set.
Phones are voiced
for a shorter time than words,
and have high variability
as phone sounds can be affected
by the previous and next phones voiced,
making phone classification
a more difficult task than word classification;
(??? word error rate vs phone error rate?).
Since we are not solving the full
speech recognition problem,
and are instead classifying
pre-segmented speech samples,
we anticipate that word classification
would be too simple to meaningfully
compare MFCCs and NCCs.

Additionally, classifying phones
is advantageous because phones
are voiced for a similar length of time
compared to words.
Another way in which our statistical comparison
differs from the full speech recognition problem
is that we must stretch
feature vector trajectories
such that all trajectories are the same length,
due to the constraints of SVM classifiers;
therefore,
we aim to minimize the amount
of stretching done.

We stretch feature vector trajectories
to be as long as the longest trajectory
in the training and testing samples
using simple linear interpolation.
An alternative method
for stretching the trajectory
would be to use dynamic time warping
\cite{ratanamahatana2004}.
However, dynamic time warping
involves aligning each speech sample
to a representative example,
which would necessitate manually
determining a suitable example
for each phone for each experiment.
Additionally, the manual effort may not
provide any concrete benefits;
??? showed that linearly interpolating
time series data
produced classification accuracy rates
statistically indistinguishable
from accuracy rates on time series data
aligned using dynamic time warping.

TIMIT defines a set of 61 phones,
which include vowels, consonants,
consonantal closures, pauses and silences.
??? leehon.pdf identified
allophones and
other situation where two phones
are used interchangeably,
collapsing the full set of phones
to a set of 39 phones,
which are typically used
instead of the full set.
Many studies improve error rates
by further categorizing
the phones with similar properties;
e.g., grouping together all
plosives, fricatives, etc.
\cite{lopes2011}.
We will use the reduced set of 39 phones.\footnote{
  The set of 39 phones includes a ``silence'' phone,
  which we include when using all phones,
  but do not include in either the vowel or consonant
  phone set.}
Because vowels are typically voiced
longer than consonants,
we will investigate whether
separating them is necessary
since we lengthen all samples
to the longest sample.

Support vector machines are a standard
supervised learning technique
often applied to classification tasks,
and are particularly well suited
to high-dimensional data.
The SVM algorithm learns a hyperplane
that separates data
in one class from other classes,
which it can subsequently use
to classify data
by evaluating where the data point
lies in vector space compared to the hyperplane.
When dealing with more than two classes,
hyperplanes can be learned to separate
each combination of two classes
(``one-against-one'' classification)
or each class from all other classes
(``one-vs-the-rest'' classification).
When data is relatively low dimensional,
the kernel trick can be applied,
which projects the input data
into a higher dimensional space
based on a kernel function.

In the case of speech feature vector trajectories,
we have relatively many samples,
classes and features,
so we use a one-vs-the-rest
support vector machine classifier
that does not use the kernel trick
(i.e., a linear SVM).
We use LibLinear's fast implementation
of linear SVMs \cite{fan2008},
exposed to Python through
the scikit-learn package
\cite{pedregosa2011}.

The metric that we collect
in the experiments below
is classification correctness,
either in absolute terms,
or NCC classification correctness
relative to the baseline correctness
obtained with NFCCs.
In the ASR literature,
phone error rates are often
reported on TIMIT.
The phone error rate
takes into account
incorrect classifications
(substitutions),
a lack of classification
when there should be one
(deletions)
and erroneous classifications
where there should not be one
(insertions).
However, since we pre-segment
the speech samples into
those that correspond to some phone,
we cannot have deletions or insertions,
only substitutions,
which corresponds to classification correctness.
Since we use correctness
rather than accuracy,
we can only compare our approach
to some reported results
\cite{lopes2011},
but even then the comparison is flawed.
Hence, we generate both MFCCs
and NCCs for the same speech samples
in order to verify that
NCCs generate feature vector trajectories
that are at least as statistically separable
as the equivalent MFCC trajectories.

\subsection{Experiments and results}

In each experiment,
we isolate the TIMIT speech samples
corresponding to the phones
of interest,
and generate a feature vector trajectory
(i.e., MFCC or NCC)
for that sample.
MFCCs are generated using frames
of 25 ms,
which advance 10 ms on each timestep,
as is standard in many ASR systems.
NCCs are generated
by running the model described in
Section~\ref{sec:ncc-neural} with a 1 ms simulation timestep.
Because NCC generation takes several minutes,
and because the SVM fitting procedure
is quadratic in the number of samples,
we limit our experiments to
one region of the TIMIT dataset
(the ``army brat'' region,
which consists of a training set
of 220 utterances across 22 speakers,
and a test set of of 110 utterances
across 11 different speakers),
but run each experiment
10 times with different random seeds
to generate confidence intervals
for each experimental condition.
In the MFCC case,
the feature vectors are the same
on every trial;
the random seed controls the
stochastic nature of
the supervised learning procedure
used in the SVM.
In the NCC case,
the random seed affects both
the generation of model parameters
that are sampled from random distributions,
and the random seed used by the SVM.

All trajectories are
made to be the same length,
as is required by the SVM algorithm.
The number of frames used
for all samples is
the number of frames in
the longest MFCC trajectory
in the training set.
Samples that are too short
(e.g., most MFCC trajectories)
are lengthened to match the
target number of frames
using linear interpolations.
Samples that are too long
(e.g., all NCC trajectories)
are shortened to match the
target number of frames
by splitting the trajectory
into equally spaced time bins
and taking the average
for each bin.
See Figure~\ref{fig:temp-scaling} for
an illustration of temporal scaling,
and example MFCC and NCC trajectories.

% \fig{temp-scaling}{0.7}{???}{???}
% ??? figure: scaling; plot a bunch of
% pcolormeshes of the same phoneme
% in a set, some MFCCs lengthened,
% some NCCs shortened

Each trajectory is then flattened
to produce a vector of length
$n_{\text{cepstra}} \times n_{\text{frames}}$.
Each trajectory is associated with
the phone label associated with
the original speech sample,
producing an input-output pair
that can be used to do supervised learning
in the linear SVM.

Finally, the same procedure is done
with the test set to produce another
set of input-output pairs.
The input is fed to the linear SVM
to generate predicted phone labels,
which are compared to the actual phones
to yield a correctness measure.

Wherever not specified,
the parameters used for each model
are as listed in Table~???.

??? table of default parameters

\subsubsection{Derivatives and z-scoring}

Two important choices for a feature vector
are whether to include derivatives,
and whether to normalize the resulting vector.
Typically, ASR systems include at least
one derivative, and perform normalization
through z-scoring
(i.e., removing the mean
and normalizing to unit variance).
We investigated whether these
choices were important for NCCs.

??? z-score figure

Figure~??? shows
the training and testing accuracy
for ten trials
with and without z-scoring.
For MFCCs,
z-scoring greatly reduces the variance
of the training and testing accuracy;
the mean accuracy values
are indistinguishable for
vowel phonemes,
and may be slightly lower
for consonant phonemes.
For NCCs,
z-scoring sigiificantly
raises training accuracy,
but lowers testing accuracy
(this trend is similar for MFCCs,
but is obscured by the
highly variable results
when not z-scoring).

We interpret these results
as meaning that z-scoring is essential
for MFCCs because without it,
the SVM fitting procedure
varies significantly for
different random seeds
(hence the high variability without z-scoring).
NCCs, on the other hand,
have low variance in either condition.
Since a higher test accuracy indicates
better generalization,
we believe that z-scoring
does more harm than good for NCCs.
Therefore, for future simulations,
we will z-score the MFCC feature vector,
but not the NCC feature vector.

??? derviative figure

Figure~??? shows
the same experiment
varying the number of derivatives used
while always z-scoring MFCCs,
and never z-scoring NCCs.
Note that using derivative means
that the feature vector has
length 26,
where the first 13 elements
are cepstral coefficients,
and the last 13 are derivatives
of the cepstral coefficients;
using two derivatives means that
the vector has length 39,
with the derivative of the
derivative of the cepstral coefficient
appended to vector with 26 elements.

For MFCCs, adding a single derivative
improves testing accuracy
by almost double
(??? 0deriv to 1deriv),
but adding a second derivative
has very little effect.
For NCCs, adding derivatives
improves training accuracy,
but slightly reduces testing accuracy.

??? derivative time figure

Unlike with z-scoring,
we believe it is important
for the MFCC and NCC features to
be the same length,
so for subsequent experiments,
both feature vectors
will contain the first derivative
(and therefore have length 26).
We use one derivative
because it puts MFCCs and NCCs
on nearly equal footing;
MFCCs are greatly improved
with derivatives,
while NCCs are only slightly worsened.
Adding derivatives
has a significant effect
on the amount of time taken to
generate NCCs
(see Figure~???),
so we add only one derivative
to keep the experiments tractable.

??? Also try ff deriv vs int deriv

Note that in both of these experiments,
it is accuracy is generally higher
for vowel phones versus consonant phones.
Therefore, in subsequent experiments,
we will test only with consonant phones
as they should pose a greater challenge
to the model.

\subsubsection{Number of neurons}

Ideally,
we would like to reduce the amount of time
that the NCC generation process takes.
As shown in Figure~???,
generating NCCs for all
of the speech samples is expensive.
Reducing the number of neurons
used in the NCC model
can speed it up,
but at the cost of performance.
In these experiments,
we vary the number of neurons used
in different layers of the model
in order to see their impact
on speed and performance.

??? figure: periph figure

Figure~??? shows the results of
varying the number of neurons
in the auditory periphery layer.
In this layer,
each auditory filter projects
to an ensemble of neurons
of the size varied in this experiment.
Since the representation here
is a simple one
(a positive scalar value),
the number of neurons required
should not be very large,
so we vary between
1 and 32 neurons with steps
at each power of two.
The number of neurons used
in the feature layers
is fixed at 20 per feature.
Instead of plotting the absolute accuracy,
in this and subsequent experiments,
we plot the accuracy of the NCC model
relative to the mean accuracy
of the same number
of MFCC model instance accuracies.

As seen in Figure~???,
training accuracy
does not change significantly
whether we use
1 or 32 neurons neurons per filter,
thought variance is higher
with fewer neurons.
Testing accuracy,
however, improves up to
using 8 neurons per filter,
but then plateaus,
or may even decline
(possibly due to overfitting).
Therefore, in all
subsequent experiments,
we use 8 neurons to represent
the output of each auditory filter.

??? Figure: feature figure

Figure~??? shows the results
of varying the number of neurons
in the feature layer,
which includes the ensembles
representing the cepstral coefficients,
the ensembles representing
the derivatives of those coefficients,
and any additional ensembles required
to compute the derivative.
The number of neurons for each filter
in the periphery is fixed at 8.

A very different trend is evident
when varying the number of neurons
used in the feature layer.
For very small number of neurons,
training accuracy is worse
than MFCC training accuracy.
As the number of neurons increases,
accuracy improves up to
using 8 neurons per ensemble,
then steadily worsens.
Testing accuracy, however,
is highly variable until
we use 8 neurons per ensemble,
at which point performance
steadily increases up to
the highest number of neurons tested,
64 neurons per ensemble.

??? Figure: feature times

As with the number of derivatives,
the number of neurons in the feature layer
has a big impact on the amount of time
each experiment takes;
see Figure~???.
The time taken for each trial
is the same up to 12 neurons per ensemble,
at which point
the time taken sharply increases.
Since the accuracy using 12 neurons per ensemble
is significantly higher than
the mean accuracy of the equivalent MFCC feature vector,
we believe that using 12 neurons per ensemble
is sufficiently accurate,
and so in subsequent experiments,
we fix the number of neurons
in the feature layer to contain
12 neurons per ensemble.

\subsubsection{NCC averaging}

Before the eventual goal of
varying the auditory periphery model used,
we address concerns about the validity
of our comparison.

One possible concern is that
the NCC model runs at a lower timestep,
giving it some numerical advantages
over the MFCC model;
similarly, the shortening and lengthening
procedures may affect the results.
To examine the validity of these concerns,
we set up an experiment
with a fixed number of frames
per audio sample,
and varied the amount that the frame window
advanced on each timestep.
When the frame window moves at the
same rate as the Nengo simulation timestep,
the two models are manipulated
in the same way by the
shortening algorithm,
which averages over a time windo.

??? Figure: timewindow

Figure~??? shows the consonant accuracy
as a function of the
frame window advance in seconds.
While using a lower timestep results in
significantly better training accuracy,
testing accuracy is actually better
with a longer timestep,
indicating that the increased temporal
resolution may be causing overfitting.
It is possible that this overfitting
also occurs with the NCC model,
but running with a high simulation timestep
would result in an abundance of
synchronous spiking activity
and lowered decoding accuracy.

\subsubsection{Phone groups}

Finally, to this point we have
evaluated all of the experiments using
the consonant phones
because we believe that they are
more difficult than vowel phones.
However, it may be the case that
using all phones together
is a more advantageous situation
for MFCCs compared to NCCs.
For that reason,
we compared the relative accuracy
of NCCs when using
vowel phones, consonant phones,
and all phones together
(including the silent phone).

??? Figure: phones

Figure~??? presents the results.
All conditions have relative accuracy
significantly above one,
meaning that NCC features performed
better than MFCC features.
Additionally, ??? more

??? figure: time taken

As seen in Figure~???,
using all phones increases the
amount of time to run experiments significantly.
Not only do NCCs take a long time to generate,
the increased number of samples
causes the SVM fitting procedure
to take a long time
for both MFCCs and NCCs.
Interestingly, fitting for NCCs
takes ??? almost half
the time as fitting for MFCCs,
despite the number of samples
and number of features being identical.
This result indicates that it is
easier to find linearly separating hyperplanes
for NCCs than MFCCs.

In sum, we believe that our choice to
only test with consonant phones
was justified,
and results in the closest
comparison between MFCCs and NCCs.

\subsubsection{Auditory periphery model}
\label{sec:results-periphmodel}

The final experiment varies the auditory periphery model
used at the beginning of the NCC pipeline.
We looked at two choices in the auditory periphery
and how those choices impacted classification accuracy.

The first choice is auditory filter model.
We ran 20 trials with each of the five
auditory filter models presented
in Section~\ref{sec:periphery-models}.
While most of the parameters are the same
as those in Table~???,
the current implementation of the Tan Carney model
can only be used for audio signals
with a 50 kHz sampling rate;
for this reason, all TIMIT samples
were upsampled to 50 kHz,
and all models (the MFCC model and all NCC models)
used the upsampled audio.
Additionally, the Tan Carney model
has numerical instabilities for auditory filters
with low characteristic frequencies,
so all models use frequencies
evenly distributed over the Mel scale
from 100 Hz to 8000 Hz.

??? periphmodel results

The results are plotted in Figure~???.
The Gammatone and Tan Carney models
both perform well,
with around 40\% better relative testing accuracy
than the MFCC model.
The compressive Gammachirp model
also performed well,
though statistically worse
than the Gammatone and Tan Carney models.
The log Gammachirp and Dual Resonance models
both perform significantly worse
than the other three,
achieving only around 20--24\%
more accurately than the MFCC model.

??? periphmodel speed

However, as seen in Figure~???,
the compressive Gammachirp
and Tan Carney models are
computationally expensive
(3--4 times slower)
compared to the other three models.
As the least expensive model,
the Gammatone filter
provides by far the best tradeoff
between accuracy and computational cost.

??? adaptive lif results

We also varied whether we used
normal LIF neuron or adaptive LIF neurons,
as spiral ganglion cells
have facilitating and depressing behavior
that is partly captured in the adaptive LIF neuron.
As shown in Figure~???,
using adaptive LIF neurons had no impact
on accuracy in any of the five periphery model.
The only effect may be a reduction
in the testing accuracy variance
for the Tan Carney model,
but more trials would be needed
in order to determine this definitively.

\subsection{Scaling}

The model used in these experiments
represents a small portion
of the human auditory system;
it is matched primarily to the parameters
commonly used in ASR systems.
However, since our model is
implemented with biologically realistic parts,
it is important to determine
if a scaled up version of the model
matches known neuroanatomical constraints.

The number of spiking neurons used
in the model is
\begin{equation}
  N = N_f |f| + N_c |c| + \sum_{i=0}^{|d|} 2 N_d |c|,
\end{equation}
where $N_f$ is the number of neurons per frequency,
$|f|$ is the number of frequencies modeled,
$N_c$ is the number of neurons per cepstral coefficient,
$|c|$ is the number of cepstral coefficients,
$|d|$ is the number of derivatives,
and $N_d$ is the number of neurons per derivative.

With the parameters used
in the majority of the experiments above,
$N=1036$ neurons.
According to the BioNumbers database,
the human cochlea has 3500 inner hair cells
\cite[BNID~100697]{milo2010}.
Assuming that each inner hair cell
represents a separate auditory filter
(which is not necessarily true,
but a worst case assumption),
we conservatively estimate
that a scaled up version
of the NCC model
would use 20 cepstral coefficients,
1 derivative, and 50 neurons per feature,
resulting in $N=7.3 \times 10^4$ neurons.
If we are somewhat more generous
and use 40 cepstral coefficients,
2 derivatives, and 200 neurons per feature,
we get $N=1.8 \times 10^5$ neurons.
\cite{smiley2013} estimated
that human A1 alone contains
2$\times 10^7$--3$\times 10^7$ neurons,
meaning that the NCC model
fits well within human neuroanatomical constraints.

\subsection{Summary}

??? main results

\section{Syllable sequencing and production}
\label{sec:results-production}

The goal of the syllable sequencing and production model
is to generate a continuous trajectory
of production information
that can be used to control
an articulatory synthesizer,
given a static representation
of a sequence of syllables.
As the quality of generated speech
is difficult to quantify,
we evaluate this model
by generating a gesture score
for the decoded production information trajectory,
and comparing it
to the gesture score that was provided
to the model as input.

\subsection{Evaluation}

While the syllable sequencing and production model
can be used to control any articulatory synthesizer,
we use the VocalTractLab synthesizer
because of its high quality speech
and available API \cite{birkholz2013}.
The vocal tract gestures representation
in VocalTractLab is based on a set of
predefined articular positions.
Most gestures correspond to
a particular shape,
and the overall vocal tract shape
is based on linearly interpolating
between the shapes of
all currently active gestures.
Taking into account all of the
possible shapes and glottal parameters,
there are 48 possible vocal tract gestures
(see Table~???) despite having only
??? articulator sets
controlling 22??? articulators.\footnote{
  There is also an f0 gesture which is not included here,
  as it controls the pitch of the utterance.
  Since German is not a tonal language,
  the pitch is not required to voice a syllable,
  and instead is used for generating
  varied prosody;
  we do not consider prosody in this model,
  though including it would be straightforward.}

??? table of vocal tract gestures

As discussed in Section~\ref{sec:impl-prod-overview},
the input to the model
is a semantic pointer
representing a sequence of syllables.
Each syllable has an associated DMP
which is generated using
a gesture score corresponding to
a German syllable;
the gesture scores were provided by
Bernd Kr\"{o}ger and are available at
\url{http://www.phonetik.phoniatrie.rwth-aachen.de/bkroeger/research.htm}.\footnote{
  These German syllable gesture scores are among
  the few publicly available gesture scores.
  We were not able to find any gesture scores
  for English syllables.}
Each gesture score in VocalTractLab
is a collection of
gestures that are parameterized by target value,
onset time, gesture length,
and a time constant that determines
how quickly the gesture becomes active.

For each experiment described below,
we chose a random set of syllables
as the frequent syllables
that make up the vocabulary
of the model
(i.e., syllables in the mental syllabary).
A sequence of syllables of a given length
are randomly chosen and assigned a speed
sampled from $\mathcal{U}(1, 6)$ Hz,
which corresponds to syllable trajectories of
167--1000 ms in length.
Depending on the number of syllables
and the randomly sampled lengths,
in each trial we end up with a syllable sequence
generating several seconds
of production information trajectories,
which in the case of these experiments,
are 48-dimensional trajectories
where each dimension is a gesture.

The model is provided the static syllable sequence input,
and is run for the amount of time that the sequence
should take to voice (determined by the
randomly assigned syllable speeds),
plus a small amount of time to account
for variability in each model instance.
The output of the temporal output associative memory
is recorded.

That output is then converted into a gesture score
by estimating the gesture parameters
from the associative memory output.
Individual gestures are determined by
taking the absolute value of the temporal derivative
across each gesture;
time slices which have high derivatives
are indicative of the onset or offset of a gesture.
We make a new gesture for each
time slice with high derivative
with the beginning of the gesture as
the beginning of the first time slice,
and the end of the gesture
as the end of the next time slice;
the last time slice ends at the end
of the utterance.
The time constant associated
with the gesture
is the length of
the onset slice with high derivative.
The value of the gesture is the
average value between the two
time slices with high derivatives.
Every gesture has a neutral value
representing that the gesture
is not active;
identified gestures with values
sufficiently close to the neutral gestures
are removed from the gesture score.

??? figure with zoom-in showing start and end of gesture

The gesture score resulting from the model simulation
is then compared to the original gesture score
on three criteria.
First, we calculate the accuracy
of the reconstructed gesture score
with the same equation as phoneme classification accuracy;
specifically,
\begin{equation} \label{eq:accuracy}
  \text{Acc} = \frac{N_g - S - D - I}{N_g},
\end{equation}
Therefore, we must find the number of
substitutions, deletions, and insertions
in our gesture score compared to
a gesture score composed of
the original gestures scores
for the target syllables
concatenated together.
In order to determine the number of
insertions, deletions and substitutions,
we first convert the original
and reconstructed gesture scores
to a sequence of characters
by assigning a character to each gesture
and determining their order
within each articulator set,
then concatenating them.
Neutral gestures are recorded
and denoted with the ``\texttt{0}'' character;
numerical gestures are assigned
a digit from \texttt{0} to \texttt{9}
depending on the relative value
in the gesture's range.
We then do a global pairwise alignment\footnote{
  Using the Needleman-Wunsch algorithm \cite{needleman1970},
  implemented by the \texttt{nwalign} Python package
  \url{https://pypi.python.org/pypi/nwalign/}.}
of the string determined
from the target gesture
and the reconstructed gesture,
which is then evaluated
according to Equation~\eqref{eq:accuracy}.
In order to evaluate whether
accuracy scores are good
(since we know of no comparable
speech trajectory generation techniques)
we will compare the accuracy
of the reconstructed gesture scores
to a baseline calculated as
the mean accuracy of all the
non-matching syllables in the
repository of syllables.
In other words, we will calculate
the accuracy of all combinations
of syllables in the syllable repository
(except for matching syllables)
and set the baseline as the mean
of those accuracy measures.

Since the accuracy measure only tells us
that the gesture score contains
the right gestures in the right order,
we also aim to quantify
how the temporal characteristics
of the reconstructed gesture score
differ from the target gesture score.
For each correctly aligned gesture
in the alignment computed
for the accuracy measure,
we record the difference
between the gesture duration,
and report both the mean and variance.
The mean tells us about
overall temporal difference,
indicating that gesture durations
differ in predictable ways;
e.g., a high mean difference
indicates that
the reconstructed gesture score
is slower than thee target gesture score,
but may still be able to
produce good speech.
High variance, however,
indicates that
gesture durations vary in unpredictable ways,
which is likely to be detrimental
to natural speech.

It is critically important
for speech that certain gestures
either begin or end at the same time;
we will call these co-occurring gestures,
though they may end or start at different times.
Therefore, for each pair
of co-occurring gestures in the
target gesture score,
we record the difference
in the reconstructed gesture score's
corresponding co-occurrence pair,
if those gestures exist.
We report the average absolute
difference between the start or end times
of reconstructed gestures
that co-occur in the target gesture.

Finally, we synthesize a speech sample
using VocalTractLab,
and perform a qualitative evaluation
of its intelligibility.
Speech corresponding to the
original syllable sequence is compared
to that generated by the model.
We do not obscure the label
of the synthesized speech,
as this measure is reported primarily
to reinforce the other two measures;
it is not the primary measure
by which the model should be judged.
As such, we evaluate each sample
as either completely silent,
unintelligible, partially intelligible,
or intelligible.

\subsection{Experiments and results}

??? Basic operation fig and text

For each trial in each experiment,
we randomly select a set of syllables
and a syllable sequence
from a repository of 417 German syllables.
Of these syllables,
48 have CCV structure,
184 have CV structure,
162 have CVC structure,
and 23 have V structure;
we do not bias the random sample
toward any of the structures,
meaning that we will generally
have more CV syllables
than any other type in these experiments.
Each randomly selected syllables
has a semantic pointer,
an ensemble in the associative memory,
and a DMP network associated with it.
The intrinsic frequency (speed)
of each syllable is randomly assigned
between 0.8 and 3.0 Hz,
except where noted otherwise.
For each trial,
both the syllable repository
and the length and order
of the syllable sequence
to be produced are randomly generated,
in order to obtain robust metrics
not biased toward certain syllables
or sequences of syllables.

As with the NCC model,
we begin by varying the number of neurons
in different parts of the model.
Specifically, we vary the number of neurons
used in each DMP network,
the number of neurons used in the
timing ensemble in the sequencing network,
and the number of neurons used
in the readout population that
aggregates the trajectories
from each DMP.
For each experimental condition,
we run 20 trials using
a sequence of three syllables
from a repository of three syllables,
meaning that syllable repetition can occur,
but is not guaranteed.

??? fig: nneurons

As shown in Figure~???,
the number of neurons in each DMP network
impacts the performance
of the overall model.
Accuracy is either very low or
highly variable until around
90 neurons per dimension
in the DMP, after which
performance is essentially the same;
in subsequent simulations,
we will therefore
use 90 neurons per dimension
for each DMP network.
On the other hand,
both the timing ensemble
and the readout ensembles
can have relatively few neurons
with little impact on
the performance of the overall network.
The timing ensemble
has some dynamics when the reset signal is active,
so ??? test with smaller nneurons.
The readout ensembles
only represent positive scalar values
computed by the DMP networks,
so they require very few neurons.
In subsequent experiments,
we will use 8 neurons per readout ensemble
in order to speed up simulations.

??? fig : freq

Next, we investigated the
range of frequencies in which the model
can produce accurate trajectories.
In this experiment, we use a random sequence
of three syllables from a
repository of three random syllables
with the same frequency.
Figure~??? shows the accuracy
of the model as a function of the frequency.
Interestingly, the model is least accurate
when operating at relatively low frequencies.
It performs well above 2 Hz,
but starts to break down ??? higher freqs.

??? fig : nsyllables

We then investigated whether the model
behavior changes when the size
of the syllabary changes.
Since adults have a repository
of thousands of commonly uttered syllables,
the size of the syllabary should have
little effect on the model.
However, as is shown in Figure~???,
model accuracy decreases
as the size of the syllabary increases.
??? more, look into the SPA stuff

??? fig : sequencelen

In a similar vein,
we investigated whether model behavior
depends on the length of the syllable sequence,
with the size of the syllabary fixed.
In this case, we know from \cite{choo2010}
that sequences of length five or greater
tend to have difficulty with elements
in the middle of the sequence.
As shown in Figure~???,
accuracy does indeed drop off steadily
after five syllables.

??? fig : repeat

Finally, to this point we have
allowed syllables to repeat
under the assumption that our network design
(specifically the choice to use rhythmic DMPs)
resulted in similar performance when
repeating syllables compared to
a sequence of unique syllables.
Therefore, we investigated whether
accuracy is impacted when
sequences of length three
are constrained to be unique.
As can be seen in Figure~???,
the model performs the same
whether syllables are allowed to repeat or not,
verifying our assumption.

\subsection{Scaling}
\label{sec:res-prod-scaling}

The number of neurons in the
sequencing and production model is
\begin{equation}
  N = N_{\text{SPA}} + N_S + N_{\text{DMP}} + N_O,
\end{equation}
where these variables
are the number of neurons in
a subset of the model.
$N_{\text{SPA}}$ is the number of
neurons for all SPA modules,
which is
\begin{equation}
  N_{\text{SPA}} = 5 D_{\text{SYLL}} N_D_{\text{SYLL}} +
      |DFT_{\text{SYLL}}| N_D_{\text{SYLL}}
      + 2 N_{\text{AM}} (3 |\text{SYLL}| + 1),
\end{equation}
where $D_{\text{SYLL}}$ is the dimensionality
of syllable semantic pointers,
$N_D_{\text{SYLL}}$ is the number of neurons
per syllable dimension,
$|DFT_{\text{SYLL}}|$ is the number of
discrete Fourier transform coefficients
used by the binding operator,
$N_{\text{AM}}$ is the number of neurons
per associative memory ensemble,
and $|\text{SYLL}|$ is the number of
syllables in the syllabary.
$N_S$ is the number of neurons
in the sequencer network,
which is
\begin{equation}
  N_S = 4 N_D_S + 140,
\end{equation}
where $N_D_S$ is the number of neurons
per dimension in the sequencer network,
and 140 neurons come from ensembles
with fixed size.
$N_{\text{DMP}}$ is the number of neurons
in DMP networks, which is
\begin{equation}
  N_{\text{DMP}} = 3 |\text{SYLL}| N_{|\text{DMP}|} + 40,
\end{equation}
where $N_{|\text{DMP}|}$ is the number of neurons
per DMP dimension,
and 40 neurons comes from esnembles with fixed size.
$N_O$ is the number of neurons
in readout ensembles, which is
\begin{equation}
  N_O = 48 N_{|O|}
\end{equation}
where 48 is the number of gestures,
and $N_{|O|}$ is the number of neurons
per readout ensemble.

With the parameters used
in the majority of the experiments above,
$N=49030$ neurons,
with most of those neurons
in the SPA populations
($N_{\text{SPA}}=44600$)
and the readout ensembles ($N_O=2880$).
Fortunately, these parts of the model
grow at a slow rate
as additional syllables are added
to the syllabary.
If we assume a conservative syllabary
with around 2000 syllables,
and increase the dimensionality
of each syllable to 256,
the model uses
$N=1.34 \times 10^6$ neurons.
If we assume a more generous estimate
with 4000 syllables
and 512 dimensions
the semantic pointer representations,
then the model uses
$N=2.67 \times 10^6$ neurons.
Given that there are approximately
2.7 \times 10^4 neurons per mm$^3$ (on average)
in human cortex \cite[BNID 112050]{milo2010},
a human scale syllabary would take up
approximately 102.7 mm$^3$ of cortex
(0.1027 cm$3$)
in the generous case,
which is a very small portion of cortex,
especially considering that this model
would be distributed over
several cortical areas.

\subsection{Summary}

% ??? main results

\section{Syllable recognition}

The goal of the syllable recognition system
is to temporally classify syllables
given a continuous trajectory
of production information.
We will evaluate the system
based on its ability to emit
correct syllable classifications
at reasonable times,
and whether those classifications
are available in working memory
between classifications.

\subsection{Evaluation}

The infrastructure for evaluating
the recognition model
is similar to that of the production model.
We use the same library
of German syllable gesture scores,
from which we create
48-dimensional gesture trajectories.
On each trial,
we generate a trajectory
by concatenating the trajectories
of a randomly chosen sequence
of trajectories together
(i.e., we do not use the
syllable production model
to generate input for the model).
That trajectory is provided
to all of the iDMPs as input,
and we record the output of the
temporal input associative memory
and the memory module.

From the data collected,
we compute three metrics
to evaluate performance
in different situations.
First, we calculate
classification accuracy
using the same formula
as is used to calculate
accuracy in the production model
(see Equation~\eqref{eq:accuracy}.
Specifically,
we generate a list of all of the
syllables classified by the model,
and compare that list to the
originally specified list
using the formula
\begin{equation}
  \text{Acc} = \frac{N_g - S - D - I}{N_g},
\end{equation}
where $N_g$ is the number of syllables
in the trajectory,
$S$ is the number of substitutions,
$D$ is the number of deletions,
and $I$ is the number of insertions.
A substitution occurs when
the syllable is misclassified.
A deletion occurs when
a syllable has occurred,
but the model does not emit a classification.
An insertion occurs
when the model emits a classification
when no syllable has occurred
(e.g., it classifies the same syllable twice).

Second, we record the times at which
syllable classification occur,
and compare those times
to the end of the actual syllable trajectory.
Here, we are attempting to determine
if the model can classify syllables
while they are being voiced,
or if the entire syllable must complete
before the classification can be made.

Third, we analyze the contents
of the memory module
in between syllable presentations.
Specifically,
we find the semantic pointer
with the maximum similarity
at the midpoint between
the start and the end of a syllable
for all syllables in the trajectory.
The memory representation of the trial
is perfect if the
previously classified syllable
is in memory at the midpoint
of a syllable.

\subsection{Experiments and results}

??? basic operation fig and text

The basic experimental setup
in the following experiments
is similar to that of the production model.
However, instead of using
a sequence of syllables
as input and producing
a production information trajectory,
the recognition model
uses a production information trajectory
as input and produces
a sequence of syllable classifications.
Again, for each trial in each experiment,
we randomly select a set of syllables
and a syllable sequence
from which to generate a trajectory.
The random selection process
is the same as in the production model.
The randomly selected
gesture scores are combined
into a single trajectory,
and that trajectory
is provided to the network
through a node.
The intrinsic frequency (speed)
of each syllable is randomly assigned
between 1.3 and 1.5 Hz, % verify???
except where noted otherwise.

Again, we begin by varying the number of neurons
in different parts of the model.
In the recognition model,
we only vary the number of neurons
used in each iDMP network.
For each experimental condition,
we run 20 trials using
a trajectory made from
three random syllables
in a repository of three syllables,
meaning that syllable repetition can occur,
but is not guaranteed.

??? fig: nneurons

As shown in Figure~???,
the number of neurons in each iDMP network
has a significant impact on the performance
of the overall model.
Accuracy is relatively low
until around 350 neurons per dimension,
and continues to increase
to the maximum size attempted,
1000 neurons per dimension.
??? more with new results
we will therefore
use ??? neurons per dimension
for each iDMP network.

??? similarity thresh fig

In the case of the recognition model,
two parameters other than the number of neurons
have a large effect on model behavior.
??? similarity thresh

??? scale fig

??? scale text

??? fig : freq

Next, we investigated the
range of frequencies in which the model
can successfully classify trajectories.
In this experiment, we use a random sequence
of three syllables from a
repository of three random syllables
with the same frequency.
Figure~??? shows the accuracy
of the model as a function of the frequency.
Accuracy is initially poor;
however, as shown in Figure~???,
this is due to a large number
of insertions,
which are likely to be insertions
of the correct syllable.
Maximal accuracy around 75\%
occurs at 1.5 Hz,
which is also the frequency at which
deletions become more common
than insertions.
The monotonic decrease in accuracy
from 1.5 Hz onward
is therefore a result of deletions
(i.e., a failure to classify),
though some substitutions also occur
up to 2.5 Hz.
As can be seen in Figure~???,
the variance of the timing difference
(i.e., when the classification occurs
relative to the end of the syllable)
is very small from about 1 Hz and above.
The mean timing difference
is negative in for all frequencies
above 1.0 Hz,
indicating that classifications
occur before the syllable trajectory
is complete.

??? fig : nsyllables

We then investigated whether the model
behavior changes when the size
of the syllabary changes.
As shown in Figure~???,
accuracy drops off quickly when
more than five syllables
are part of the syllabary.
Unlike in the production system,
this effect is easy to predict
from the structure of the network,
due to the competitive nature
of the iDMPs;
all iDMPs are constantly tracking
the system state,
except when a classification has occurred
and the iDMP networks are being inhibited.

??? fig : sequencelen

The length of the sequence,
on the other hand,
has little effect on classification accuracy,
as can be seen in Figure~???.
In this case, the trajectories
of the three syllables in the syllabary
are repeated several times.
The iDMP networks continue to operate
at the same accuracy rate
regardless of how long they have been active,
which is a benefit of this method
over HMMs which must be renormalized
and reset often.
However, as seen in Figure~???,
the accuracy of the memory representation
decreases as the length of the sequence decreases.
This result suggests that
the current memory system,
which is a simple decaying integrator,
is not sufficient for syllable recognition.
Instead, it should be replaced with
an input-gated working memory module,
as is used in the production system
for representing syllable position.

??? fig : repeat

Finally, we verified that the model
operate well in situations with
sequences of unique or repeated syllables.
As shown in Figure~???,
the overall accuracy is indistinguishable
whether repeated syllables are allowed or not.
All other measure are similar as well.
However, there is some indication
in Figure~??? that repeated syllables
increase the proportion of insertions
while lowering the proportion of
deletions and substitutions.
Intuitively, this result makes sense,
as the previously classified syllable
will have a slightly higher system state
even after the iDMPs are reset
through neural inhibition.
However, more trials are required
to show this result
with strong statistical significance.

\subsection{Scaling}

The number of neurons in the
syllable recognition model is
\begin{equation}
  N = N_{in} + N_{\text{SPA}} + N_{\text{iDMP}} + N_C,
\end{equation}
where these variables
are the number of neurons in
a subset of the model.
$N_{in}$ is the number of neurons
in trajectory input ensembles, which is
\begin{equation}
  N_{in} = 48 N_{|in|}
\end{equation}
where 48 is the number of gestures,
and $N_{|in|}$ is the number of neurons
per input ensemble.
$N_{\text{SPA}}$ is the number of
neurons for all SPA modules,
which is
\begin{equation}
  N_{\text{SPA}} = 3 |\text{SYLL}| N_{\text{AM}}
    + D_{\text{SYLL}} N_D_{\text{SYLL}},
\end{equation}
where $|\text{SYLL}|$ is the number of
syllables in the syllabary,
$N_{\text{AM}}$ is the number of neurons
per associative memory ensemble,
$D_{\text{SYLL}}$ is the dimensionality
of syllable semantic pointers,
and $N_D_{\text{SYLL}}$ is the number of neurons
per syllable dimension.
$N_{\text{iDMP}}$ is the number of neurons
in iDMP networks, which is
\begin{equation}
  N_{\text{iDMP}} = |\text{SYLL}| \left( \left(\bar{D_{\text{iDMP}}} + 2 \right)
    N_D_{|\text{iDMP}|} + 20 \right),
\end{equation}
where $\bar{D_{\text{iDMP}}}$ is the average
number of gestures in each syllable,
$N_D_{|\text{iDMP}|}$ is the number of neurons
per iDMP dimension,
and 40 neurons come from ensembles with fixed size.
$N_C$ is the number of neurons
in the classifier,
which is $N_C = 20$.

With the parameters used
in the majority of the experiments above,
$N\approx18500$ neurons,
with most of those neurons
in the iDMP populations
($N_{\text{iDMP}}\approx10500$).
Note that these values are approximate
because the number of gestures per syllable
varies depending on the syllable,
which is randomly selected.\footnote{
  If we did not sparsify the syllable representation,
  then we would be using all 48 gesture dimensions.
  Typically, $bar{D_{\text{iDMP}}}$ is around 6--7,
  which is a significant savings in terms
  of neural resources used.}
Unlike the recognition model,
this portion of the model
grows as the number of syllables grows.
While the growth is linear,
it is scaled by $N_D_{|\text{iDMP}}$,
the number of neurons per iDMP dimensions,
which has to be relatively high
(???, as determined in the experiments above).
If we assume a conservative syllabary
with around 2000 syllables,
and increase the dimensionality
of each syllable to 256,
the model uses
$N \approx 7.29 \times 10^6$ neurons,
which is more than the generous
estimate for the production model.
If we assume a more generous estimate
with 4000 syllables
and 512 dimensions
for syllable semantic pointers,
then the model uses
$N \approx 1.3 \times 10^7$ neurons.
The estimate represents approximately
500 mm$^3$ of cortex (0.5 cm$^3$),
which is still a small amount
considering the size of the brain areas
involved in sensorimotor integration,
but represent poorer scaling
than the other two models presented in this thesis.

\subsection{Summary}

% ??? main results
