{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal derivatives\n",
    "\n",
    "We'll compare and contrast methods from\n",
    "[Tripp](http://compneuro.uwaterloo.ca/publications/tripp2010.html)\n",
    "and [Voelker](https://github.com/ctn-waterloo/summerschool2015/tree/master/tutorials/temprep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import nengo\n",
    "import nengo.utils.numpy as npext\n",
    "\n",
    "# Some plotting niceties\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "sns.set_style('white')\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_deriv(deriv_func, deriv_args, dims, ramp=True, t=1.0, dt=0.001):\n",
    "    with nengo.Network() as net:\n",
    "        if ramp:\n",
    "            timesteps = int(t / dt)\n",
    "            ramp = np.concatenate([np.linspace(0, 1, timesteps/2), np.linspace(1, 0, timesteps/2)])\n",
    "            proc = lambda time: ramp[int(time / dt) % ramp.size]\n",
    "        else:\n",
    "            proc = nengo.processes.BrownNoise()\n",
    "        inp = nengo.Node(size_in=dims)\n",
    "        for i in range(dims):\n",
    "            nengo.Connection(nengo.Node(output=proc), inp[i])\n",
    "        ea = nengo.networks.EnsembleArray(n_neurons=40, n_ensembles=dims)\n",
    "        nengo.Connection(inp, ea.input)\n",
    "        deriv = deriv_func(dimensions=dims, **deriv_args)\n",
    "        nengo.Connection(ea.output, deriv.input)\n",
    "        in_probe = nengo.Probe(inp, synapse=None)\n",
    "        out_probe = nengo.Probe(deriv.output, synapse=0.01)\n",
    "    print(\"%d neurons\" % (sum(e.n_neurons for e in net.all_ensembles)))\n",
    "    sim = nengo.Simulator(net)\n",
    "    sim.run(t)\n",
    "\n",
    "    inp = sim.data[in_probe]\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sim.trange(), inp)\n",
    "    plt.ylabel(\"Input\")\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(sim.trange(), sim.data[out_probe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripp 1: Feedforward, intermediate population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Derivative(dimensions, n_neurons=100, tau=0.01, net=None):\n",
    "    if net is None:\n",
    "        net = nengo.Network(label=\"Derivative\")\n",
    "\n",
    "    with net:\n",
    "        net.input = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        net.intermediate = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        net.output = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        nengo.Connection(net.input, net.intermediate, synapse=tau)\n",
    "        nengo.Connection(net.input, net.output, synapse=tau, transform=1/tau)\n",
    "        nengo.Connection(net.intermediate, net.output, synapse=tau, transform=-1/tau)\n",
    "    return net\n",
    "\n",
    "test_deriv(Derivative, {}, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripp 2: Feedforward, different synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Derivative(dimensions, n_neurons=100,\n",
    "               tau_fast=0.01, tau_slow=0.1, net=None):\n",
    "    if net is None:\n",
    "        net = nengo.Network(label=\"Derivative\")\n",
    "    \n",
    "    with net:\n",
    "        tau_diff = tau_slow - tau_fast\n",
    "        net.input = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        net.output = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        nengo.Connection(net.input, net.output, synapse=tau_fast, transform=1/tau_diff)\n",
    "        nengo.Connection(net.input, net.output, synapse=tau_slow, transform=-1/tau_diff)\n",
    "    return net\n",
    "\n",
    "test_deriv(Derivative, {}, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripp 3: Feedback approximating 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adjust_abc(old_a, old_b, old_c, dimensions):\n",
    "    degree = old_a.shape[0]\n",
    "    a = np.zeros((degree * dimensions, degree * dimensions))\n",
    "    b = np.zeros((degree * dimensions, dimensions))\n",
    "    c = np.zeros((dimensions, degree * dimensions))\n",
    "\n",
    "    # Replicate the existing system in the right blocks\n",
    "    for dim in range(dimensions):\n",
    "        blk = slice(dim * degree, (dim+1) * degree)\n",
    "        a[blk, blk] = old_a\n",
    "        b[blk, dim] = old_b[:, 0]\n",
    "        c[dim, blk] = old_c\n",
    "    return a, b, c\n",
    "\n",
    "def FeedbackDerivative(dimensions, a, b, c,\n",
    "                       n_neurons=100, tau=0.005, net=None):\n",
    "    if net is None:\n",
    "        net = nengo.Network(label=\"Derivative\")\n",
    "    \n",
    "    tau_a = np.identity(a.shape[0]) + a * tau\n",
    "    tau_b = b * tau\n",
    "    tau_a, tau_b, c = adjust_abc(tau_a, tau_b, c, dimensions)\n",
    "\n",
    "    with net:\n",
    "        net.input = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        net.output = nengo.Ensemble(n_neurons * dimensions, dimensions)\n",
    "        net.diff = nengo.Ensemble(n_neurons * dimensions * 2, dimensions * 2)\n",
    "        nengo.Connection(net.input, net.diff, synapse=tau, transform=tau_b)\n",
    "        nengo.Connection(net.diff, net.diff, synapse=tau, transform=tau_a)\n",
    "        nengo.Connection(net.diff, net.output, transform=c)\n",
    "    return net\n",
    "\n",
    "\n",
    "def Derivative(dimensions, n_neurons=100, tau=0.01, net=None):\n",
    "    a = np.array([[-5, -7.5], [3.3333, -15]])\n",
    "    b = np.array([[10], [20]])\n",
    "    c = np.array([[10, 0]])\n",
    "    return FeedbackDerivative(dimensions, a, b, c, n_neurons, tau, net)\n",
    "\n",
    "test_deriv(Derivative, {}, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripp 4: Butterworth filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Derivative(dimensions, n_neurons=100, tau=0.01, net=None):\n",
    "    a = np.array([[-8.8858, 19.9931], [-3.9492, -8.8858]])\n",
    "    b = np.array([[27.4892], [-12.2174]])\n",
    "    c = np.array([[5.7446, 0]])\n",
    "    return FeedbackDerivative(dimensions, a, b, c, n_neurons, tau, net)\n",
    "\n",
    "test_deriv(Derivative, {}, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voelker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nengo.utils.filter_design import zpk2ss, tf2ss, cont2discrete\n",
    "from scipy.linalg import solve_lyapunov\n",
    "from scipy.misc import factorial, pade\n",
    "\n",
    "\n",
    "class LTI(object):\n",
    "    \"\"\"Methods for dealing with LTI filters in Nengo.\n",
    "\n",
    "    Adapted from Aaron Voelker's delay notebook at\n",
    "    summerschool2015/tutorials/temprep/delay.ipynb\n",
    "    \"\"\"\n",
    "    def __init__(self, a, b, c, d):\n",
    "        self.a = np.array(a)\n",
    "        self.b = np.array(b)\n",
    "        self.c = np.array(c)\n",
    "        self.d = np.array(d)\n",
    "\n",
    "    @property\n",
    "    def abcd(self):\n",
    "        return (self.a, self.b, self.c, self.d)\n",
    "\n",
    "    @classmethod\n",
    "    def from_synapse(cls, synapse):\n",
    "        \"\"\"Instantiate class from a Nengo synapse.\"\"\"\n",
    "        if not hasattr(synapse, 'num') or not hasattr(synapse, 'den'):\n",
    "            raise ValueError(\"Must be a linear filter with 'num' and 'den'\")\n",
    "        return cls(*tf2ss(synapse.num, synapse.den))\n",
    "\n",
    "    @classmethod\n",
    "    def from_tf(cls, num, den):\n",
    "        \"\"\"Instantiate class from a transfer function.\"\"\"\n",
    "        return cls(*tf2ss(num, den))\n",
    "\n",
    "    @classmethod\n",
    "    def from_zpk(cls, z, p, k):\n",
    "        \"\"\"Instantiate class from a zero-pole-gain representation.\"\"\"\n",
    "        return cls(*zpk2ss(z, p, k))\n",
    "\n",
    "    def copy(self):\n",
    "        return LTI(*self.abcd)\n",
    "\n",
    "    def scale_to(self, radii=1.0):\n",
    "        \"\"\"Scales the system to give an effective radius of r to x.\"\"\"\n",
    "        r = np.asarray(radii, dtype=np.float64)\n",
    "        if r.ndim > 1:\n",
    "            raise ValueError(\n",
    "                \"radii (%s) must be a 1-D array or scalar\" % radii)\n",
    "        elif r.ndim == 0:\n",
    "            r = np.ones(len(self.a)) * r\n",
    "        self.a = self.a / r[:, None] * r\n",
    "        self.b /= r[:, None]\n",
    "        self.c *= r\n",
    "\n",
    "    def ab_norm(self):\n",
    "        \"\"\"Returns H2-norm of each component of x in the state-space.\n",
    "\n",
    "        Equivalently, this is the H2-norm of each component of (A, B, I, 0).\n",
    "        This gives the power of each component of x in response to white-noise\n",
    "        input with uniform power.\n",
    "\n",
    "        Useful for setting the radius of an ensemble array with continuous\n",
    "        dynamics (A, B).\n",
    "        \"\"\"\n",
    "        p = solve_lyapunov(self.a, -np.dot(self.b, self.b.T))  # AP + PA^H = Q\n",
    "        assert np.allclose(np.dot(self.a, p)\n",
    "                           + np.dot(p, self.a.T)\n",
    "                           + np.dot(self.b, self.b.T), 0)\n",
    "        c = np.eye(len(self.a))\n",
    "        h2norm = np.dot(c, np.dot(p, c.T))\n",
    "        # The H2 norm of (A, B, C) is sqrt(tr(CXC^T)), so if we want the norm\n",
    "        # of each component in the state-space representation, we evaluate\n",
    "        # this for each elementary vector C separately, which is equivalent\n",
    "        # to picking out the diagonals\n",
    "        return np.sqrt(h2norm[np.diag_indices(len(h2norm))])\n",
    "\n",
    "    def to_sim(self, synapse, dt=0):\n",
    "        \"\"\"Maps a state-space LTI to the synaptic dynamics on A and B.\"\"\"\n",
    "        if not isinstance(synapse, nengo.Lowpass):\n",
    "            raise TypeError(\"synapse (%s) must be Lowpass\" % (synapse,))\n",
    "        if dt == 0:\n",
    "            a = synapse.tau * self.a + np.eye(len(self.a))\n",
    "            b = synapse.tau * self.b\n",
    "        else:\n",
    "            a, b, c, d, _ = cont2discrete(self.abcd, dt=dt)\n",
    "            aa = np.exp(-dt / synapse.tau)\n",
    "            a = 1. / (1 - aa) * (a - aa * np.eye(len(a)))\n",
    "            b = 1. / (1 - aa) * b\n",
    "        self.a, self.b, self.c, self.d = a, b, c, d\n",
    "\n",
    "\n",
    "def exp_delay(p, q, c=1.0):\n",
    "    \"\"\"Returns F = p/q such that F(s) = e^(-sc).\"\"\"\n",
    "    i = np.arange(p+q) + 1\n",
    "    taylor = np.append([1.0], (-c)**i / factorial(i))\n",
    "    return pade(taylor, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lti(n_neurons, dimensions, lti_system, synapse=nengo.Lowpass(0.05),\n",
    "        controlled=False, dt=0.001, radii=None, radius=1.0):\n",
    "    if radii is None:\n",
    "        radii = lti_system.ab_norm()\n",
    "    radii *= radius\n",
    "    lti_system.scale_to(radii)\n",
    "    lti_system.to_sim(synapse, dt)\n",
    "\n",
    "    degree = lti_system.a.shape[0]\n",
    "    a = np.zeros((degree * dimensions, degree * dimensions))\n",
    "    b = np.zeros((degree * dimensions, dimensions))\n",
    "    c = np.zeros((dimensions, degree * dimensions))\n",
    "    d = np.zeros(dimensions)\n",
    "\n",
    "    # Replicate the existing system in the right blocks\n",
    "    for dim in range(dimensions):\n",
    "        blk = slice(dim * degree, (dim+1) * degree)\n",
    "        a[blk, blk] = lti_system.a\n",
    "        b[blk, dim] = lti_system.b[:, 0]\n",
    "        c[dim, blk] = lti_system.c\n",
    "        d[dim] = lti_system.d\n",
    "\n",
    "    size_in = b.shape[1]\n",
    "    size_state = a.shape[0]\n",
    "    size_out = c.shape[0]\n",
    "\n",
    "    inp = nengo.Node(size_in=size_in, label=\"input\")\n",
    "    out = nengo.Node(size_in=size_out, label=\"output\")\n",
    "    x = nengo.networks.EnsembleArray(n_neurons, size_state)\n",
    "    x_in = x.input\n",
    "    x_out = x.output\n",
    "\n",
    "    nengo.Connection(x_out, x_in, transform=a, synapse=synapse)\n",
    "    nengo.Connection(inp, x_in, transform=b, synapse=synapse)\n",
    "    nengo.Connection(x_out, out, transform=c, synapse=None)\n",
    "    nengo.Connection(inp, out, transform=d, synapse=None)\n",
    "\n",
    "    return inp, out\n",
    "\n",
    "\n",
    "def deconvolution(n_neurons, dimensions, tf, delay, degree=4, **lti_kwargs):\n",
    "    \"\"\"Approximate the inverse of a given transfer function using a delay.\"\"\"\n",
    "    num, den = [np.poly1d(tf[0]), np.poly1d(tf[1])]\n",
    "    order = len(den) - len(num)\n",
    "    if order >= degree:\n",
    "        raise ValueError(\"order (%d) must be < degree (%d)\"\n",
    "                         % (order, degree))\n",
    "    edp, edq = exp_delay(degree - order, degree, delay)\n",
    "    p, q = np.polymul(edp, den), np.polymul(edq, num)\n",
    "    inp, out = lti(n_neurons, dimensions, LTI.from_tf(p, q), **lti_kwargs)\n",
    "    return inp, out, degree\n",
    "\n",
    "\n",
    "def derivative(n_neurons, dimensions, tau, delay, **deconv_kwargs):\n",
    "    \"\"\"Output a signal that is a derivative of the input.\"\"\"\n",
    "    return deconvolution(n_neurons, dimensions, ([1], [tau, 0]), delay, **deconv_kwargs)\n",
    "\n",
    "\n",
    "def Derivative(delay, dimensions,\n",
    "               n_neurons=100, tau=0.005, tau_highpass=0.05, net=None):\n",
    "    if net is None:\n",
    "        net = nengo.Network(label=\"Derivative\")\n",
    "\n",
    "    with net:\n",
    "        net.input, net.output, _ = derivative(\n",
    "            n_neurons, dimensions=dimensions, delay=delay, tau=tau, radius=0.1, degree=2)\n",
    "\n",
    "        actual_output = nengo.Node(size_in=dimensions)\n",
    "        nengo.Connection(net.output, actual_output, synapse=tau_highpass)\n",
    "        net.output = actual_output\n",
    "    return net\n",
    "\n",
    "test_deriv(Derivative, {'delay': 0.01}, dims=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
